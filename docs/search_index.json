[
["index.html", "Data Analysis in Ecology with R and Stan Preface Why this book? About this book How to contribute? Acknowledgments", " Data Analysis in Ecology with R and Stan Fränzi Korner-Nievergelt and Tobias Roth 2018-08-13 Preface Why this book? In 2015, we wrote a statistics book for Master/PhD level Bayesian data analyses in ecology (F. Korner-Nievergelt et al. 2015). You can order it here. People seemed to like it (e.g. (Harju 2016)). Since then, two parallel processes happen. First, we learn more and we become more confident in what we do, or what we do not, and why we do what we do. Second, several really clever people develop software that broaden the spectrum of ecological models that now easily can be applied by ecologists used to work with R. With this e-book, we open the possibility to add new or substantially revised material. In most of the time, it should be in a state that it can be printed and used together with the book as handout for our stats courses. About this book We do not copy text from the book into the e-book. Therefore, we refer to the book (F. Korner-Nievergelt et al. 2015) for reading about the basic theory on doing Bayesian data analyses using linear models. However, Chapters 1 to 17 of this dynamic e-book correspond to the book chapters. In each chapter, we may provide updated R-codes and/or additional material. The following chapters contain completely new material that we think may be useful for ecologists. While we show the R-code behind most of the analyses, we sometimes choose not to show all the code in the html version of the book. This is particularly the case for some of the illustrations. An intrested reader can always consult the public GitHub repository with the rmarkdown-files that were used to generate the book. How to contribute? It is open so that everybody with a GitHub account can make comments and suggestions for improvement. Readers can contribute in two ways. One way is to add an issue. The second way is to contribute content directly through the edit button at the top of the page (i.e. a symbol showing a pencil in a square). That button is linked to the rmarkdown source file of each page. You can correct typos or add new text and then submit a GitHub pull request. We try to respond to you as quickly as possible. We are looking forward to your contribution! Acknowledgments We thank Yihui Xie for providing bookdown which makes is much fun to write open books such as ours. "],
["PART-I.html", "1 Introduction to PART I 1.1 Further reading", " 1 Introduction to PART I During our courses we are sometimes asked to give an introduction to some R-related stuff covering data analysis, presentation of results or rather specialist topics in ecology. In this part we present collected these introduction and try to keep them updated. This is also a commented collection of R-code that we documented for our own work. We hope this might be useful olso for other readers. 1.1 Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],
["analyses-steps.html", "2 Data analysis step by step 2.1 Plausibility of Data 2.2 Relationships 2.3 Error Distribution 2.4 Preparation of Explanatory Variables 2.5 Data Structure 2.6 Fit the Model 2.7 Check Model 2.8 Model Uncertainty 2.9 Draw Conclusions Further reading", " 2 Data analysis step by step In this chapter we provide a checklist with some guidance for data analysis. However, do not expect tthe list to be complete and for different studies, a different order of the steps may make more sense. We usually repeat steps 2.2 to 2.7 until we find one or a set of models that fit the data well and that are realistic enough to be useful for the intended purpose. Data analysis is always a lot of work and, often, the following steps have to be repeated many times until we find a useful and robust model. There is a danger with this: we may find interesting results that answer different questions than we asked originally.We can report such findings, but we should state that they appeared (more or less by chance) during the data exploration and model fitting phase, and we have to be aware that the estimates may be biased because the study was not optimally designed with respect to these findings. It is important to always keep the original aim of the study in mind. Do not adjust the study question according to the data. We also recommend reporting what the model started with at the first iteration and describing the strategy and reasoning behind the model development process. 2.1 Plausibility of Data Prepare the data and check graphically, or via summary statistics, whether all the data are plausible. Prepare the data so that errors (typos, etc.) are minimal, for example, by double-checking the entries. See chapter 3 for useful R-code that can be used for data preparation and to make plausibility controls. 2.2 Relationships Think about the direct and indirect relationships among the variables of the study. We normally start a data analysis by drawing a sketch of the model including all explanatory variables and interactions that may be biologically meaningful.We will most likely repeat this step after having looked at the model fit. To make the data analysis transparent we should report every model that was considered. A short note about why a specific model was considered and why it was discarded helps make the modeling process reproducible. 2.3 Error Distribution What is the nature of the variable of interest (outcome, dependent variable)? Chapter 5 give an overview of the distributions that are most relevant for ecologists. 2.4 Preparation of Explanatory Variables Look at the distribution (histogram) of every explanatory variable: Linear models do not assume that the explanatory variables have any specific distribution. Thus there is no need to check for a normal distribution! However, very skewed distributions result in unequal weighting of the observations in the model. In extreme cases, the slope of a regression line is defined by one or a few observations only. We also need to check whether the variance is large enough, and to think about the shape of the expected effect. The following four questions may help with this step: l Is the variance (of the explanatory variable) big enough so that an effect of the variable can be measured? l Is the distribution skewed? If an explanatory variable is highly skewed, it may make sense to transform the variable. l Does it show a bimodal distribution? Consider making the variable binary. l Is it expected that a change of 1 at lower values for x has the same biological effect as a change of 1 at higher values of x? If not, a trans- formation (e.g., log) could linearize the relationship between x and y. Centering: Centering (x_centered 1⁄4 x mean(x)) is a transformation that produces a variable with a mean of 0. With centered predictors, the intercept and main effects in the linear model are better interpretable (they are measured at the center of the data instead of at the covariate value 1⁄4 0 which may be far off; Section 4.2.6); the model fitting algorithm converges faster and better. Scaling: To make the estimate of the effect sizes comparable between var- iables, the variables can be scaled, that is, x_scaled 1⁄4 x/sd(x). The unit of the scaled variable is then 1 standard deviation. Gelman and Hill (2007, p. 55 f) propose to scale the variables by two times the standard deviation (x_scaled 1⁄4 x/(2*sd(x))) to make effect sizes comparable between numeric and binary variables. Scaling can be important for model convergence, especially when polynomials are included. Consider the use of orthogonal polynomials (Section 4.2.9). Collinearity: l Look at the correlation among the explanatory variables (pairs plot or correlation matrix). l If the explanatory variables are correlated, go back to step 2 and see Section 4.2.7 for more details about collinearity. Are interactions and polynomial terms needed in the model? If not already done in step 2, think about the relationship between each explanatory variable and the dependent variable. l Is it linear or do polynomial terms have to be included in the model? If the relationship cannot be described appropriately by polynomial terms, think of a nonlinear model or a generalized additive model (GAM). l May the effect of one explanatory variable depend on the value of another explanatory variable (interaction)? 2.5 Data Structure After having taken into account all of the (fixed effect) terms from step 4: are the observations independent or grouped/structured? 2.6 Fit the Model Fit the model as described in Chapters 4, 7, 8, 9, or 14. 2.7 Check Model We assess model fit by graphical analyses of the residuals (Chapter 6), by predictive model checking (Section 10.1), or by sensitivity analysis (Chapter 15). The following is a nonexhaustive list of aspects that can be looked at in a residual analysis and posterior predictive model checking. For non-Gaussian models it is often easier to assess model fit using pos- terior predictive checks (Chapter 10) rather than residual analyses. Posterior predictive checks usually show clearly in which aspect the model failed so we can go back to step 2 of the analysis. Recognizing in what aspect a model does not fit the data based on residual plots improves with experience. Therefore, the following table lists some patterns that can appear in residual plots together with what these patterns possibly indicate. We also indicate what could be done in the specific cases. 2.8 Model Uncertainty If, while working through steps 1 to 7, possibly repeatedly, we came up with one or more models that fit the data reasonably well, we then turn to the methods presented in Chapter 11 to draw inference from more than one model. If we have only one model, we proceed to step 9. 2.9 Draw Conclusions Simulate values from the joint posterior distribution of the model parameters (sim, BUGS, Stan). Use these samples to present parameter uncertainty, to obtain posterior distributions for predictions, probabilities of specific hypotheses, and derived quantities. Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],
["datamanip.html", "3 Data preparation 3.1 Basic operations 3.2 Conection to existing sql-DB 3.3 Further reading", " 3 Data preparation 3.1 Basic operations Alle Packete laden library(tidyverse) oder nur library(tidyverse). dat &lt;- iris %&gt;% as.tibble() %&gt;% filter(Sepal.Length &gt; 5) %&gt;% group_by(Species) %&gt;% summarise(n = n(), mittel = mean(Petal.Length)) 3.2 Conection to existing sql-DB Die in diesem Kapitel präsentierten Datenabfragen nutzen das R-Packet dplyr. Damit der R-Code funktioniert muss eine Kopie der SQLite Datenbank lokal verfügbar sein (DropBox-Zugang von Ro beziehen). Als Grundlage muss also erst das R-Packet geladen werden und den Link zu den Datebbanken hergestellt werden. # Connection to data base db &lt;- src_sqlite(path = &quot;~/Documents/Dropbox/DB_BDM.db&quot;, create = FALSE) rd &lt;- src_sqlite(path = &quot;~/Documents/Dropbox/DB_CHRD.db&quot;, create = FALSE) 3.2.1 Abfragen zur Artenvielfalt Im Folgenden soll die mittlere Artenvielfalt für Z9 Plfanzen im LANAG für die Periode 2013 bis 2017 im Wald berechnet werden. # Auswahl der gültigen Aufnahmen in der entprechenden Periode ausw &lt;- inner_join( tbl(db, &quot;STICHPROBE_Z9&quot;) %&gt;% filter(LANAG_aktuell == &quot;ja&quot;) %&gt;% dplyr::select(aID_STAO), tbl(db, &quot;KD_Z9&quot;) %&gt;% filter(yearP &gt;= 2013 &amp; yearP &lt;= 2017 &amp; !is.na(yearPl) &amp; HN == &quot;Wald&quot;) %&gt;% dplyr::select(aID_KD, aID_STAO) ) # Artenvielfalt für jede Aufnahme anhängen dat &lt;- left_join( ausw, tbl(db, &quot;PL&quot;) %&gt;% filter(Z7 == 0) %&gt;% group_by(aID_KD) %&gt;% summarise(AZ = n()) ) %&gt;% data.frame # AZ von gültigen Aufnahmen ohne Arten auf 0 setzen dat &lt;- replace_na(dat, list(AZ = 0)) # Mittelwert berechnen mean(dat$AZ) Der folgende Code plottet die mittlere Artenvielfalt der BDM Z7 Pflanzen (ohne Verdichtung) gegen die Meereshöhe. ausw &lt;- inner_join( tbl(db, &quot;STICHPROBE_Z7&quot;) %&gt;% filter(BDM_aktuell == &quot;ja&quot; &amp; BDM_Verdichtung == &quot;nein&quot;) %&gt;% dplyr::select(aID_STAO), tbl(db, &quot;KD_Z7&quot;) %&gt;% filter(yearP &gt;= 2013 &amp; yearP &lt;= 2017 &amp; !is.na(yearPl) &amp; Aufnahmetyp == &quot;Normalaufnahme_Z7&quot;) %&gt;% dplyr::select(aID_KD, aID_STAO) ) # Artenvielfalt für jede Aufnahme berechnen dat &lt;- left_join( ausw, tbl(db, &quot;PL&quot;) %&gt;% filter(Z7 == 1) %&gt;% group_by(aID_KD) %&gt;% summarise(AZ = n()) ) # Meereshöhe anhängen dat &lt;- left_join(dat, tbl(db, &quot;RAUMDATEN_Z7&quot;) %&gt;% select(aID_STAO, Hoehe)) dat &lt;- replace_na(dat %&gt;% data.frame, list(AZ = 0)) # Plot results ggplot(dat, aes(x = Hoehe, y = AZ)) + geom_point(shape = 16) + geom_smooth() 3.3 Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],
["figures.html", "4 Visualizations 4.1 Short Checklist for figures Further reading", " 4 Visualizations 4.1 Short Checklist for figures The figure should represent the answer to the study question. Often, the classical types of plots such as a scatterplot, a bar plot, or an effects plot are sufficient. However, in many cases, adding a little bit of creativity can greatly improve readability or the message of the figure. 2.Label the x- and y-axes. Make sure that the units are indicated either within the title or in the figure legend. Starty-axisatzeroifthereferencetozeroisimportantfortheinterpretationof effects. The argument ylim[c(0, max(dat$y)) in R is used for this purpose. Scale the axes so that all data are shown. Make sure that sample size is indicated either in the figure or in the legend (or, at least, easy to find in the text). Use interpretable units. That means, if the variable on the x-axis has been z-transformed to fit the model, back-transform the effects to the original scale. Give the raw data whenever possible. Sometimes, a significant effect cannot be seen in the raw data because so many other variables have an influence on the outcome. Then, you may prefer showing the effect only (e.g., a regression line with a credible interval) and give the residual standard deviation in the figure legend. Even then, we think it is important to show the raw data graphically somewhere else in the paper or in the supplementary material. A scatterplot of the data can contain structures that are lost in summary statistics. Draw the figures as simply as possible. Avoid 3D graphics. Delete all unnecessary elements. Reduce the number of different colors to a minimum necessary. A color scale from orange to blue gives a gray scale in a black-and-white print. colorRampPalette(c(&quot;orange&quot;, &quot;blue&quot;))(5) produces five colors on a scale from orange to blue. Remember that around 8% of the northern European male population have difficulties distinguishing red from green but it is easier for them to distinguish orange from blue. Further reading Data Visualization. A practical introduction: A practical introduction to data visulization in R. Good data visualizations also make it easier to communicate your ideas and findings to other people. Beyond that, producing effective plots from your own data is the best way to develop a good eye for reading and understanding graphs—good and bad—made by others. Fundamentals of Data Visualization: A guide to making visualizations that accurately reflect the data, tell a story, and look professional. […] This is an online preview of the book “Fundamentals of Data Visualization” to be published with O’Reilly Media, Inc. Completed chapters will be posted here as they become available. The book is meant as a guide to making visualizations that accurately reflect the data, tell a story, and look professional. It has grown out of my experience of working with students and postdocs in my laboratory on thousands of data visualizations. "],
["distributions.html", "5 Probability distributions 5.1 Introduction 5.2 Normal distribution 5.3 Poisson distribution 5.4 Gamma distribution", " 5 Probability distributions 5.1 Introduction hist(rnorm(1000)) Figure 5.1: Das ist ein Versuch. 5.2 Normal distribution xxx 5.3 Poisson distribution xxx 5.4 Gamma distribution xxx "],
["rgis.html", "6 Spatial analyses and maps 6.1 Hintergrund 6.2 Coordinate systems 6.3 Further reading", " 6 Spatial analyses and maps 6.1 Hintergrund Some useful packages - raster: Very efficient for raster data. - sf: Very efficient package for data other than raster data. It also link to GEOS, GDAL proj.4. library(sf) library(raster) 6.2 Coordinate systems The following are some definitions of coordinate systems that I often use: ch1903 &lt;- CRS(&quot;+init=epsg:21781&quot;) # Old Swiss grid 1903 chLV95 &lt;- CRS(&quot;+init=epsg:2056&quot;) # New Swiss grid 1903+ wgs84 &lt;- CRS(&quot;+init=epsg:4326&quot;) # WGS 84 The following code transfers a spatial point from one coordinate reference system to an ohter: pt &lt;- data.frame(x=650007.0, y=227023.0) coordinates(pt) &lt;- ~ x + y proj4string(pt) &lt;- ch1903 spTransform(pt, chLV95) ## class : SpatialPoints ## features : 1 ## extent : 2650007, 2650007, 1227023, 1227023 (xmin, xmax, ymin, ymax) ## coord. ref. : +init=epsg:2056 +proj=somerc +lat_0=46.95240555555556 +lon_0=7.439583333333333 +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel +towgs84=674.374,15.056,405.346,0,0,0,0 +units=m +no_defs 6.3 Further reading Geocomputation with R: This book is aimed at people who want to do spatial data analysis, visualization and modeling using open source software and reproducible workflows. "],
["reproducible.html", "7 Reproducible Research 7.1 Introduction 7.2 Add citations 7.3 Further reading", " 7 Reproducible Research 7.1 Introduction 7.2 Add citations Mit dem Packet knitcitations können Referenzen relativ einfach gesucht und in das .bib File eingefügt werden. Erst muss das Packet geladen und der lokale Speicher gelöscht werden. Das Format pandoc scheint auch nötig zu sein. library(knitcitations) cleanbib() cite_options(citation_format = &quot;pandoc&quot;) Danach kann man einfach nach einer Referenz mit Stichworten, Autoren oder DOI-Nummer. Zum Beispiel sucht der Befehl citep(&quot;Roth, Plattner Amrhein&quot;) die entsprechende Referenz und fügt diese ein (T. Roth, Plattner, and Amrhein 2014). Der folgende Befehl schreibt alle Referenzen aus der Zwischenablage in das .bib File. write.bibtex(file=&quot;References.bib&quot;, append = TRUE) Am einfachsten folgende Hilfsfunktion benutzen um aus der Konsole eine Referenz ins .bib File zu schreiben. ref &lt;- function(x) { library(knitcitations) cleanbib() print(citep(x)) write.bibtex(file=&quot;References.bib&quot;, append = TRUE) } 7.3 Further reading Rmarkdown: The first official book authored by the core R Markdown developers that provides a comprehensive and accurate reference to the R Markdown ecosystem. With R Markdown, you can easily create reproducible data analysis reports, presentations, dashboards, interactive applications, books, dissertations, websites, and journal articles, while enjoying the simplicity of Markdown and the great power of R and other languages. Bookdown by Yihui Xie: A guide to authoring books with R Markdown, including how to generate figures and tables, and insert cross-references, citations, HTML widgets, and Shiny apps in R Markdown. The book can be exported to HTML, PDF, and e-books (e.g. EPUB). The book style is customizable. You can easily write and preview the book in RStudio IDE or other editors, and host the book wherever you want (e.g. bookdown.org). Our book is written using bookdown. "],
["furthertopics.html", "8 Further topics 8.1 Bioacoustic analyse 8.2 Python", " 8 Further topics This is a collection of short introductions or links with commented R code that cover other topics that might be usful for ecologists. 8.1 Bioacoustic analyse Bioacoustic analyses are nicely covered in a blog by Marcelo Araya-Salas. 8.2 Python Like R, python is a is a high-level programming language that is used by many ecologists. The reticulate package provides a comprehensive set of tools for interoperability between Python and R. library(reticulate) use_python(&quot;/usr/local/bin/python&quot;) # Reference to python installation # This is python print &#39;Hello, world!&#39; ## Hello, world! "],
["PART-II.html", "9 Introduction to PART II Further reading", " 9 Introduction to PART II Further reading A really good introductory book to Bayesian data analyses is (McElreath 2016). This book starts with a thorough introduction to applying the Bayes theorem for drawing inference from data. In addition, it carefully discusses what can and what cannot be concluded from statistical results. We like this very much. We like looking up statistical methods in papers and books written by Andrew Gelman (e.g. (Gelman et al. 2014)) and Trevor Hastie (e.g. (T. Hastie, Tibshirani, and Friedman 2009, Efron and Hastie (2016))) because both explain complicated things in a concise and understandable way. "],
["priors.html", "10 Prior distributions 10.1 Introduction 10.2 How to choose a prior 10.3 Prior sensitivity", " 10 Prior distributions 10.1 Introduction 10.2 How to choose a prior Tabelle von Fränzi (CourseIII_glm_glmmm/course2018/presentations_handouts/presentations) 10.3 Prior sensitivity xxx "],
["ridge.html", "11 Ridge regression 11.1 Introduction", " 11 Ridge regression We should provide an example in Stan. 11.1 Introduction # Settings library(R2OpenBUGS) bugslocation &lt;- &quot;C:/Program Files/OpenBUGS323/OpenBugs.exe&quot; # location of OpenBUGS bugsworkingdir &lt;- file.path(getwd(), &quot;BUGS&quot;) # Bugs working directory #------------------------------------------------------------------------------- # Simulate fake data #------------------------------------------------------------------------------- library(MASS) n &lt;- 50 # sample size b0 &lt;- 1.2 b &lt;- rnorm(5, 0, 2) Sigma &lt;- matrix(c(10,3,3,2,1, 3,2,3,2,1, 3,3,5,3,2, 2,2,3,10,3, 1,1,2,3,15),5,5) Sigma x &lt;- mvrnorm(n = n, rep(0, 5), Sigma) simresid &lt;- rnorm(n, 0, sd=3) # residuals x.z &lt;- x for(i in 1:ncol(x)) x.z[,i] &lt;- (x[,i]-mean(x[,i]))/sd(x[,i]) y &lt;- b0 + x.z%*%b + simresid # calculate y, i.e. the data #------------------------------------------------------------------------------- # Function to generate initial values #------------------------------------------------------------------------------- inits &lt;- function() { list(b0=runif(1, -2, 2), b=runif(5, -2, 2), sigma=runif(1, 0.1, 2)) } #------------------------------------------------------------------------------- # Run OpenBUGS #------------------------------------------------------------------------------- parameters &lt;- c(&quot;b0&quot;, &quot;b&quot;, &quot;sigma&quot;) lambda &lt;- c(1, 2, 10, 25, 50, 100, 500, 1000, 10000) bs &lt;- matrix(ncol=length(lambda), nrow=length(b)) bse &lt;- matrix(ncol=length(lambda), nrow=length(b)) for(j in 1:length(lambda)){ datax &lt;- list(y=as.numeric(y), x=x, n=n, mb=rep(0, 5), lambda=lambda[j]) fit &lt;- bugs(datax, inits, parameters, model.file=&quot;ridge_regression.txt&quot;, n.thin=1, n.chains=2, n.burnin=5000, n.iter=10000, debug=FALSE, OpenBUGS.pgm = bugslocation, working.directory=bugsworkingdir) bs[,j] &lt;- fit$mean$b bse[,j] &lt;- fit$sd$b } range(bs) plot(1:length(lambda), seq(-2, 1, length=length(lambda)), type=&quot;n&quot;) colkey &lt;- rainbow(length(b)) for(j in 1:nrow(bs)){ lines(1:length(lambda), bs[j,], col=colkey[j], lwd=2) lines(1:length(lambda), bs[j,]-2*bse[j,], col=colkey[j], lty=3) lines(1:length(lambda), bs[j,]+2*bse[j,], col=colkey[j], lty=3) } abline(h=0) round(fit$summary,2) #------------------------------------------------------------------------------- # Run WinBUGS #------------------------------------------------------------------------------- library(R2WinBUGS) bugsdir &lt;- &quot;C:/Users/fk/WinBUGS14&quot; # mod &lt;- bugs(datax, inits= inits, parameters, model.file=&quot;normlinreg.txt&quot;, n.chains=2, n.iter=1000, n.burnin=500, n.thin=1, debug=TRUE, bugs.directory=bugsdir, program=&quot;WinBUGS&quot;, working.directory=bugsworkingdir) #------------------------------------------------------------------------------- # Test convergence and make inference #------------------------------------------------------------------------------- library(blmeco) # Make Figure 12.2 par(mfrow=c(3,1)) historyplot(fit, &quot;beta0&quot;) historyplot(fit, &quot;beta1&quot;) historyplot(fit, &quot;sigmaRes&quot;) # Parameter estimates print(fit$summary, 3) # Make predictions for covariate values between 10 and 30 newdat &lt;- data.frame(x=seq(10, 30, length=100)) Xmat &lt;- model.matrix(~x, data=newdat) predmat &lt;- matrix(ncol=fit$n.sim, nrow=nrow(newdat)) for(i in 1:fit$n.sim) predmat[,i] &lt;- Xmat%*%c(fit$sims.list$beta0[i], fit$sims.list$beta1[i]) newdat$lower.bugs &lt;- apply(predmat, 1, quantile, prob=0.025) newdat$upper.bugs &lt;- apply(predmat, 1, quantile, prob=0.975) plot(y~x, pch=16, las=1, cex.lab=1.4, cex.axis=1.2, type=&quot;n&quot;, main=&quot;&quot;) polygon(c(newdat$x, rev(newdat$x)), c(newdat$lower.bugs, rev(newdat$upper.bugs)), col=grey(0.7), border=NA) abline(c(fit$mean$beta0, fit$mean$beta1), lwd=2) box() points(x,y) "],
["sem.html", "12 Structural equation model 12.1 Introduction", " 12 Structural equation model We should provide an example in Stan. 12.1 Introduction ------------------------------------------------------------------------------------------------------ # General settings #------------------------------------------------------------------------------------------------------ library(MASS) library(rjags) library(MCMCpack) #------------------------------------------------------------------------------------------------------ # Simulation #------------------------------------------------------------------------------------------------------ n &lt;- 100 heffM &lt;- 0.6 # effect of H on M heffCS &lt;- 0.0 # effect of H on Clutch size meffCS &lt;- 0.6 # effect of M on Clutch size SigmaM &lt;- matrix(c(0.1,0.04,0.04,0.1),2,2) meffm1 &lt;- 0.6 meffm2 &lt;- 0.7 SigmaH &lt;- matrix(c(0.1,0.04,0.04,0.1),2,2) meffh1 &lt;- 0.6 meffh2 &lt;- -0.7 # Latente Variablen H &lt;- rnorm(n, 0, 1) M &lt;- rnorm(n, heffM * H, 0.1) # Clutch size CS &lt;- rnorm(n, heffCS * H + meffCS * M, 0.1) # Indicators eM &lt;- cbind(meffm1 * M, meffm2 * M) datM &lt;- matrix(NA, ncol = 2, nrow = n) eH &lt;- cbind(meffh1 * H, meffh2 * H) datH &lt;- matrix(NA, ncol = 2, nrow = n) for(i in 1:n) { datM[i,] &lt;- mvrnorm(1, eM[i,], SigmaM) datH[i,] &lt;- mvrnorm(1, eH[i,], SigmaH) } #------------------------------------------------------------------------------ # JAGS Model #------------------------------------------------------------------------------ dat &lt;- list(datM = datM, datH = datH, n = n, CS = CS, #H = H, M = M, S3 = matrix(c(1,0,0,1),nrow=2)/1) # Function to create initial values inits &lt;- function() { list( meffh = runif(2, 0, 0.1), meffm = runif(2, 0, 0.1), heffM = runif(1, 0, 0.1), heffCS = runif(1, 0, 0.1), meffCS = runif(1, 0, 0.1), tauCS = runif(1, 0.1, 0.3), tauMH = runif(1, 0.1, 0.3), tauH = rwish(3,matrix(c(.02,0,0,.04),nrow=2)), tauM = rwish(3,matrix(c(.02,0,0,.04),nrow=2)) # M = as.numeric(rep(0, n)) ) } t.n.thin &lt;- 50 t.n.chains &lt;- 2 t.n.burnin &lt;- 20000 t.n.iter &lt;- 50000 # Run JAGS jagres &lt;- jags.model(&#39;JAGS/BUGSmod1.R&#39;,data = dat, n.chains = t.n.chains, inits = inits, n.adapt = t.n.burnin) params &lt;- c(&quot;meffh&quot;, &quot;meffm&quot;, &quot;heffM&quot;, &quot;heffCS&quot;, &quot;meffCS&quot;) mod &lt;- coda.samples(jagres, params, n.iter=t.n.iter, thin=t.n.thin) res &lt;- round(data.frame(summary(mod)$quantiles[, c(3, 1, 5)]), 3) res$TRUEVALUE &lt;- c(heffCS, heffM, meffCS, meffh1, meffh2, meffm1, meffm2) res # Traceplots post &lt;- data.frame(rbind(mod[[1]], mod[[2]])) names(post) &lt;- dimnames(mod[[1]])[[2]] par(mfrow = c(3,3)) param &lt;- c(&quot;meffh[1]&quot;, &quot;meffh[2]&quot;, &quot;meffm[1]&quot;, &quot;meffm[2]&quot;, &quot;heffM&quot;, &quot;heffCS&quot;, &quot;meffCS&quot;) traceplot(mod[, match(param, names(post))]) "],
["PART-III.html", "13 Introduction to PART III 13.1 Model notations", " 13 Introduction to PART III This part is a collection of more complicated ecological models to analyse data that may not be analysed with the traditional linear models that we covered in PART I of this book. 13.1 Model notations It is unavoidable that different authors use different notations for the same thing, or that the same notation is used for different things. We try to use, whenever possible, notations that is commonly used at the International Statistical Ecology Congress ISEC. Resulting from an earlier ISEC, Thomson et al. (2009) give guidelines on what letter should be used for which parameter in order to achieve a standard notation at least among people working with classical mark-recapture models. However, the alphabet has fewer letters compared to the number of ecological parameters. Therefore, the same letter cannot stand for the same parameter across all papers, books and chapters. Here, we try to use the same letter for the same parameter within the same chapter. "],
["cjs-with-mix.html", "14 Fränzi Modell 14.1 Introduction", " 14 Fränzi Modell 14.1 Introduction "],
["referenzen.html", "Referenzen", " Referenzen "]
]
