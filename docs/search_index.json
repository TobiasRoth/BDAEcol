[
["index.html", "Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and Stan Preface Acknowledgments", " Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and Stan Fränzi Korner-Nievergelt, Tobias Roth, Stefanie von Felten, Jérôme Guéla, Bettina Almasi and Pius Korner-Nievergelt 2018-01-28 Preface Acknowledgments The basis of this book is a course script written for statistics classes at the International Max Planck Research School for Organismal Biology (IMPRS)dsee www.orn.mpg.de/2453/Short_portrait. We, therefore, sincerely thank all the IMPRS students who have used the script and worked with us. The text grew as a result of questions and problems that appeared during the application of linear models to the various Ph.D. projects of the IMPRS stu- dents. Their enthusiasm in analyzing data and discussion of their problems motivated us to write this book, with the hope that it will be of help to future students. We especially thank Daniel Piechowski and Margrit Hieber-Ruiz for hiring us to give the courses at the IMPRS. The main part of the book was written by FK and PK during time spent at the Colorado Cooperative Fish and Wildlife Research Unit and the Department of Fish, Wildlife, and Conservation Biology at Colorado State University in the spring of 2014. Here, we were kindly hosted and experienced a motivating time. William Kendall made this possible, for which we are very grateful. Gabriele Engeler and Joyce Pratt managed the administrational challenges of tenure there and made us feel at home. Allison Level kindly introduced us to the CSU library system, which we used extensively while writing this book. We enjoyed a very inspiring environment and cordially thank all the Fish, Wildlife, and Conservation Biology staff and students who we met during our stay. The companies and institutions at which the authors were employed during the work on the book always positively supported the project, even when it produced delays in other projects. We are grateful to all our colleagues at the Swiss Ornithological Institute (www.vogelwarte.ch), oikostat GmbH (www.oikostat.ch), Hintermann &amp; Weber AG (www.hintermannweber.ch), the University of Basel, and the Clinical Trial Unit at the University of Basel (www.scto.ch/en/CTU-Network/CTU-Basel.html). We are very grateful to the R Development Core Team (http://www. r-project.org/contributors.html) for providing and maintaining this wonderful software and network tool. We appreciate the flexibility and understandability of the language R and the possibilitiy to easily exchange code. Similarly, we would like to thank the developers of BUGS (http://www.openbugs.net/w/ BugsDev) and Stan (http://mc-stan.org/team.html) for making all their extremely useful software freely available. Coding BUGS or Stan has helped us in many cases to think more clearly about the biological processes that have generated our data. Example data were kindly provided by the Ulmet-Kommission (www.bnv. ch), the Landschaft und Gewa ̈sser of Kanton Aargau, the Swiss Ornithological Institute (www.vogelwarte.ch), Valentin Amrhein, Anja Bock, Christoph Bu ̈hler, Karl-Heinz Clever, Thomas Gottschalk, Martin Gru ̈ebler, Gu ̈nther Herbert, Thomas Hoffmeister, Rainer Holler, Beat Naef-Daenzer, Werner Peter, Luc Schifferli, Udo Seum, Maris Strazds, and Jean-Luc Zollinger. For comments on the manuscript we thank Martin Bulla, Kim Meichtry- Stier and Marco Perrig. We also thank Roey Angel, Karin Boos, Paul Conn, and two anonymous reviewers for many valuable suggestions regarding the book’s structure and details in the text. Holger Schielzeth gave valuable comments and input for Chapter 10, and David Anderson and Michael Schaub commented on Chapter 11. Bob Carpenter figured out essential parts of the Stan code for the CormackeJollyeSeber model. Michael Betancourt and Bob Carpenter commented on the introduction to MCMC and the Stan examples. Valentin Amrhein and Barbara Helm provided input for Chapter 17. All these people greatly improved the quality of the book, made the text more accessible, and helped reduce the error rate. Finally, we are extremely thankful for the tremendous work that Kate Huyvaert did proofreading our English. "],
["about.html", "Kapitel 1 Why do we Need Statistical Models and What is this Book About? 1.1 WHY WE NEED STATISTICAL MODELS 1.2 WHAT THIS BOOK IS ABOUT FURTHER READING", " Kapitel 1 Why do we Need Statistical Models and What is this Book About? 1.1 WHY WE NEED STATISTICAL MODELS There are at least four main reasons why statistical models are used: (1) models help to describe how we think a system works, (2) data can be summarized using models, (3) comparison of model predictions with data helps with understanding the system, and (4) models allow for predictions, including the quantification of their uncertainty, and, therefore, they help with making decisions. A statistical model is a mathematical construct based on probability theory that aims to reconstruct the system or the process under study; the data are observations of this system or process. When we speak of “models” in this book, we always mean statistical models. Models express what we know (or, better, what we think we know) about a natural system. The difference between the model and the observations shows that what we think about the system may still not be realistic and, therefore, points out what we may want to think about more intensively. In this way, statistical models help with understanding natural systems. Analyzing data using statistical models is rarely just applying one model to the data and extracting the results. Rather, it is an iterative process of fitting a model, comparing the model with the data, gaining insight into the system from specific discrepancies between the model and the data, and then finding a more realistic model. Analyzing data using statistical models is a learning process. Reality is usually too complex to be perfectly represented by a model. Thus, no model is perfect, but a good model is useful (e.g., Box, 1979). Often, several models may be plausible and fit the data reasonably well. In such cases, the inference can be based on the set of all models, or a model that performs best for a specific purpose is selected. In Chapter 11 we have compiled a number of approaches we found useful for model comparisons and multimodel inference. Once we have one or several models, we want to draw inferences from the model(s). Estimates of the effects of the predictor variables on the outcome variables, fitted values, or derived quantities that are of biological interest are extracted, together with an uncertainty estimate. In this book we use, except in one example, Bayesian methods to assess uncertainty of the estimates. Models summarize data. When we have measured the height of 100 trees in a forest and we would like to report these heights to colleagues, we report the mean and the standard deviation instead of reporting all 100 values. The mean and the standard deviation, together with a distributional assumption (e.g., the normal distribution) represent a statistical model that describes the data. We do not need to report all 100 values because the 2 values (mean and standard deviation) describe the distribution of the 100 values sufficiently well so that people have a picture of the heights of the 100 trees. With increasing complexity of the data, we need more complex models that summarize the data in a sensible way. Statistical models are widely applied because they allow for quantifying uncertainty and making predictions. A well-known application of statistical models is the weather forecast. Additional examples include the prediction of bird or bat collision risks at wind energy turbines based on some covariates, the avalanche bulletins, or all the models used to predict changes of an ecosystem when temperatures rise. Political decisions are often based on models or model predictions. Models are pervasive; they even govern our daily life. For example, we first expected our children to get home before 3:30 p.m. because we knew that the school bus drops them off at 3:24, and a child can walk 200 m in around 4 min. What we had in mind was a model child. After some weeks observing the time our children came home after school, we could compare the model prediction with real data. Based on this comparison and short interviews with the children, we included “playing with the neighbor’s dog” in our model and updated the expected arrival time to 3:45 p.m. 1.2 WHAT THIS BOOK IS ABOUT This book is about a broad class of statistical models called linear models. Such models have a systematic part and a stochastic part. The systematic part describes how the outcome variable (y, variable of interest) is related to the predictor variables (x, explanatory variables). This part produces the fitted values that are completely defined by the values of the predictor variables. The stochastic part of the model describes the scatter of the observations around the fitted values using a probability distribution. For example, a regression line is the systematic part of the model, and the scatter of the data around the regression line (more precisely: the distribution of the residuals) is the stochastic part. Linear models are probably the most commonly used models in biology and in many other research areas. Linear models form the basis for many statistical methods such as survival analysis, structural equation analysis, variance components analysis, time-series analysis, and most multivariate techniques. It is of crucial importance to understand linear models when doing quantitative research in biology, agronomy, social sciences, and so on. This book introduces linear models and describes how to fit linear models in R, BUGS, and Stan. The book is written for scientists (particularly organismal biologists and ecologists; many of our examples come from ecology). The number of mathematical formulae is reduced to what we think is essential to correctly interpret model structure and results. Chapter 2 provides some basic information regarding software used in this book, important statistical terms, and how to work with them using the statistical software package R, which is used in most chapters of the book. The linear relationship between the outcome y and the predictor x can be straightforward, as in linear models with normal error distribution (normal linear model, LM, Chapter 4). But the linear relationship can also be indirect via a link function. In this case, the direct linear relationship is between a transformed outcome variable and the predictor variables, and, usually, the model has a nonnormal error distribution such as Poisson or binomial (generalized linear model, GLM, Chapter 8). Generalized linear models can handle outcome variables that are not on a continuous and infinite scale, such as counts and proportions. For some linear models (LM, GLM) the observations are required to be independent of each other. However, this is often not the case, for example, when more than one measurement is taken on the same individual (i.e., repeated measurements) or when several individuals belong to the same nest, farm, or another grouping factor. Such data should be analyzed using mixed models (LMM, GLMM, Chapters 7 and 9); they account for the nonindependence of the observations. Nonindependence of data may also be introduced when observations are made close to each other (in space or time). In Chapter 6 we show how temporal or spatial autocorrelation is detected and we give a few hints about how temporal correlation can be addressed. In Chapter 13, we analyze spatial data using a species distribution example. Chapter 14 contains examples of more complex analyses of ecological data sets. These models should be understandable with the theory learned in the first part of the book. The chapter presents ideas on how the linear model can be expanded to more complex models. The software BUGS and Stan, introduced in Chapter 12, are used to fit these complex models. BUGS and Stan are relatively easy to use and flexible enough to build many specific models. We hope that this chapter motivates biologists and others to build their own models for the particular process they are investigating. Throughout the book, we treat model checking using graphical methods with high importance. Residual analysis is discussed in Chapter 6. Chapter 10 introduces posterior predictive model checking. Posterior predictive model checking is used in Chapter 14 to explore the performance of more complex models such as a zero-inflated and a territory occupancy model. Finally, in Chapter 15, we present possible ways to assess prior sensitivity. The aim of the checklist in Chapter 16 is to guide scientists through a data analysis. It may be used as a look-up table for choosing a type of model depending on the data type, deciding whether to transform variables or not, deciding which test statistic to use in posterior predictive model checking, or understanding what may help when the residual plots do not look as they should. Such look-up tables cannot be general and complete, but the sugges- tions they make can help when starting an analysis. For the reasons explained in Chapter 3, we use Bayesian methods to draw inference from the models throughout the book. However, the book is not a thorough introduction to Bayesian data analysis. We introduce the principles of Bayesian data analysis that we think are important for the application of Bayesian methods. We start simply by producing posterior distributions for the model parameters of linear models fitted in the widely used open source software R (R Core Team, 2014). In the second part of the book, we introduce Markov chain Monte Carlo simulations for non-mathematicians and use the software OpenBUGS (Lunn et al., 2013) and Stan (mc-stan.org). The third part of the book includes, in addition to the data analysis checklist, example text for the presentation of results from a Bayesian data analysis in a paper. We also explain how the methods presented in the book can be described in the methods section of a paper. Hopefully, the book provides a gentle introduction to applied Bayesian data analysis and motivates the reader to deepen and expand knowledge about these techniques, and to apply Bayesian methods in their data analyses. FURTHER READING Gelman and Hill (2007) teach Bayesian data analysis using linear models in a very creative way, with examples from the social and political sciences. Kruschke (2011) gives a thorough and very understandable introduction to Bayesian data analysis. McCarthy (2007) concisely introduces Bayesian methods using WinBUGS. Ke ́ry (2010) gives an introduction to linear models using Bayesian methods with WinBUGS. Stauffer (2008) works practically through common research problems in the life sciences using Bayesian methods. Faraway (2005, 2006) and Fox and Weisberg (2011) provide applied introductions to linear models using frequentist methods in R. Note that there is an extensive erratum to Faraway (2006) on the web. Zuur et al. (2009, 2012) are practical and understandable introductions to linear models in R with a particular focus on complex real ecological data problems such as nonindependent data. Zuur et al. (2012) also introduce Bayesian methods. A more theoretical approach, including R code, is Aitkin et al. (2009). We can also recommend the chapters introducing generalized linear models in Wood (2006). "],
["prerequisites.html", "Kapitel 2 Prerequisites and Vocabulary 2.1 SOFTWARE 2.2 IMPORTANT STATISTICAL TERMS AND HOW TO HANDLE THEM IN R", " Kapitel 2 Prerequisites and Vocabulary 2.1 SOFTWARE In most chapters of this book we work with the statistical software R (R Core Team, 2014). R is a very powerful tool for statistics and graphics in general. However, it is limited with regard to Bayesian methods applied to more complex models. In Part II of the book (Chapters 12e15), we therefore use Open BUGS (www.openbugs.net; Spiegelhalter et al., 2007) and Stan (Stan Development Team, 2014), using specific interfaces to operate them from within R. OpenBUGS and Stan are introduced in Chapter 12. Here, we briefly introduce R. 2.1.1 What Is R? R is a software environment for statistics and graphics that is free in two ways: free to download and free source code (www.r-project.org). The first version of R was written by Robert Gentleman and Ross Ihaka of the University of Auckland (note that both names begin with “R”). Since 1997, R has been governed by a core group of R contributors (www.r-project.org/contributors. html). R is a descendant of the commercial S language and environment that was developed at Bell Laboratories by John Chambers and colleagues. Most code written for S runs in R, too. It is an asset of R that, along with statistical analyses, well-designed publication-quality graphics can be pro- duced. R runs on all operating systems (UNIX, Linux, Mac, Windows). R is different from many statistical software packages that work with menus. R is a programming language or, in fact, a programming environment. This means that we need to write down our commands in the form of R code. While this may need a bit of effort in the beginning, we will soon be able to reap the first fruits. Writing code enforces us to know what we are doing and why we are doing it, and enables us to learn about statistics and the R language rapidly. And because we save the R code of our analyses, they are easily reproduced, comprehensible for colleagues (especially if the code is furnished with comments), or easily adapted and extended to a similar new analysis. Due to its flexibility, R also allows us to write our own functions and to make them available for other users by sharing R code or, even better, by compiling them in an R package. R packages are extensions of the slim basic R distribution, which is supplied with only about eight packages, and typically contain R functions and sometimes also data sets. A steadily increasing number of packages are available from the network of CRAN mirror sites (currently over 5000), accessible at www.r-project.org. Compared to other dynamic, high-level programming languages such as Python (www.python.org) or Julia (Bezanson et al., 2012; www.julialang.org), R will need more time for complex computations on large data sets. However, the aim of R is to provide an intuitive, “easy to use” programming language for data analyses for those who are not computer specialists (Chambers, 2008), thereby trading off computing power and sometimes also precision of the code. For example, R is quite flexible regarding the use of spaces in the code, which is convenient for the user. In contrast, Python and Julia require a stricter coding, which makes the code more precise but also more difficult to learn. Thus, we consider R as the ideal language for many statistical problems faced by ecologists and many other scientists. 2.1.2 Working with R If you are completely new to R, we recommend that you take an introductory course or work through an introductory book or document (see recommen- dations in the Further Reading section at the end of this chapter). R is orga- nized around functions, that is, defined commands that typically require inputs (arguments) and return an output. In what follows, we will explain some important R functions used in this book, without providing a full introduction to R. Moreover, the list of functions explained in this chapter is only a selection and we will come across many other functions in this book. That said, what follows should suffice to give you a jumpstart. We can easily install additional packages by using the function install.packages and load packages by using the function library. Each R function has documentation describing what the function does and how it is used. If the package containing a function is loaded in the current R session, we can open the documentation using ?. Typing ?mean into the R console will open the documentation for the function mean (arithmetic mean). If we are looking for a specific function, we can use the function help.search to search for functions within all installed packages. Typing help. search(&quot;linear model&quot;), will open a list of functions dealing with linear models (together with the package containing them). For example, stats::lm suggests the function lm from the package stats. Shorter, but equivalent to help.search(&quot;linear model&quot;) is ??&quot;linear model&quot;. Alternatively, R’s online documentation can also be accessed with help.start(). Functions/packages that are not installed yet can be found using the specific search menu on www.r-project.org. Once familiar with using the R help and searching the internet efficiently for R-related topics, we can independently broaden our knowledge about R. Note that whenever we show R code in this book, the code is printed in this font. Comments, which are preceded by a hash sign, #, and are therefore not executed by R, are printed in gray R output is printed in blue font. 2.2 IMPORTANT STATISTICAL TERMS AND HOW TO HANDLE THEM IN R 2.2.1 Data Sets, Variables, and Observations Data are always collected on a sample of objects (e.g., animals, plants, or plots). An observation refers to the smallest observational or experimental unit. In fact, this can also be a smaller unit, such as the wing of a bird, a leaf of a plant, or a subplot. Data are collected with regard to certain characteristics (e.g., age, sex, size, weight, level of blood parameters), all of which are called variables. A collection of data, a so-called “data set,” can consist of one or many variables. The term variable illustrates that these characteristics vary between the observations. Variables can be classified in several ways, for instance, by the scale of measurement. We distinguish between nominal, ordinal, and numeric variables (see 2.1). Nominal and ordinal variables can be summarized as cate- gorical variables. Numeric variables can be further classified as discrete or continuous. Moreover, note that categorical variables are often called factors and numeric variables are often called covariates. TABELLE 2.1: Scales of Measurement Scale Examples Properties Typical coding in R Nominal Sex, genotype, habitat Identity (values have a unique meaning) factor() Ordinal Altitudinal zones (e.g., foothill, montane, subalpine, alpine zone) Identity and magnitude (values have an ordered relationship, some values are larger and some are smaller) ordered() Numeric Discrete: counts; continuous: body weight, wing length, speed Identity, magnitude, and equal intervals (units along the scale are equal to each other) and possibly a minimum value of zero (ratios are interpretable) intgeger(); numeric() Now let us look at ways to store and handle data in R. A simple, but probably the most important, data structure is a vector. It is a collection of ordered elements of the same type. We can use the function c to combine these elements, which are automatically coerced to a common type. The type of elements determines the type of the vector. Vectors can (among other things) be used to represent variables. Here are some examples: v1 &lt;- c(1,4,2,8) v2 &lt;- c(&quot;bird&quot;,&quot;bat&quot;,&quot;frog&quot;,&quot;bear&quot;) v3 &lt;- c(1,4,&quot;bird&quot;,&quot;bat&quot;) R is an object-oriented language and vectors are specific types of objects. The class of objects can be obtained by the function class. A vector of numbers (e.g., v1) is a numeric vector (corresponding to a numeric variable); a vector of words (v2) is a character vector (corresponding to a categorical variable). If we mix numbers and words (v3), we will get a character vector. class(v1) ## [1] &quot;numeric&quot; class(v2) ## [1] &quot;character&quot; class(v3) ## [1] &quot;character&quot; The function `rev can be used to reverse the order of elements. rev(v1) ## [1] 8 2 4 1 Numeric vectors can be used in arithmetic expressions, using the usual arithmetic operators +, -, *, and /, including ˆ for raising to a power. The operations are performed element by element. In addition, all of the common arithmetic functions are available (e.g., log and sqrt for the logarithm and the square root). To generate a sequence of numbers, R offers several possibilities. A simple one is the colon operator: 1:30 will produce the sequence 1, 2, 3, …, 30. The function seq is more general: seq(5, 100, by = 5) will produce the sequence 5, 10, 15, …, 100. R also knows logical vectors, which can have the values TRUE or FALSE. We can generate them using conditions defined by the logical operators &lt;, &lt;=, &gt;, &gt;= (less than, less than or equal to, greater than, greater than or equal to), == (exact equality), and != (inequality). The vector will contain TRUE where the condition is met and FALSE if not. We can further use &amp; (inter- section, logical “and”“), | (union, logical”or“), and ! (negation, logical”not“) to combine logical expressions. When logical vectors are used in arithmetic expressions, they are coerced to numeric with FALSE becoming 0 and TRUE becoming 1. Categorical variables should be coded as factors, using the function factor or as.factor. Thereby, the levels of the factor can be coded with characters or with numbers (but the former is often more informative). Ordered categorical variables can be coded as ordered factors by using factor(..., ordered = TRUE) or the function ordered. Other types of vectors include “Date” for date and time variables and “complex”&quot; for complex numbers (not used in this book). Instead of storing variables as individual vectors, we can combine them into a data frame, using the function `data.frame. The function produces an object of the class “data.frame”, which is the most fundamental data structure used for statistical modeling in R. Different types of variables are allowed within a single data frame. Note that most data sets provided in the package blmeco, which accompanies this book, are data frames. Data are often entered and stored in spreadsheet files, such as those produced by Excel or LibreOffice. To work with such data in R, we need to read them into R. This can be done by the function read.table (and its descendants), which reads in data having various file formats (e.g., comma- or tab-delimited text) and generates a data frame object. It is very important to consider the specific structure of a data frame and to use the same layout in the original spreadsheet: a data frame is a data table with observations in rows and variables in columns. The first row contains the header, which contains the names of the variables. This format is standard practice and should be compatible with all other statistical soft- ware packages, too. Now we combine the vectors v1, v2, and v3 created earlier to a data frame called “dat”&quot; and print the result by typing the name of the data frame: dat &lt;- data.frame(v1, v2, v3) dat ## v1 v2 v3 ## 1 1 bird 1 ## 2 4 bat 4 ## 3 2 frog bird ## 4 8 bear bat dat &lt;- data.frame(number = v1, animal = v2, mix = v3) dat ## number animal mix ## 1 1 bird 1 ## 2 4 bat 4 ## 3 2 frog bird ## 4 8 bear bat By default, the names of the vectors are taken as variable names in dat, but we can also give them new names. A useful function to quickly generate a data frame in some situations (e.g., if we have several categorical variables that we want to combine in a full factorial manner) is `expand.grid. We supply a number of vectors (variables) and expand.grid creates a data frame with a row for every combination of elements of the supplied vectors, the first variables varying fastest. For example: dat2 &lt;- expand.grid(number = v1, animal = v2) dat2 ## number animal ## 1 1 bird ## 2 4 bird ## 3 2 bird ## 4 8 bird ## 5 1 bat ## 6 4 bat ## 7 2 bat ## 8 8 bat ## 9 1 frog ## 10 4 frog ## 11 2 frog ## 12 8 frog ## 13 1 bear ## 14 4 bear ## 15 2 bear ## 16 8 bear Using square brackets allows for selecting parts of a vector or data frame, for example, v1[v1 &gt; 3] ## [1] 4 8 dat2[dat2$animal == &quot;bat&quot;,] ## number animal ## 5 1 bat ## 6 4 bat ## 7 2 bat ## 8 8 bat Because `dat2 has two dimensions (rows and columns), we need to provide a selection for each dimension, separated by a comma. Because we want all values along the second dimension (all columns), we do not provide anything after the comma (thereby selecting “all there is”). Now let us have a closer look at the data set “cortbowl” from the package blmeco to better understand the structure of data frame objects and to un- derstand the connection between scale of measurement and the coding of variables in R. We first need to load the package blmeco and then the data set. The function `head is convenient to look at the first six observations of the data frame. library(blmeco) # load the package data(cortbowl) # load the data set head(cortbowl) # show first six observations ## Brood Ring Implant Age days totCort ## 1 301 898331 P 49 20 5.761 ## 2 301 898332 P 29 2 8.418 ## 3 301 898332 P 47 20 8.047 ## 4 301 898333 C 25 2 25.744 ## 5 302 898185 P 57 20 8.041 ## 6 302 898188 C 28 before 6.338 The data frame cortbowl contains data on 151 nestlings of barn owls Tyto alba (identifiable by the variable Ring) of varying age from 54 broods. Each nestling either received a corticosterone implant or a placebo implant (variable Implant with levels C and P). Corticosterone levels (variable totCort) were determined from blood samples taken just before implantation, or 2 or 20 days after implantation (variable days). Each observation (row) refers to one nestling measured on a particular day. Because multiple measurements were taken per nestling and multiple nestlings may belong to the same brood, cortbowl is an example of a hierarchical data set (see 7). The function `str shows the structure of the data frame (of objects in general). str(cortbowl) # show the structure of the data.frame ## &#39;data.frame&#39;: 287 obs. of 6 variables: ## $ Brood : Factor w/ 54 levels &quot;231&quot;,&quot;232&quot;,&quot;233&quot;,..: 7 7 7 7 8 8 9 9 10 10 ... ## $ Ring : Factor w/ 151 levels &quot;898054&quot;,&quot;898055&quot;,..: 44 45 45 46 31 32 9 9 18 19 ... ## $ Implant: Factor w/ 2 levels &quot;C&quot;,&quot;P&quot;: 2 2 2 1 2 1 1 1 2 1 ... ## $ Age : int 49 29 47 25 57 28 35 53 35 31 ... ## $ days : Factor w/ 3 levels &quot;2&quot;,&quot;20&quot;,&quot;before&quot;: 2 1 2 1 2 3 1 2 1 1 ... ## $ totCort: num 5.76 8.42 8.05 25.74 8.04 ... str returns the number of observations (287 in our example) and variables (6), the names and the coding of variables. Note that not all nestlings could be measured on each day, so the data set only contains 287 rows (instead of 151 nestlings 3 days 1⁄4 453). Brood, Ring, and Implant are nominal categorical variables, although numbers are used to name the levels of Brood and Ring. While character vectors such as Implant are by default transformed to factors by the functions data.frame and read.table, numeric vectors are kept numeric. Thus, if a categorical variable is coded with numbers (as are Brood and Ring), it must be explicitly transformed to a factor using the functions factor or `as.factor. Coding as factor ensures that, when used for modeling, these variables are recognized as nominal. However, using words rather than numbers to code factors is good practice to avoid erroneously treating a factor as a numeric variable. The variable days is also coded as factor (with levels “before”, “2”, and “20”). Age is coded as an integer with only whole years recorded, although age is clearly continuous rather than discrete in nature. Counts would be a more typical case of a discrete variable (see Chapter 8). The variable totCort is a continuous numeric variable. Special types of categorical variables are binary variables, with only two categories (e.g., implant and sex, with variables coded as no/yes or 0/1). We often have to choose whether we treat a variable as a factor or as numeric: for example, we may want to use the variable days as a nominal variable if we are mainly interested in differences (e.g., in totCort) between the day before the implantation, day 2, and day 20 after implantation. If we had measured totCort on more than three days, it may be more interesting to use the variable days as numeric (replacing “before” by day 1), to be able to look at the temporal course of totCort. 2.2.2 Distributions and Summary Statistics The values of a variable typically vary. This means they exhibit a certain distribution. Histograms provide a graphical tool to display the shape of distributions. Summary statistics inform us about the distribution of observed values in a sample and allow communication of a lot of information in a few numbers. A statistic is a sample property, that is, it can be calculated from the observations in the sample. In contrast, a parameter is a property of the population from which the sample was taken. As parameters are usually unknown (unless we simulate data from a known distribution), we use sta- tistics to estimate them. Table 2-2 gives an overview of some statistics, given a sample x of size n, \\({x_1, x_2, x_3, ., x_n}\\), ordered with \\(x_1\\) being the smallest and \\(x_n\\) being the largest value, including the corresponding R function (note that the ordering of x is only important for the median). There are different measures of location of a distribution, that is, the value around which most values scatter, and measures of dispersion that describe the spread of the distribution. The most important measure of location is the arithmetic mean (or average). It describes the “center” of symmetric distri- butions (such as the normal distribution). However, it has the disadvantage of being sensitive to extreme values. The median is an alternative measure of location that is generally more appropriate in the case of asymmetric distri- butions; it is not sensitive to extreme values. The median is the central value of the ordered sample (the formula is given in Table 2-2). If n is even, it is the arithmetic mean of the two most central values. ADD TABLE 2.2 The spread of a distribution can be measured by the variance or the standard deviation. The variance of a sample is the sum of the squared deviations from the sample mean over all observations, divided by (n 1). The variance is hard to interpret, as it is usually quite a large number (due to squaring). The standard deviation (SD), which is the square root of the variance, is easier. It is approximately the average deviation of an observation from the sample mean. In the case of a normal distribution, about two thirds of the data are expected within one standard deviation around the mean. Quantiles inform us about both location and spread of a distribution. The p-quantile is the value x with the property that a proportion p of all values are less than or equal to x. The median is the 50% quantile. The 25% quantile and the 75% quantile are also called the lower and upper quartiles, respectively. The range between the 25% and the 75% quartile is called the interquartile range. This range includes 50% of the distribution and is also used as a measure of dispersion. The R function quantile extracts sample quantiles. The median, the quartiles, and the interquartile range can be graphically displayed using box-and-whisker plots (boxplots for short). When we use statistical models, we need to make reasonable assump- tions about the distribution of the variable we aim to explain (outcome or response variable). Statistical models, of which a variety is dealt with in this book, are based on certain parametric distributions. “Parametric” means that these distributions are fully described by a few parameters. The most important parametric distribution used for statistical modeling is the normal distribution, also known as the Gaussian distribution. The Gaussian distribution is introduced more technically in Chapter 5. Qualitatively, it describes, at least approximately, the distribution of observations of any (continuous) variable that tends to cluster around the mean. The impor- tance of the normal distribution is a consequence of the central limit theorem. Without going into detail about this, the practical implications are as follows: The sample mean of any sample of random variables (also if these are themselves not normally distributed), tends to have a normal distribution. The larger the sample size, the better the approximation. The binomial distribution and the Poisson (Chapter 8) distribution can be approximated by the normal distribution under some circumstances. Any variable that is the result of a combination of a large number of small effects (such as phenotypic characteristics that are determined by many genes) tends to show a bell-shaped distribution. This justifies the common use of the normal distribution to describe such data. For the same reason, the normal distribution can be used to describe error variation (residual variance) in linear models. In practice, the error is often the sum of many unobserved processes. If we have a sample of n observations that are normally distributed with mean m and standard deviation s, then it is known that the arithmetic mean x of the sample is normally distributed around m with standard deviation pffiffi SDx 1⁄4 s= n. In practice, however, we do not know s, but estimate it by the sample standard deviation s. Thus, the standard deviation of the sample mean is estimated by the “standard error of the mean” (SEM), which is calculated as pffiffi SEM 1⁄4 s= n. While the standard deviation s ð1⁄4 bsÞ describes the variability of individual observations (Table 2-2), SEM describes the uncertainty about pffiffi the sample mean as an estimate for m. Due to the division by n, SEM is smaller for large samples and larger for small samples. We may wonder about the division by (n 1) in the formula for the sample variance in Table 2-2. This is due to the infamous “degrees of freedom” issue. A quick explanation why we divide by (n 1) instead of n is that we need x, an estimate of the sample mean, to calculate the variance. Using this estimate costs us one degree of freedom, so we divide by n 1. To see why, let us assume we know that the sum of three numbers is 42. Can we tell what the three numbers are? The answer is no because we can freely choose the first and the second number. But the third number is fixed as soon as we know the first and the second: it is 42 (first number þ second number). So the degrees of freedom are 3 1 1⁄4 2 in this case, and this also applies if we know the average of the three numbers instead of their sum. Another explanation is that, because x is estimated from the sample, it is exactly in the middle of the data whereas the true population mean would be a bit off. Thus, the sum of the squared differences, Pni1⁄41 ðx xi Þ2 is a little bit smaller than what it should be (the sum of squared differences is smallest when taken with regard to the sample mean), and dividing by (n 1) instead of n corrects for this. In general, whenever k parameters that are estimated from the data are used in a formula to estimate a new parameter, the degrees of freedom for this estimation are n k (n being the sample size). 2.2.3 More on R Objects Most R functions are applied to one or several objects and produce one or several new objects. For example, the functions data.frame and read.table produce a data frame. Other data structures are offered by the object classes “array” and “list”. An array is an n-dimensional vector. Unlike the different columns of a data frame, all elements of an array need to be of the same type. The object class “matrix” is a special case of an array, one with two dimensions. The function sim, which we introduce in Chapter 3, returns parts of its results in the form of an array. A very useful function to do calculations based on an array (or a matrix) isapply. We simulate a data set to illustrate this: two sites were visited over five years by each of three observers who counted the number of breeding pairs of storks. We simulate the number of pairs by using the function rpois to get random numbers from a Poisson distribution (Chapter 8). We can create a three-dimensional array containing the numbers of stork pairs using site, year, and observer as dimensions. Sites &lt;- c(&quot;Site1&quot;, &quot;Site2&quot;) Years &lt;- 2010:2014 Observers &lt;- c(&quot;Ben&quot;,&quot;Sue&quot;,&quot;Emma&quot;) set.seed(0470) pairs &lt;- rpois(n = 2*5*3, lambda = 10) birds &lt;- array(data = pairs, dim = c(2, 5, 3), dimnames = list(site = Sites, year = Years, observer = Observers)) birds ## , , observer = Ben ## ## year ## site 2010 2011 2012 2013 2014 ## Site1 11 11 12 9 11 ## Site2 10 10 14 15 7 ## ## , , observer = Sue ## ## year ## site 2010 2011 2012 2013 2014 ## Site1 14 5 9 12 5 ## Site2 16 8 9 8 10 ## ## , , observer = Emma ## ## year ## site 2010 2011 2012 2013 2014 ## Site1 12 19 8 8 7 ## Site2 10 13 9 8 11 Using apply, we can easily calculate the sum of pairs per observer (across all sites and years) by choosing MARGIN 1⁄4 3 (for observer) or the mean number of pairs per site and year (averaged over all observers) by choosing MARGIN 1⁄4 c(1,2) for site and year: apply(birds, MARGIN = 3, FUN = sum) ## Ben Sue Emma ## 110 96 105 apply(birds, MARGIN = c(1,2), FUN = mean) ## year ## site 2010 2011 2012 2013 2014 ## Site1 12.33333 11.66667 9.666667 9.666667 7.666667 ## Site2 12.00000 10.33333 10.666667 10.333333 9.333333 Yet another and rather flexible class of object are lists. A list is a more general form of vector that can contain various elements of different types; often these are themselves lists or vectors. Lists are often used to return the results of a computation. For example, the summary of a linear model pro- duced by lm is contained in a list. 2.2.4 R Functions for Graphics R offers a variety of possibilities to produce publication-quality graphics (see recommendations in the Further Reading section at the end of this chapter). In this book we stick to the most basic graphical function plot to create graphics, to which more elements can easily be added. The plot function is a generic function. This means that the action performed depends on the class of arguments given to the function. We can add lines, points, segments, or text to an existing plot by using the functions lines or abline, points, segments, and text, respectively. Let’s look at some simple examples using the data set cortbowl: # to divide the graphics panel in two columns and to set the margin widths par(mfrow = c(1,2), mar = c(4,5,0.5,0), las=1) plot(totCort ~ Implant, data = cortbowl) plot(totCort ~ Age, data = cortbowl[cortbowl$Implant == &quot;P&quot;,]) points(totCort ~ Age, data = cortbowl[cortbowl$Implant == &quot;C&quot;,], pch = 20) ABBILDUNG 2.1: Left: Boxplot of blood corticosterone measurements (totCort) for corticosterone (C) and placebo (P) treated barn owl nestlings. Bold horizontal bar 1⁄4 median; box 1⁄4 interquartile range. The whiskers are drawn from the first or third quartile to the lowest or to the largest value within 1.5 times the interquartile range, respectively. Circles are observations beyond the whiskers. Right: Blood corticosterone measurements (totCort) in relation to age. Open symbols 1⁄4 placebo-treated nestlings, closed symbols 1⁄4 corticosterone-treated nestlings. #--------------------------------------------------------------------------------------------- #2.1.7 Writing own R functions #--------------------------------------------------------------------------------------------- # function to calculate the standard error of the mean sem &lt;- function(x) sd(x)/sqrt(length(x)) # try for example x &lt;- c(10,7,5,9,13,2,20,5) sem(x) ## [1] 1.994971 # however, if we add a missing value to the data, the function produces a missing value too x &lt;- c(x, NA) sem(x) ## [1] NA sd(x) ## [1] NA # improved version sem &lt;- function(x) sd(x, na.rm = TRUE)/sqrt(sum(!is.na(x))) sem(x) ## [1] 1.994971 "],
["bayes.html", "Kapitel 3 The Bayesian and the Frequentist Ways of Analyzing Data 3.1 SHORT HISTORICAL OVERVIEW", " Kapitel 3 The Bayesian and the Frequentist Ways of Analyzing Data 3.1 SHORT HISTORICAL OVERVIEW Reverend Thomas Bayes (1701 or 1702e1761) developed the Bayes theorem. Based on this theorem, he described how to obtain the probability of a hy- pothesis given an observation, that is, data. However, he was so worried whether it would be acceptable to apply his theory to real-world examples that he did not dare to publish it. His methods were only published posthumously (Bayes, 1763). Without the help of computers, Bayes’ methods were appli- cable to just simple problems. "],
["lm.html", "Kapitel 4 Normal Linear Models 4.1 LINEAR REGRESSION", " Kapitel 4 Normal Linear Models 4.1 LINEAR REGRESSION 4.1.1 Background Linear regression is the basis of a large part of applied statistical analysis. Analysis of variance (ANOVA) and analysis of covariance (ANCOVA) can be considered special cases of linear regression, and generalized linear models are extensions of linear regression. We, therefore, start with a rather detailed introduction to linear regression. "],
["likelihood.html", "Kapitel 5 Likelihood 5.1 THEORY", " Kapitel 5 Likelihood 5.1 THEORY As described in Section 3.2, in Bayesian statistics the likelihood is the prob- ability distribution of the data given the model p(yjq), also called the pre- dictive density. In contrast, frequentists use the likelihood as a relative measure of the probability of the data given a specific model (i.e., a model with specified parameter values). Often, we see the notation L(qjy) 1⁄4 p(yjq) for the likelihood of a model. Let’s look at an example. According to values that we found on the internet, black-tailed prairie dogs, Cynomys ludovicianus, weigh on average 1 kg with a standard deviation of 0.2 kg. "],
["modelchecking.html", "Kapitel 6 Assessing Model Assumptions 6.1 MODEL ASSUMPTIONS", " Kapitel 6 Assessing Model Assumptions 6.1 MODEL ASSUMPTIONS Every statistical model makes assumptions. We try to build models that reflect the data-generating process as realistically as possible. However, a model never is the truth. Yet, all inferences drawn from a model, such as estimates of effect size or derived quantities with credible intervals, are based on the assumption that the model is true. However, if a model captures the data- generating process poorly, for example, because it misses important struc- tures (predictors, interactions, polynomials), inferences drawn from the model are probably biased and results become unreliable. In a (hypothetical) model that captures all important structures of the data generating process, the sto- chastic part, the difference between the observation and the fitted value (the residuals), should only show random variation. Analyzing residuals is a very important part of the data analysis process. "],
["lmm.html", "Kapitel 7 Linear Mixed Effects Models 7.1 BACKGROUND", " Kapitel 7 Linear Mixed Effects Models 7.1 BACKGROUND 7.1.1 Why Mixed Effects Models? Mixed effects models (or hierarchical models; see Gelman &amp; Hill, 2007, for a discussion on the terminology) are used to analyze nonindependent, grouped, or hierarchical data. For example, when we measure growth rates of nestlings in different nests by taking mass measurements of each nestling several times during the nestling phase, the measurements are grouped within nestlings (because there are repeated measurements of each) and the nestlings are grouped within nests. Measurements from the same individual are likely to be more similar than measurements from different individuals, and individuals from the same nest are likely to be more similar than nestlings from different nests. Measurements of the same group (here, the “groups” are individuals or nests) are not independent. If the grouping structure of the data is ignored in the model, the residuals do not fulfill the independence assumption. "],
["glm.html", "Kapitel 8 Generalized Linear Models 8.1 BACKGROUND", " Kapitel 8 Generalized Linear Models 8.1 BACKGROUND Up to now, we have dealt with models that assume normally distributed residuals (first row in Figure 8-1). Sometimes the nature of the outcome variable makes it impossible to fulfill this assumption as might occur with binary variables (e.g., alive/dead, a specific behavior occurred/did not occur), proportions (which are confined to be between 0 and 1), or counts that cannot have negative values. For such cases, models for distributions other than the normal distribution are needed; such models are called generalized linear models (GLM). They consist of three elements: the linear predictor, the link function, f, and the error distribution. "],
["glmm.html", "Kapitel 9 Generalized Linear Mixed Models 9.1 BINOMIAL MIXED MODEL", " Kapitel 9 Generalized Linear Mixed Models 9.1 BINOMIAL MIXED MODEL 9.1.1 Background To illustrate the binomial mixed model we have adapted a data set used by Gru ̈ebler et al. (2010) on barn swallow Hirundo rustica nestling survival (we have selected a nonrandom sample to be able to fit a simple model; hence, the results do not add unbiased knowledge about the swallow biology!). For 63 swallow broods, we know the clutch size and the number of the nestlings that fledged. The broods came from 51 farms, thus some of them had more than one brood. There are three predictors measured at the level of the farm: colony size (the number of swallow broods on that farm), cow (whether there are cows on the farm or not), and dung heap (the number of dung heaps, piles of cow dung, within 500 m of the farm). "],
["predictivemodcheck.html", "Kapitel 10 Posterior Predictive Model Checking and Proportion of Explained Variance 10.1 POSTERIOR PREDICTIVE MODEL CHECKING", " Kapitel 10 Posterior Predictive Model Checking and Proportion of Explained Variance 10.1 POSTERIOR PREDICTIVE MODEL CHECKING 10.1.1 Why Mixed Effects Models? Only if the model describes the data-generating process sufficiently accurately can we draw relevant conclusions from the model. It is therefore essential to assess model fit: our goal is to describe how well the model fits the data with respect to different aspects of the model. In this book, we present three ways to assess how well a model reproduces the data-generating process: (1) residual analysis (Chapter 6), (2) posterior predictive model checking (this chapter) and (3) prior sensitivity analysis (Chapter 15). "],
["moddelection.html", "Kapitel 11 Model Selection and Multimodel Inference 11.1 WHEN AND WHY WE SELECT MODELS AND WHY THIS IS DIFFICULT", " Kapitel 11 Model Selection and Multimodel Inference 11.1 WHEN AND WHY WE SELECT MODELS AND WHY THIS IS DIFFICULT Model selection and multimodel inference are delicate topics! During the data analysis process we sometimes come to a point where we have more than one model that adequately describes the data (Chapters 6 and 10), and that are potentially interpretable in a sensible way. The more complex a model is, the better it fits the data and residual plots (Chapter 6) and predictive model checking (Chapter 10) look even better. But, at what point do we want to stop adding complexity? There is no unique answer to this question, except that the choice of a model is central to science, and that this choice may be based on mathematical criteria and/or on expert knowledge (e.g., Gelman &amp; Rubin, 1995, 1999; Anderson, 2008; Claeskens &amp; Hjort, 2008; Link &amp; Barker, 2010; Gelman et al., 2014; and many more). Biologists should build biologically meaningful models based on their experience of the subject. Consequently, thinking about the processes that have generated the data is a central aspect of model selection. "],
["mcmc.html", "Kapitel 12 Markov Chain Monte Carlo Simulation 12.1 BACKGROUND", " Kapitel 12 Markov Chain Monte Carlo Simulation 12.1 BACKGROUND Markov chain Monte Carlo (MCMC) simulation techniques were developed in the mid-1950s by physicists (Metropolis et al., 1953). Later, statisticians discovered MCMC (Hastings, 1970; Geman &amp; Geman, 1984; Tanner &amp; Wong, 1987; Gelfand et al., 1990; Gelfand &amp; Smith, 1990). MCMC methods make it possible to obtain posterior distributions for parameters and latent variables (unobserved variables) of complex models. In parallel, personal computer capacities increased in the 1990s and user-friendly software such as the different programs based on the programming language BUGS (Spiegelhalter et al., 2003) came out. These developments boosted the use of Bayesian data analyses, particularly in genetics and ecology. "],
["spatial.html", "Kapitel 13 Modeling Spatial Data Using GLMM 13.1 BACKGROUND", " Kapitel 13 Modeling Spatial Data Using GLMM 13.1 BACKGROUND 13.1.1 Why Mixed Effects Models? The first law of geography says: “Everything is related to everything else, but near things are more related than distant things” (Tobler, 1970). Statisticians call this phenomenon spatial autocorrelation. It can be seen as a simple 2D generalization of temporal autocorrelation, which describes the tendency of two things nearer to each other along the time axis to be more similar than things further apart in time. Legendre (1993) proposed a more formal defi- nition: “The property of random variables taking values, at pairs of locations a certain distance apart, that are more similar (positive autocorrelation) or less similar (negative autocorrelation) than expected for randomly associated pairs of random observations”. Some common examples of spatial autocorrelation in ecology are patchiness, gradients, or regular distributions. "],
["advancedmodels.html", "Kapitel 14 Advanced Ecological Models 14.1 HIERARCHICAL MULTINOMIAL MODEL TO ANALYZE HABITAT SELECTION USING BUGS", " Kapitel 14 Advanced Ecological Models 14.1 HIERARCHICAL MULTINOMIAL MODEL TO ANALYZE HABITAT SELECTION USING BUGS Categorical response variables are quite common in ecological studies. For example, when studying habitat selection of animals, the outcome variable often is one of several habitat types in which an animal is observed at different time points. Questions can be about whether the animal uses the different habitat types proportional to their availability in its home range, whether the use of the habitat types differs between the sexes or between young and adult animals, or whether the use of habitat types is affected by any covariate such as weather or age of the animal. The statistical methods used to analyze habitat selection with respect to such questions are manifold. They range from simple preference indices to complicated multivariate methods (see overview in Manly et al., 2002) or compositional analysis (Aebischer &amp; Robertson, 1993). "],
["estimability.html", "Kapitel 15 Prior Influence and Parameter Estimability 15.1 HOW TO SPECIFY PRIOR DISTRIBUTIONS", " Kapitel 15 Prior Influence and Parameter Estimability 15.1 HOW TO SPECIFY PRIOR DISTRIBUTIONS In Bayesian data analysis, the prior distribution is an inherent part of the model, as are the error distribution and the link function. It is, therefore, important to choose meaningful prior distributions given the study at hand. "],
["checklist.html", "Kapitel 16 Checklist 16.1 DATA ANALYSIS STEP BY STEP", " Kapitel 16 Checklist 16.1 DATA ANALYSIS STEP BY STEP This checklist can give some guidance for data analysis. However, the list is not complete. For specific studies, a different order of the steps may make more sense or further data structures not considered here may need to be checked. We usually repeat steps 2 to 7 until we find one or a set of models that fit the data well and that are realistic enough to be useful for the intended purpose. Data analysis is always a lot of work and, often, the following steps have to be repeated many times until we find a useful and robust model. There is a danger with this: we may find interesting results that answer different questions than we asked originally. "],
["reportforpaper.html", "Kapitel 17 What Should I Report in a Paper 17.1 HOW TO PRESENT THE RESULTS", " Kapitel 17 What Should I Report in a Paper 17.1 HOW TO PRESENT THE RESULTS Sometimes, we find it helpful to write the results before the methods, espe- cially when data analyses were extensive. Once the results are written it is often easier to distinguish between the important modeling steps that go into the main text from the ones that are supplied in an (electronic) appendix only. "],
["referenzen.html", "Referenzen", " Referenzen "]
]
