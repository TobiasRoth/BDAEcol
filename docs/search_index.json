[
["index.html", "Bayesian Data Analysis in Ecology with R, BUGS, and Stan Preface Acknowledgments", " Bayesian Data Analysis in Ecology with R, BUGS, and Stan Fränzi Korner-Nievergelt, Tobias Roth, Stefanie von Felten, Jérôme Guélat, Bettina Almasi and Pius Korner-Nievergelt 2018-08-10 Preface Acknowledgments "],
["about.html", "1 What is this dynamic e-book about? 1.1 Why did we start this dynamic e-book? 1.2 What is the content of this dynamic e-book? Further reading in Bayesian data analysis", " 1 What is this dynamic e-book about? 1.1 Why did we start this dynamic e-book? In 2015, we wrote a statistics book for Master/PhD level Bayesian data analyses in ecology (F. Korner-Nievergelt et al. 2015). People seemed to like it (e.g. (Harju 2016)). Since then, two parallel processes happen. First, we learn more and we become more confident in what we do, or what we do not, and why we do what we do. Second, several really clever people develop software that broaden the spectrum of ecological models that now easily can be applied by ecologists used to work with R. With this e-book, we open the possibility to add new or substantially revised material. In most of the time, it should be in a state that it can be printed and used together with the book as handout for our stats courses. It is open so that everybody with a GitHub account can make comments and suggestions for improvement. We are looking forward to your contribution! 1.2 What is the content of this dynamic e-book? We do not copy text from the book into the e-book. Therefore, we refer to the book (F. Korner-Nievergelt et al. 2015) for reading about the basic theory on doing Bayesian data analyses using linear models. However, Chapters 1 to 17 of this dynamic e-book correspond to the book chapters. In each chapter, we may provide updated R-codes and/or additional material. The following chapters contain completely new material that we think may be useful for ecologists. Further reading in Bayesian data analysis A really good introductory book to Bayesian data analyses is (McElreath 2016). This book starts with a thorough introduction to applying the Bayes theorem for drawing inference from data. In addition, it carefully discusses what can and what cannot be concluded from statistical results. We like this very much. We like looking up statistical methods in papers and books written by Andrew Gelman (e.g. (Gelman et al. 2014)) and Trevor Hastie (e.g. (T. Hastie, Tibshirani, and Friedman 2009, Efron and Hastie (2016))) because both explain complicated things in a concise and understandable way. For further reading recommendations, see (F. Korner-Nievergelt et al. 2015). "],
["prerequisites.html", "2 Introduction to statistical thinking and working with R 2.1 Notations 2.2 Start with R 2.3 IMPORTANT STATISTICAL TERMS AND HOW TO HANDLE THEM IN R FURTHER READING", " 2 Introduction to statistical thinking and working with R 2.1 Notations It is unavoidable that different authors use different notations for the same thing, or that the same notation is used for different things. We try to use, whenever possible, notations that is commonly used at the International Statistical Ecology Congress ISEC. Resulting from an earlier ISEC, Thomson et al. (2009) give guidelines on what letter should be used for which parameter in order to achieve a standard notation at least among people working with classical mark-recapture models. However, the alphabet has fewer letters compared to the number of ecological parameters. Therefore, the same letter cannot stand for the same parameter across all papers, books and chapters. Here, we try to use the same letter for the same parameter within the same chapter. ab hier ist book copy 2.2 Start with R R is a software environment for statistics and graphics that is free in two ways: free to download and free source code. The first version of R was written by Robert Gentleman and Ross Ihaka of the University of Auckland (note that both names begin with “R”). Since 1997, R has been governed by a core group of R contributors. R is a descendant of the commercial S language and environment that was developed at Bell Laboratories by John Chambers and colleagues. Most code written for S runs in R, too. It is an asset of R that, along with statistical analyses, well-designed publication-quality graphics can be pro- duced. R runs on all operating systems (UNIX, Linux, Mac, Windows). R is different from many statistical software packages that work with menus. R is a programming language or, in fact, a programming environment. This means that we need to write down our commands in the form of R code. While this may need a bit of effort in the beginning, we will soon be able to reap the first fruits. Writing code enforces us to know what we are doing and why we are doing it, and enables us to learn about statistics and the R language rapidly. And because we save the R code of our analyses, they are easily reproduced, comprehensible for colleagues (especially if the code is furnished with comments), or easily adapted and extended to a similar new analysis. Due to its flexibility, R also allows us to write our own functions and to make them available for other users by sharing R code or, even better, by compiling them in an R package. R packages are extensions of the slim basic R distribution, which is supplied with only about eight packages, and typically contain R functions and sometimes also data sets. A steadily increasing number of packages are available from the network of CRAN mirror sites (currently over 5000), accessible at www.r-project.org. Compared to other dynamic, high-level programming languages such as Python or Julia (Bezanson et al., 2012), R will need more time for complex computations on large data sets. However, the aim of R is to provide an intuitive, “easy to use” programming language for data analyses for those who are not computer specialists (Chambers, 2008), thereby trading off computing power and sometimes also precision of the code. For example, R is quite flexible regarding the use of spaces in the code, which is convenient for the user. In contrast, Python and Julia require a stricter coding, which makes the code more precise but also more difficult to learn. Thus, we consider R as the ideal language for many statistical problems faced by ecologists and many other scientists. 2.2.1 Working with R If you are completely new to R, we recommend that you take an introductory course or work through an introductory book or document (see recommen- dations in the Further Reading section at the end of this chapter). R is orga- nized around functions, that is, defined commands that typically require inputs (arguments) and return an output. In what follows, we will explain some important R functions used in this book, without providing a full introduction to R. Moreover, the list of functions explained in this chapter is only a selection and we will come across many other functions in this book. That said, what follows should suffice to give you a jumpstart. We can easily install additional packages by using the function install.packages and load packages by using the function library. Each R function has documentation describing what the function does and how it is used. If the package containing a function is loaded in the current R session, we can open the documentation using ?. Typing ?mean into the R console will open the documentation for the function mean (arithmetic mean). If we are looking for a specific function, we can use the function help.search to search for functions within all installed packages. Typing help. search(&quot;linear model&quot;), will open a list of functions dealing with linear models (together with the package containing them). For example, stats::lm suggests the function lm from the package stats. Shorter, but equivalent to help.search(&quot;linear model&quot;) is ??&quot;linear model&quot;. Alternatively, R’s online documentation can also be accessed with help.start(). Functions/packages that are not installed yet can be found using the specific search menu on www.r-project.org. Once familiar with using the R help and searching the internet efficiently for R-related topics, we can independently broaden our knowledge about R. 2.3 IMPORTANT STATISTICAL TERMS AND HOW TO HANDLE THEM IN R 2.3.1 Data Sets, Variables, and Observations Data are always collected on a sample of objects (e.g., animals, plants, or plots). An observation refers to the smallest observational or experimental unit. In fact, this can also be a smaller unit, such as the wing of a bird, a leaf of a plant, or a subplot. Data are collected with regard to certain characteristics (e.g., age, sex, size, weight, level of blood parameters), all of which are called variables. A collection of data, a so-called “data set,” can consist of one or many variables. The term variable illustrates that these characteristics vary between the observations. Variables can be classified in several ways, for instance, by the scale of measurement. We distinguish between nominal, ordinal, and numeric variables (see 2.1). Nominal and ordinal variables can be summarized as categorical variables. Numeric variables can be further classified as discrete or continuous. Moreover, note that categorical variables are often called factors and numeric variables are often called covariates. Table 2.1: Scales of Measurement Scale Examples Properties Typical coding in R Nominal Sex, genotype, habitat Identity (values have a unique meaning) factor() Ordinal Altitudinal zones (e.g., foothill, montane, subalpine, alpine zone) Identity and magnitude (values have an ordered relationship, some values are larger and some are smaller) ordered() Numeric Discrete: counts; continuous: body weight, wing length, speed Identity, magnitude, and equal intervals (units along the scale are equal to each other) and possibly a minimum value of zero (ratios are interpretable) intgeger(); numeric() Now let us look at ways to store and handle data in R. A simple, but probably the most important, data structure is a vector. It is a collection of ordered elements of the same type. We can use the function c to combine these elements, which are automatically coerced to a common type. The type of elements determines the type of the vector. Vectors can (among other things) be used to represent variables. Here are some examples: v1 &lt;- c(1,4,2,8) v2 &lt;- c(&quot;bird&quot;,&quot;bat&quot;,&quot;frog&quot;,&quot;bear&quot;) v3 &lt;- c(1,4,&quot;bird&quot;,&quot;bat&quot;) R is an object-oriented language and vectors are specific types of objects. The class of objects can be obtained by the function class. A vector of numbers (e.g., v1) is a numeric vector (corresponding to a numeric variable); a vector of words (v2) is a character vector (corresponding to a categorical variable). If we mix numbers and words (v3), we will get a character vector. class(v1) ## [1] &quot;numeric&quot; class(v2) ## [1] &quot;character&quot; class(v3) ## [1] &quot;character&quot; The function `rev can be used to reverse the order of elements. rev(v1) ## [1] 8 2 4 1 Numeric vectors can be used in arithmetic expressions, using the usual arithmetic operators +, -, *, and /, including ˆ for raising to a power. The operations are performed element by element. In addition, all of the common arithmetic functions are available (e.g., log and sqrt for the logarithm and the square root). To generate a sequence of numbers, R offers several possibilities. A simple one is the colon operator: 1:30 will produce the sequence 1, 2, 3, …, 30. The function seq is more general: seq(5, 100, by = 5) will produce the sequence 5, 10, 15, …, 100. R also knows logical vectors, which can have the values TRUE or FALSE. We can generate them using conditions defined by the logical operators &lt;, &lt;=, &gt;, &gt;= (less than, less than or equal to, greater than, greater than or equal to), == (exact equality), and != (inequality). The vector will contain TRUE where the condition is met and FALSE if not. We can further use &amp; (inter- section, logical “and”“), | (union, logical”or“), and ! (negation, logical”not“) to combine logical expressions. When logical vectors are used in arithmetic expressions, they are coerced to numeric with FALSE becoming 0 and TRUE becoming 1. Categorical variables should be coded as factors, using the function factor or as.factor. Thereby, the levels of the factor can be coded with characters or with numbers (but the former is often more informative). Ordered categorical variables can be coded as ordered factors by using factor(..., ordered = TRUE) or the function ordered. Other types of vectors include “Date” for date and time variables and “complex”&quot; for complex numbers (not used in this book). Instead of storing variables as individual vectors, we can combine them into a data frame, using the function `data.frame. The function produces an object of the class “data.frame”, which is the most fundamental data structure used for statistical modeling in R. Different types of variables are allowed within a single data frame. Note that most data sets provided in the package blmeco, which accompanies this book, are data frames. Data are often entered and stored in spreadsheet files, such as those produced by Excel or LibreOffice. To work with such data in R, we need to read them into R. This can be done by the function read.table (and its descendants), which reads in data having various file formats (e.g., comma- or tab-delimited text) and generates a data frame object. It is very important to consider the specific structure of a data frame and to use the same layout in the original spreadsheet: a data frame is a data table with observations in rows and variables in columns. The first row contains the header, which contains the names of the variables. This format is standard practice and should be compatible with all other statistical soft- ware packages, too. Now we combine the vectors v1, v2, and v3 created earlier to a data frame called “dat”&quot; and print the result by typing the name of the data frame: dat &lt;- data.frame(v1, v2, v3) dat ## v1 v2 v3 ## 1 1 bird 1 ## 2 4 bat 4 ## 3 2 frog bird ## 4 8 bear bat dat &lt;- data.frame(number = v1, animal = v2, mix = v3) dat ## number animal mix ## 1 1 bird 1 ## 2 4 bat 4 ## 3 2 frog bird ## 4 8 bear bat By default, the names of the vectors are taken as variable names in dat, but we can also give them new names. A useful function to quickly generate a data frame in some situations (e.g., if we have several categorical variables that we want to combine in a full factorial manner) is `expand.grid. We supply a number of vectors (variables) and expand.grid creates a data frame with a row for every combination of elements of the supplied vectors, the first variables varying fastest. For example: dat2 &lt;- expand.grid(number = v1, animal = v2) dat2 ## number animal ## 1 1 bird ## 2 4 bird ## 3 2 bird ## 4 8 bird ## 5 1 bat ## 6 4 bat ## 7 2 bat ## 8 8 bat ## 9 1 frog ## 10 4 frog ## 11 2 frog ## 12 8 frog ## 13 1 bear ## 14 4 bear ## 15 2 bear ## 16 8 bear Using square brackets allows for selecting parts of a vector or data frame, for example, v1[v1 &gt; 3] ## [1] 4 8 dat2[dat2$animal == &quot;bat&quot;,] ## number animal ## 5 1 bat ## 6 4 bat ## 7 2 bat ## 8 8 bat Because `dat2 has two dimensions (rows and columns), we need to provide a selection for each dimension, separated by a comma. Because we want all values along the second dimension (all columns), we do not provide anything after the comma (thereby selecting “all there is”). Now let us have a closer look at the data set “cortbowl” from the package blmeco to better understand the structure of data frame objects and to un- derstand the connection between scale of measurement and the coding of variables in R. We first need to load the package blmeco and then the data set. The function `head is convenient to look at the first six observations of the data frame. library(blmeco) # load the package data(cortbowl) # load the data set head(cortbowl) # show first six observations ## Brood Ring Implant Age days totCort ## 1 301 898331 P 49 20 5.761 ## 2 301 898332 P 29 2 8.418 ## 3 301 898332 P 47 20 8.047 ## 4 301 898333 C 25 2 25.744 ## 5 302 898185 P 57 20 8.041 ## 6 302 898188 C 28 before 6.338 The data frame cortbowl contains data on 151 nestlings of barn owls Tyto alba (identifiable by the variable Ring) of varying age from 54 broods. Each nestling either received a corticosterone implant or a placebo implant (variable Implant with levels C and P). Corticosterone levels (variable totCort) were determined from blood samples taken just before implantation, or 2 or 20 days after implantation (variable days). Each observation (row) refers to one nestling measured on a particular day. Because multiple measurements were taken per nestling and multiple nestlings may belong to the same brood, cortbowl is an example of a hierarchical data set (see 7). The function `str shows the structure of the data frame (of objects in general). str(cortbowl) # show the structure of the data.frame ## &#39;data.frame&#39;: 287 obs. of 6 variables: ## $ Brood : Factor w/ 54 levels &quot;231&quot;,&quot;232&quot;,&quot;233&quot;,..: 7 7 7 7 8 8 9 9 10 10 ... ## $ Ring : Factor w/ 151 levels &quot;898054&quot;,&quot;898055&quot;,..: 44 45 45 46 31 32 9 9 18 19 ... ## $ Implant: Factor w/ 2 levels &quot;C&quot;,&quot;P&quot;: 2 2 2 1 2 1 1 1 2 1 ... ## $ Age : int 49 29 47 25 57 28 35 53 35 31 ... ## $ days : Factor w/ 3 levels &quot;2&quot;,&quot;20&quot;,&quot;before&quot;: 2 1 2 1 2 3 1 2 1 1 ... ## $ totCort: num 5.76 8.42 8.05 25.74 8.04 ... str returns the number of observations (287 in our example) and variables (6), the names and the coding of variables. Note that not all nestlings could be measured on each day, so the data set only contains 287 rows (instead of 151 nestlings 3 days 1⁄4 453). Brood, Ring, and Implant are nominal categorical variables, although numbers are used to name the levels of Brood and Ring. While character vectors such as Implant are by default transformed to factors by the functions data.frame and read.table, numeric vectors are kept numeric. Thus, if a categorical variable is coded with numbers (as are Brood and Ring), it must be explicitly transformed to a factor using the functions factor or `as.factor. Coding as factor ensures that, when used for modeling, these variables are recognized as nominal. However, using words rather than numbers to code factors is good practice to avoid erroneously treating a factor as a numeric variable. The variable days is also coded as factor (with levels “before”, “2”, and “20”). Age is coded as an integer with only whole years recorded, although age is clearly continuous rather than discrete in nature. Counts would be a more typical case of a discrete variable (see Chapter 8). The variable totCort is a continuous numeric variable. Special types of categorical variables are binary variables, with only two categories (e.g., implant and sex, with variables coded as no/yes or 0/1). We often have to choose whether we treat a variable as a factor or as numeric: for example, we may want to use the variable days as a nominal variable if we are mainly interested in differences (e.g., in totCort) between the day before the implantation, day 2, and day 20 after implantation. If we had measured totCort on more than three days, it may be more interesting to use the variable days as numeric (replacing “before” by day 1), to be able to look at the temporal course of totCort. 2.3.2 Distributions and Summary Statistics The values of a variable typically vary. This means they exhibit a certain distribution. Histograms provide a graphical tool to display the shape of distributions. Summary statistics inform us about the distribution of observed values in a sample and allow communication of a lot of information in a few numbers. A statistic is a sample property, that is, it can be calculated from the observations in the sample. In contrast, a parameter is a property of the population from which the sample was taken. As parameters are usually unknown (unless we simulate data from a known distribution), we use sta- tistics to estimate them. Table 2-2 gives an overview of some statistics, given a sample x of size n, \\({x_1, x_2, x_3, ., x_n}\\), ordered with \\(x_1\\) being the smallest and \\(x_n\\) being the largest value, including the corresponding R function (note that the ordering of x is only important for the median). There are different measures of location of a distribution, that is, the value around which most values scatter, and measures of dispersion that describe the spread of the distribution. The most important measure of location is the arithmetic mean (or average). It describes the “center” of symmetric distri- butions (such as the normal distribution). However, it has the disadvantage of being sensitive to extreme values. The median is an alternative measure of location that is generally more appropriate in the case of asymmetric distri- butions; it is not sensitive to extreme values. The median is the central value of the ordered sample (the formula is given in Table 2-2). If n is even, it is the arithmetic mean of the two most central values. ADD TABLE 2.2 The spread of a distribution can be measured by the variance or the standard deviation. The variance of a sample is the sum of the squared deviations from the sample mean over all observations, divided by (n 1). The variance is hard to interpret, as it is usually quite a large number (due to squaring). The standard deviation (SD), which is the square root of the variance, is easier. It is approximately the average deviation of an observation from the sample mean. In the case of a normal distribution, about two thirds of the data are expected within one standard deviation around the mean. Quantiles inform us about both location and spread of a distribution. The p-quantile is the value x with the property that a proportion p of all values are less than or equal to x. The median is the 50% quantile. The 25% quantile and the 75% quantile are also called the lower and upper quartiles, respectively. The range between the 25% and the 75% quartile is called the interquartile range. This range includes 50% of the distribution and is also used as a measure of dispersion. The R function quantile extracts sample quantiles. The median, the quartiles, and the interquartile range can be graphically displayed using box-and-whisker plots (boxplots for short). When we use statistical models, we need to make reasonable assump- tions about the distribution of the variable we aim to explain (outcome or response variable). Statistical models, of which a variety is dealt with in this book, are based on certain parametric distributions. “Parametric” means that these distributions are fully described by a few parameters. The most important parametric distribution used for statistical modeling is the normal distribution, also known as the Gaussian distribution. The Gaussian distribution is introduced more technically in Chapter 5. Qualitatively, it describes, at least approximately, the distribution of observations of any (continuous) variable that tends to cluster around the mean. The impor- tance of the normal distribution is a consequence of the central limit theorem. Without going into detail about this, the practical implications are as follows: The sample mean of any sample of random variables (also if these are themselves not normally distributed), tends to have a normal distribution. The larger the sample size, the better the approximation. The binomial distribution and the Poisson (Chapter 8) distribution can be approximated by the normal distribution under some circumstances. Any variable that is the result of a combination of a large number of small effects (such as phenotypic characteristics that are determined by many genes) tends to show a bell-shaped distribution. This justifies the common use of the normal distribution to describe such data. For the same reason, the normal distribution can be used to describe error variation (residual variance) in linear models. In practice, the error is often the sum of many unobserved processes. If we have a sample of n observations that are normally distributed with mean m and standard deviation s, then it is known that the arithmetic mean x of the sample is normally distributed around m with standard deviation pffiffi SDx 1⁄4 s= n. In practice, however, we do not know s, but estimate it by the sample standard deviation s. Thus, the standard deviation of the sample mean is estimated by the “standard error of the mean” (SEM), which is calculated as pffiffi SEM 1⁄4 s= n. While the standard deviation s ð1⁄4 bsÞ describes the variability of individual observations (Table 2-2), SEM describes the uncertainty about pffiffi the sample mean as an estimate for m. Due to the division by n, SEM is smaller for large samples and larger for small samples. We may wonder about the division by (n 1) in the formula for the sample variance in Table 2-2. This is due to the infamous “degrees of freedom” issue. A quick explanation why we divide by (n 1) instead of n is that we need x, an estimate of the sample mean, to calculate the variance. Using this estimate costs us one degree of freedom, so we divide by n 1. To see why, let us assume we know that the sum of three numbers is 42. Can we tell what the three numbers are? The answer is no because we can freely choose the first and the second number. But the third number is fixed as soon as we know the first and the second: it is 42 (first number þ second number). So the degrees of freedom are 3 1 1⁄4 2 in this case, and this also applies if we know the average of the three numbers instead of their sum. Another explanation is that, because x is estimated from the sample, it is exactly in the middle of the data whereas the true population mean would be a bit off. Thus, the sum of the squared differences, Pni1⁄41 ðx xi Þ2 is a little bit smaller than what it should be (the sum of squared differences is smallest when taken with regard to the sample mean), and dividing by (n 1) instead of n corrects for this. In general, whenever k parameters that are estimated from the data are used in a formula to estimate a new parameter, the degrees of freedom for this estimation are n k (n being the sample size). 2.3.3 More on R Objects Most R functions are applied to one or several objects and produce one or several new objects. For example, the functions data.frame and read.table produce a data frame. Other data structures are offered by the object classes “array” and “list”. An array is an n-dimensional vector. Unlike the different columns of a data frame, all elements of an array need to be of the same type. The object class “matrix” is a special case of an array, one with two dimensions. The function sim, which we introduce in Chapter 3, returns parts of its results in the form of an array. A very useful function to do calculations based on an array (or a matrix) is apply. We simulate a data set to illustrate this: two sites were visited over five years by each of three observers who counted the number of breeding pairs of storks. We simulate the number of pairs by using the function rpois to get random numbers from a Poisson distribution (Chapter 8). We can create a three-dimensional array containing the numbers of stork pairs using site, year, and observer as dimensions. Sites &lt;- c(&quot;Site1&quot;, &quot;Site2&quot;) Years &lt;- 2010:2014 Observers &lt;- c(&quot;Ben&quot;,&quot;Sue&quot;,&quot;Emma&quot;) set.seed(0470) pairs &lt;- rpois(n = 2*5*3, lambda = 10) birds &lt;- array(data = pairs, dim = c(2, 5, 3), dimnames = list(site = Sites, year = Years, observer = Observers)) birds ## , , observer = Ben ## ## year ## site 2010 2011 2012 2013 2014 ## Site1 11 11 12 9 11 ## Site2 10 10 14 15 7 ## ## , , observer = Sue ## ## year ## site 2010 2011 2012 2013 2014 ## Site1 14 5 9 12 5 ## Site2 16 8 9 8 10 ## ## , , observer = Emma ## ## year ## site 2010 2011 2012 2013 2014 ## Site1 12 19 8 8 7 ## Site2 10 13 9 8 11 Using apply, we can easily calculate the sum of pairs per observer (across all sites and years) by choosing MARGIN 1⁄4 3 (for observer) or the mean number of pairs per site and year (averaged over all observers) by choosing MARGIN 1⁄4 c(1,2) for site and year: apply(birds, MARGIN = 3, FUN = sum) ## Ben Sue Emma ## 110 96 105 apply(birds, MARGIN = c(1,2), FUN = mean) ## year ## site 2010 2011 2012 2013 2014 ## Site1 12.33333 11.66667 9.666667 9.666667 7.666667 ## Site2 12.00000 10.33333 10.666667 10.333333 9.333333 Yet another and rather flexible class of object are lists. A list is a more general form of vector that can contain various elements of different types; often these are themselves lists or vectors. Lists are often used to return the results of a computation. For example, the summary of a linear model pro- duced by lm is contained in a list. 2.3.4 R Functions for Graphics R offers a variety of possibilities to produce publication-quality graphics (see recommendations in the Further Reading section at the end of this chapter). In this book we stick to the most basic graphical function plot to create graphics, to which more elements can easily be added. The plot function is a generic function. This means that the action performed depends on the class of arguments given to the function. We can add lines, points, segments, or text to an existing plot by using the functions lines or abline, points, segments, and text, respectively. Let’s look at some simple examples using the data set cortbowl: # to divide the graphics panel in two columns and to set the margin widths par(mfrow = c(1,2), mar = c(4,5,0.5,0), las=1) plot(totCort ~ Implant, data = cortbowl) plot(totCort ~ Age, data = cortbowl[cortbowl$Implant == &quot;P&quot;,]) points(totCort ~ Age, data = cortbowl[cortbowl$Implant == &quot;C&quot;,], pch = 20) Figure 2.1: Left: Boxplot of blood corticosterone measurements (totCort) for corticosterone (C) and placebo (P) treated barn owl nestlings. Bold horizontal bar 1⁄4 median; box 1⁄4 interquartile range. The whiskers are drawn from the first or third quartile to the lowest or to the largest value within 1.5 times the interquartile range, respectively. Circles are observations beyond the whiskers. Right: Blood corticosterone measurements (totCort) in relation to age. Open symbols 1⁄4 placebo-treated nestlings, closed symbols 1⁄4 corticosterone-treated nestlings. 2.3.5 Writing Our Own R Functions R allows us to write our own functions. Here we write a function that cal- culates the standard error of the mean (SEM). We define the following function: sem &lt;- function(x) sd(x)/sqrt(length(x)) function(x) means that sem is a function of x, x being the only argument required by the function. The function sd calculates the standard deviation of the sample, length extracts the number of elements of the sample (sample size), and sqrt calculates its square root. We can define a vector x and apply the function sem as follows: x &lt;- c(10,7,5,9,13,2,20,5) sem(x) ## [1] 1.994971 This is a good time to mention the topic of missing values. Biological data sets often have missing values (e.g., due to organisms that died, or failure to take measurements). In R, missing values are coded as NA and need to be treated explicitly. If we add a missing value to x and apply sem again, sem produces NA, because the function sd produces NA: x &lt;- c(x, NA) sem(x) ## [1] NA sd(x) ## [1] NA Unless we specify how to handle missing values, any calculation in R that involves missing values will produce a missing value again. An adapted version of sem could look as follows: sem &lt;- function(x) sd(x, na.rm = TRUE)/sqrt(sum(!is.na(x))) sem(x) ## [1] 1.994971 Despite the missing value we have added to x we get the same result as before. The argument na.rm 1⁄4 TRUE within the function sd causes R to ignore missing values. FURTHER READING If you are interested in an introductory statistics book (on frequentist statistics) that works with R, we recommend Crawley (2005), Crawley (2007), Dalgaard (2008), or Logan (2010). The books by Crawley and Logan use biological examples whereas most of the examples in Daalgard (2008) come from the medical sciences. Basic statistical knowledge can be acquired from any classical textbook in statistics. Quinn and Keough (2009) provide a nice introduction to experimental design and data analysis, focusing on the close link between design and analysis. They work through a large number of ecological examples, unfortunately omitting modern mixed-effects models. If you really have a hard time in finding the motivation to read about statistics, you may want to look at Larry Gonick &amp; Woollcott Smith (2005). If you need a more thorough introduction to R, we recommend Venables et al. (2014; downloadable from http://cran.r-project.org/doc/manuals/R-intro.pdf ), Zuur et al. (2009), or Chambers (2008). To learn more about how to generate specialized, publication-quality graphics, you might want to read a book focusing on R graphics such as Murrell (2006) or Chang (2012), or read the reference books on the R graphics packages lattice (Sarkar, 2008) or ggplot (Wickham, 2009). "],
["bayes.html", "3 The Bayesian and the Frequentist Ways of Analyzing Data 3.1 SHORT HISTORICAL OVERVIEW 3.2 THE BAYESIAN WAY 3.3 THE FREQUENTIST WAY", " 3 The Bayesian and the Frequentist Ways of Analyzing Data 3.1 SHORT HISTORICAL OVERVIEW Reverend Thomas Bayes (1701 or 1702e1761) developed the Bayes theorem. Based on this theorem, he described how to obtain the probability of a hy- pothesis given an observation, that is, data. However, he was so worried whether it would be acceptable to apply his theory to real-world examples that he did not dare to publish it. His methods were only published posthumously (Bayes, 1763). Without the help of computers, Bayes’ methods were appli- cable to just simple problems. Much later, the concept of null hypothesis testing was introduced by Ronald A. Fisher (1890e1962) in his book Statistical Methods for Research Workers (Fisher, 1925) and many other publications. Egon Pearson (1895e1980) and others developed the frequentist statistical methods, which are based on the probability of the data given a null hypothesis. These methods are solvable for many simple and some moderately complex examples. The rapidly improving capacity of computers in recent years now enables us to use Bayesian methods also for more (and even very) complex problems using simulation techniques (Smith et al., 1985; Gelfand &amp; Smith, 1990; Gilks et al., 1996). 3.2 THE BAYESIAN WAY Bayesian methods use Bayes’ theorem to update prior knowledge about a parameter with information coming from the data to obtain posterior knowl- edge. The prior and posterior knowledge are mathematically described by a probability distribution (prior and posterior distributions of the parameter). Bayes’ theorem for discrete events says that the probability of event A given event B has occurred, P(A|B), equals the probability of event A, P(A), times the probability of event B conditional on A, P(B|A), divided by the probability of event B, P(B): \\[\\begin{align} P(A|B) = \\frac{P(A)P(B|A)}{P(B)} \\tag{3.1} \\end{align}\\] When using Bayes’ theorem for drawing inference from data, we are interested in the probability distribution of one or several parameters, \\(\\theta\\) (called “theta”), after having looked at the data y, that is, the posterior distribution, \\(p(\\theta|y)\\). To this end, Bayes’ theorem (3.1) is reformulated for continuous parameters using probability distributions rather than probabilities for discrete events: \\[\\begin{align} p(\\theta|y) = \\frac{p(\\theta)p(y|\\theta)}{p(y)} \\tag{3.2} \\end{align}\\] The posterior distribution, \\(p(\\theta|y)\\), describes what we know about the parameter (or about the set of parameters), \\(\\theta\\), after having looked at the data and given the prior knowledge and the model. The prior distribution of \\(\\theta\\), \\(p(\\theta)\\), describes what we know about \\(\\theta\\) before having looked at the data. This is often very little but it can include information from earlier studies. The probability of the data conditional on \\(\\theta\\), \\(p(y|\\theta)\\), is called likelihood. The word likelihood is used in Bayesian statistics with a slightly different meaning than it is used in frequentist statistics. The frequentist likelihood, \\(L(\\theta|y)\\), is a relative measure for the probability of the observed data given specific parameter values. The likelihood is a number often close to zero (see also Chapter 5). In contrast, Bayesians use likelihood for the density distribution of the data conditional on the parameters of a model. Thus, in Bayesian statistics the likelihood is a distribution (i.e., the area under the curve is 1) whereas in frequentist statistics it is a scalar. The prior probability of the data, \\(p(y)\\), equals the integral of \\(p(y|\\theta)p(\\theta)\\) over all possible values of \\(\\theta\\); thus \\(p(y)\\) is a constant. The integral can be solved numerically only for a few simple cases. For this reason, Bayesian statistics were not widely applied before the computer age. Nowadays, a variety of different simulation algorithms exist that allow sam- pling from distributions that are only known to proportionality (Gilks et al., 1996; Bre ́maud, 1999). Dropping the term p(y) in Equation(3.2) leads to a term that is proportional to the posterior distribution: \\(p(\\theta|y) \\propto p(\\theta)p(y|\\theta)\\). Simulation algorithms such as Markov chain Monte Carlo simulation (MCMC) can, therefore, sample from the posterior distribution without having to know \\(p(y)\\). A large enough sample of the posterior distribution can then be used to draw inference about the parameters of interest. 3.2.1 Estimating the Mean of a Normal Distribution with a Known Variance The purpose of this section is to illustrate the Bayesian method using a theoretical example. It has only limited practical value. Therefore, feel free to skip this chapter if you are afraid of mathematical meditations. One of the simplest examples is to estimate the mean of a normal distribution with known variance based on a sample of n measurements. The model of the data is \\(y \\sim Norm(\\theta, \\sigma^2)\\), which means “y is normally distributed with mean \\(\\theta\\) and variance \\(\\sigma^2\\)”. \\(\\Theta\\) is the true (population) mean and \\(\\sigma^2\\) the variance of the population from which the data are a random sample. Given the data contain three measurements, y1 = 27.1, y2 = 14.6, y3 = 14.6, and we know that the variance, \\(\\sigma^2\\), is 20, what do we know about the mean \\(\\theta\\) of the population from which the data are a random sample? Based on Bayes’ theorem and the normal prior distribution it is possible to see that the posterior distribution of the mean \\(\\theta\\) is itself a normal distribution with mean \\(\\mu_n\\) and variance \\(\\sigma_n\\), \\(p(\\theta|y) \\sim Norm(\\mu_n, \\sigma_n)\\), where \\[\\begin{align} \\mu_n = \\frac{\\frac{\\mu_o}{\\sigma^2_0}+\\frac{n\\overline{y}}{\\sigma^2}}{\\frac{1}{\\sigma^2_0}+\\frac{n}{\\sigma^2}}\\text{, and } \\frac{1}{\\sigma2_n}=\\frac{1}{\\sigma^2_0}+\\frac{n}{\\sigma^2}. \\end{align}\\] \\(\\mu_0\\) and \\(\\sigma^2_0\\) are the mean and the variance of the prior distribution for \\(\\theta\\). Gelman et al. (2014) provide the derivation of these formulas. When we know in advance that m cannot be very far from 0, but we have very little knowledge about \\(\\theta\\), we might assume a flat normal distribution around 0 – for example, \\(Norm(0, \\sigma^2_0 = 200)\\) – as the prior distribution for \\(\\theta\\) (the dotted line in Figure 3.1; only part of the right tail is visible). This results in a posterior distribution $p(q |y) Norm(18.2, ^2_0 = 6.5) (the solid line in Figure 3.1). This posterior distribution expresses what we know about q based on our prior knowledge, the data, and assuming our model adequately describes the process that generated the data (the data and the model are formalized in the likelihood, which, in this example, is a normal distribution with mean equal to the arithmetic mean of the data and variance equal to 20, the dashed line in Figure 3.1). Figure 3.1: Prior distribution, likelihood, and posterior distribution of the mean q. The plot has been drawn using the R function triplot.normal.knownvariance provided in the R package blmeco. The posterior distribution is a combination of the information of the data and the prior; the data and the prior are weighted according to the information they contain. This information content is measured by the precision (which is equal to the inverse of the variance). If we use a completely flat (i.e., non- informative) prior distribution, the posterior distribution equals the likelihood (upper left panel in Figure 3.2). In this case the inference drawn does not differ from the inference drawn with frequentist methods. The more we know a priori about q, the stronger the influence the prior has on the posterior (Figure 3.2). When informative prior distributions are used, the inference drawn with Bayesian methods differ from the ones drawn with frequentist methods (instead of saying “prior distribution” and “posterior distribution” one often only says “prior” and “posterior”). Note that the likelihood is the same in all panels of Figure 3.2 because the same data set and the same model is used. Figure 3.2: Prior, likelihood, and posterior distribution of the mean \\(\\theta\\) using different prior distributions for \\(\\theta\\). 3.2.2 Estimating Mean and Variance of a Normal Distribution Using Simulation The estimation of a mean $and a variance \\(\\sigma^2\\) of a normal distribution, where both parameters are unknown, is more complicated than estimating the mean only, because the posterior distribution is two-dimensional. Such a posterior distribution is called a joint posterior distribution. Nevertheless, it is still possible to obtain the joint posterior distribution analytically (Albert, 2007; Gelman et al., 2014). Rather than following the analytical route, we now demonstrate how we can simulate a posterior distribution using the sim function in the package arm. First, we obtain parameter estimates using classical methods such as least-squares or maximum likelihood. Then, sim uses the results from the model fit to calculate the posterior distribution assuming flat prior distributions (Gelman and Hill, 2007). Because the rather complicated formula of the joint posterior distribution of the model parameters is not of much help for many users (such as us biologists), sim samples pairs of random values of $and ^2 from this distribution. iven enough random samples, uncertainty measurements for each model parameter can be obtained. Often we calculate an interval within which we expect the true parameter value to be with a probability of 0.95, the so-called 95% credible interval (CrI). Note that the frequentist confidence interval does not allow such a straightforward interpretation (see Section 3.3). Credible intervals can be defined in different ways: two commonly used CrIs are the symmetric CrI and the highest posterior density interval. The symmetric 95% CrI is the interval between the 2.5% and 97.5% quantiles of the posterior distribution. For skewed posterior distributions, the density values at the 2.5% and 97.5% quantiles can be very different. The highest posterior density 95% interval is a 95% interval that is chosen so that, for any value within the interval, the density function is equal to or higher than for any value outside the interval. Let’s use a simple example. What is the mean height of humans? A random sample of 10 people was drawn and their height measured. A normal model with unknown mean and unknown variance, \\(y \\sim Norm(\\theta, \\sigma^2)\\), can be assumed for human height, and the model is fitted to the data by the least-squares method in R using the lm function to get estimates for the mean and standard deviation: # simulate hypothetical body height measurements true.mean &lt;- 165 # population mean true.sd &lt;- 10 # standard deviation y &lt;- round(rnorm(10, mean=true.mean, sd=true.sd)) mod &lt;- lm(y~1) # least-squares fit mod # least-squares estimate of mean ## ## Call: ## lm(formula = y ~ 1) ## ## Coefficients: ## (Intercept) ## 168.6 summary(mod)$sigma # least-squares estimate of standard deviation ## [1] 8.946756 From the R output we obtain least-squares estimates for \\(\\theta\\) and \\(\\sigma\\). We can describe our data distribution as \\(y \\sim Norm(\\hat{q} = 168.60 \\text{, } \\sigma^2 = 8.95)\\). The hats above the model parameters indicate that these parameters were estimated from data, that is, their true values are unknown. Estimates should always be given together with a measurement of their uncertainty. In Bayesian statistics, the uncertainty is measured by the variance of the posterior distribution \\(p(\\theta,\\sigma|y)\\). It describes which pairs of values of q and s are plausible given the data, the prior, and the model. The function sim draws pairs of values from the joint posterior distribution of the two parameters. library(arm) nsim &lt;- 5000 bsim &lt;- sim(mod, n.sim=nsim) str(bsim) ## Formal class &#39;sim&#39; [package &quot;arm&quot;] with 2 slots ## ..@ coef : num [1:5000, 1] 169 168 166 165 168 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr &quot;(Intercept)&quot; ## ..@ sigma: num [1:5000] 9.78 7.28 7.23 8.22 11.34 ... The function sim produces an object of class “sim” that contains two slots, “coef” (containing 5000 simulated values for \\(\\theta\\)) and “sigma” (containing the 5000 corresponding values for \\(\\sigma\\)). Note that the order matters: the coef and sigma values form pairs of reasonable combinations of \\(\\theta\\) and \\(\\sigma\\) values; we are talking about a joint posterior distribution of the two parameters. A scatterplot allows us to visualize the joint posterior distribution, \\(p(\\theta,\\sigma|y)\\) (lower left panel in Figure 3.3). We can use the simulated values to draw our conclusions (see the following). Figure 3.3: There are 5000 draws from the joint posterior distribution of \\(\\theta\\) and \\(\\sigma\\). Lower left panel: every dot is one draw from the joint posterior distribution of \\(\\theta\\) and \\(\\sigma\\). The histograms in the upper and right panels give the marginal posterior distributions of \\(\\theta\\) and \\(\\sigma\\), respectively. 3.3 THE FREQUENTIST WAY "],
["lm.html", "4 Normal Linear Models 4.1 LINEAR REGRESSION", " 4 Normal Linear Models 4.1 LINEAR REGRESSION 4.1.1 Background Linear regression is the basis of a large part of applied statistical analysis. Analysis of variance (ANOVA) and analysis of covariance (ANCOVA) can be considered special cases of linear regression, and generalized linear models are extensions of linear regression. We, therefore, start with a rather detailed introduction to linear regression. "],
["likelihood.html", "5 Likelihood 5.1 THEORY", " 5 Likelihood 5.1 THEORY As described in Section 3.2, in Bayesian statistics the likelihood is the prob- ability distribution of the data given the model p(yjq), also called the pre- dictive density. In contrast, frequentists use the likelihood as a relative measure of the probability of the data given a specific model (i.e., a model with specified parameter values). Often, we see the notation L(qjy) 1⁄4 p(yjq) for the likelihood of a model. Let’s look at an example. According to values that we found on the internet, black-tailed prairie dogs, Cynomys ludovicianus, weigh on average 1 kg with a standard deviation of 0.2 kg. "],
["modelchecking.html", "6 Assessing Model Assumptions 6.1 MODEL ASSUMPTIONS", " 6 Assessing Model Assumptions 6.1 MODEL ASSUMPTIONS Every statistical model makes assumptions. We try to build models that reflect the data-generating process as realistically as possible. However, a model never is the truth. Yet, all inferences drawn from a model, such as estimates of effect size or derived quantities with credible intervals, are based on the assumption that the model is true. However, if a model captures the data- generating process poorly, for example, because it misses important struc- tures (predictors, interactions, polynomials), inferences drawn from the model are probably biased and results become unreliable. In a (hypothetical) model that captures all important structures of the data generating process, the sto- chastic part, the difference between the observation and the fitted value (the residuals), should only show random variation. Analyzing residuals is a very important part of the data analysis process. "],
["lmm.html", "7 Linear Mixed Effects Models 7.1 BACKGROUND", " 7 Linear Mixed Effects Models 7.1 BACKGROUND 7.1.1 Why Mixed Effects Models? Mixed effects models (or hierarchical models; see Gelman &amp; Hill, 2007, for a discussion on the terminology) are used to analyze nonindependent, grouped, or hierarchical data. For example, when we measure growth rates of nestlings in different nests by taking mass measurements of each nestling several times during the nestling phase, the measurements are grouped within nestlings (because there are repeated measurements of each) and the nestlings are grouped within nests. Measurements from the same individual are likely to be more similar than measurements from different individuals, and individuals from the same nest are likely to be more similar than nestlings from different nests. Measurements of the same group (here, the “groups” are individuals or nests) are not independent. If the grouping structure of the data is ignored in the model, the residuals do not fulfill the independence assumption. Das ist ein Test. "],
["glm.html", "8 Generalized Linear Models 8.1 BACKGROUND", " 8 Generalized Linear Models 8.1 BACKGROUND Up to now, we have dealt with models that assume normally distributed residuals (first row in Figure 8-1). Sometimes the nature of the outcome variable makes it impossible to fulfill this assumption as might occur with binary variables (e.g., alive/dead, a specific behavior occurred/did not occur), proportions (which are confined to be between 0 and 1), or counts that cannot have negative values. For such cases, models for distributions other than the normal distribution are needed; such models are called generalized linear models (GLM). They consist of three elements: the linear predictor, the link function, f, and the error distribution. "],
["glmm.html", "9 Generalized Linear Mixed Models 9.1 Overview of R functions fitting GLMMs and their options:", " 9 Generalized Linear Mixed Models 9.1 Overview of R functions fitting GLMMs and their options: R functions that fit GLMMs: Bürkner 2017 enthält eine schöne Vergleichstabelle Package lme4: glmer greta: fully Bayesian model specification similar to BUGS, uses google translate for doing a gradient based Monte Carlo simulation Hmsc allows including phylogenie and spatial correlation glmmTMB: extremely flexible (spatial, temporal correlation, zero-inflation, error distr. etc.). Not Baysian, but allows drawing random samples from posteriors bmrs(Bürkner 2017) "],
["predictivemodcheck.html", "10 Posterior Predictive Model Checking and Proportion of Explained Variance 10.1 POSTERIOR PREDICTIVE MODEL CHECKING", " 10 Posterior Predictive Model Checking and Proportion of Explained Variance 10.1 POSTERIOR PREDICTIVE MODEL CHECKING 10.1.1 Why Mixed Effects Models? Only if the model describes the data-generating process sufficiently accurately can we draw relevant conclusions from the model. It is therefore essential to assess model fit: our goal is to describe how well the model fits the data with respect to different aspects of the model. In this book, we present three ways to assess how well a model reproduces the data-generating process: (1) residual analysis (Chapter 6), (2) posterior predictive model checking (this chapter) and (3) prior sensitivity analysis (Chapter 15). "],
["moddelection.html", "11 Model Selection and Multimodel Inference 11.1 WHEN AND WHY WE SELECT MODELS AND WHY THIS IS DIFFICULT", " 11 Model Selection and Multimodel Inference 11.1 WHEN AND WHY WE SELECT MODELS AND WHY THIS IS DIFFICULT Model selection and multimodel inference are delicate topics! During the data analysis process we sometimes come to a point where we have more than one model that adequately describes the data (Chapters 6 and 10), and that are potentially interpretable in a sensible way. The more complex a model is, the better it fits the data and residual plots (Chapter 6) and predictive model checking (Chapter 10) look even better. But, at what point do we want to stop adding complexity? There is no unique answer to this question, except that the choice of a model is central to science, and that this choice may be based on mathematical criteria and/or on expert knowledge (e.g., Gelman &amp; Rubin, 1995, 1999; Anderson, 2008; Claeskens &amp; Hjort, 2008; Link &amp; Barker, 2010; Gelman et al., 2014; and many more). Biologists should build biologically meaningful models based on their experience of the subject. Consequently, thinking about the processes that have generated the data is a central aspect of model selection. "],
["mcmc.html", "12 Markov Chain Monte Carlo Simulation 12.1 BACKGROUND", " 12 Markov Chain Monte Carlo Simulation 12.1 BACKGROUND Markov chain Monte Carlo (MCMC) simulation techniques were developed in the mid-1950s by physicists (Metropolis et al., 1953). Later, statisticians discovered MCMC (Hastings, 1970; Geman &amp; Geman, 1984; Tanner &amp; Wong, 1987; Gelfand et al., 1990; Gelfand &amp; Smith, 1990). MCMC methods make it possible to obtain posterior distributions for parameters and latent variables (unobserved variables) of complex models. In parallel, personal computer capacities increased in the 1990s and user-friendly software such as the different programs based on the programming language BUGS (Spiegelhalter et al., 2003) came out. These developments boosted the use of Bayesian data analyses, particularly in genetics and ecology. "],
["spatial.html", "13 Modeling Spatial Data Using GLMM 13.1 BACKGROUND", " 13 Modeling Spatial Data Using GLMM 13.1 BACKGROUND 13.1.1 Why Mixed Effects Models? The first law of geography says: “Everything is related to everything else, but near things are more related than distant things” (Tobler, 1970). Statisticians call this phenomenon spatial autocorrelation. It can be seen as a simple 2D generalization of temporal autocorrelation, which describes the tendency of two things nearer to each other along the time axis to be more similar than things further apart in time. Legendre (1993) proposed a more formal defi- nition: “The property of random variables taking values, at pairs of locations a certain distance apart, that are more similar (positive autocorrelation) or less similar (negative autocorrelation) than expected for randomly associated pairs of random observations”. Some common examples of spatial autocorrelation in ecology are patchiness, gradients, or regular distributions. "],
["advancedmodels.html", "14 Advanced Ecological Models 14.1 HIERARCHICAL MULTINOMIAL MODEL TO ANALYZE HABITAT SELECTION USING BUGS", " 14 Advanced Ecological Models 14.1 HIERARCHICAL MULTINOMIAL MODEL TO ANALYZE HABITAT SELECTION USING BUGS Categorical response variables are quite common in ecological studies. For example, when studying habitat selection of animals, the outcome variable often is one of several habitat types in which an animal is observed at different time points. Questions can be about whether the animal uses the different habitat types proportional to their availability in its home range, whether the use of the habitat types differs between the sexes or between young and adult animals, or whether the use of habitat types is affected by any covariate such as weather or age of the animal. The statistical methods used to analyze habitat selection with respect to such questions are manifold. They range from simple preference indices to complicated multivariate methods (see overview in Manly et al., 2002) or compositional analysis (Aebischer &amp; Robertson, 1993). "],
["estimability.html", "15 Prior Influence and Parameter Estimability 15.1 HOW TO SPECIFY PRIOR DISTRIBUTIONS", " 15 Prior Influence and Parameter Estimability 15.1 HOW TO SPECIFY PRIOR DISTRIBUTIONS In Bayesian data analysis, the prior distribution is an inherent part of the model, as are the error distribution and the link function. It is, therefore, important to choose meaningful prior distributions given the study at hand. "],
["checklist.html", "16 Checklist 16.1 DATA ANALYSIS STEP BY STEP", " 16 Checklist 16.1 DATA ANALYSIS STEP BY STEP This checklist can give some guidance for data analysis. However, the list is not complete. For specific studies, a different order of the steps may make more sense or further data structures not considered here may need to be checked. We usually repeat steps 2 to 7 until we find one or a set of models that fit the data well and that are realistic enough to be useful for the intended purpose. Data analysis is always a lot of work and, often, the following steps have to be repeated many times until we find a useful and robust model. There is a danger with this: we may find interesting results that answer different questions than we asked originally. "],
["reportforpaper.html", "17 What Should I Report in a Paper 17.1 HOW TO PRESENT THE RESULTS", " 17 What Should I Report in a Paper 17.1 HOW TO PRESENT THE RESULTS Sometimes, we find it helpful to write the results before the methods, espe- cially when data analyses were extensive. Once the results are written it is often easier to distinguish between the important modeling steps that go into the main text from the ones that are supplied in an (electronic) appendix only. "],
["referenzen.html", "Referenzen", " Referenzen "]
]
