[
["index.html", "Data Analysis in Ecology with R and Stan Preface Why this book? About this book How to contribute? Acknowledgments", " Data Analysis in Ecology with R and Stan Fränzi Korner-Nievergelt and Tobias Roth 2018-10-04 Preface Why this book? In 2015, we wrote a statistics book for Master/PhD level Bayesian data analyses in ecology (F. Korner-Nievergelt et al. 2015). You can order it here. People seemed to like it (e.g. (Harju 2016)). Since then, two parallel processes happen. First, we learn more and we become more confident in what we do, or what we do not, and why we do what we do. Second, several really clever people develop software that broaden the spectrum of ecological models that now easily can be applied by ecologists used to work with R. With this e-book, we open the possibility to add new or substantially revised material. In most of the time, it should be in a state that it can be printed and used together with the book as handout for our stats courses. About this book We do not copy text from the book into the e-book. Therefore, we refer to the book (F. Korner-Nievergelt et al. 2015) for reading about the basic theory on doing Bayesian data analyses using linear models. However, Chapters 1 to 17 of this dynamic e-book correspond to the book chapters. In each chapter, we may provide updated R-codes and/or additional material. The following chapters contain completely new material that we think may be useful for ecologists. While we show the R-code behind most of the analyses, we sometimes choose not to show all the code in the html version of the book. This is particularly the case for some of the illustrations. An intrested reader can always consult the public GitHub repository with the rmarkdown-files that were used to generate the book. How to contribute? It is open so that everybody with a GitHub account can make comments and suggestions for improvement. Readers can contribute in two ways. One way is to add an issue. The second way is to contribute content directly through the edit button at the top of the page (i.e. a symbol showing a pencil in a square). That button is linked to the rmarkdown source file of each page. You can correct typos or add new text and then submit a GitHub pull request. We try to respond to you as quickly as possible. We are looking forward to your contribution! Acknowledgments We thank Yihui Xie for providing bookdown which makes it much fun to write open books such as ours. "],
["PART-I.html", "1 Introduction to PART I 1.1 Further reading", " 1 Introduction to PART I During our courses we are sometimes asked to give an introduction to some R-related stuff covering data analysis, presentation of results or rather specialist topics in ecology. In this part we present collected these introduction and try to keep them updated. This is also a commented collection of R-code that we documented for our own work. We hope this might be useful olso for other readers. 1.1 Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],
["basics.html", "2 Prerequisits: Basic statistical terms 2.1 Scale of measurement 2.2 Correlations 2.3 Principal components analyses PCA 2.4 Standard deviation and standard error 2.5 Central limit theorem / law of large numbers 2.6 Bayes theorem 2.7 Bayes theorem for continuous parameters 2.8 Single parameter model 2.9 A model with two parameters 2.10 t-distribution 2.11 Frequentist one-sample t-test 2.12 Nullhypothesis test 2.13 Confidence interval 2.14 Posterior distribution 2.15 Posterior probability 2.16 Monte Carlo simulation (parametric bootstrap) 2.17 3 methods for getting the posterior distribution 2.18 Grid approximation 2.19 Monte Carlo simulations 2.20 Comparison of the locations between two groups 2.21 Difference between two means 2.22 Two-sample t-test 2.23 Wilxocon test 2.24 Randomisation test 2.25 Bootstrap 2.26 F-test 2.27 Analysis of variance ANOVA 2.28 Chisquare test 2.29 Bayesian way of analysing correlations between categorical variables 2.30 Summary", " 2 Prerequisits: Basic statistical terms This chapter introduces some important terms useful for doing data analyses. It also introduces the essentials of the classical frequentist tests such as t- and Chisquare test. We will not use them later but we think it is important to know how to interpret the results in order to be able to understand 100 years of scientific literature. For each classical test, we provide a suggestion how to do it in a Bayesian way and we discuss some differences between the Bayesian and frequentist statistics. 2.1 Scale of measurement Scale Examples Properties Coding in R Nominal Sex, genotype, habitat Identity (values have a unique meaning) factor() Ordinal Elevational zones Identity and magnitude (values have an ordered relationship) ordered() Numeric Discrete: counts; continuous: body weight, wing length Identity, magnitude, and equal intervals intgeger() numeric() 2.2 Correlations 2.2.1 Basics of correlations variance \\(\\hat{\\sigma^2} = s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\) standard deviation \\(\\hat{\\sigma} = s = \\sqrt{s^2}\\) covariance \\(q = \\frac{1}{n-1}\\sum_{i=1}^{n}((x_i-\\bar{x})*(y_i-\\bar{y}))\\) 2.2.2 Pearson correlation coefficient standardized covariance \\(r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\) 2.2.3 Spearman correlation coefficient rank correlation correlation between rank(x) and rank(y) robust against outliers 2.2.4 Kendall’s tau rank correlation I = number of pairs (i,k) for which \\((x_i &lt; x_k)\\) &amp; \\((y_i &gt; y_k)\\) or viceversa \\(\\tau = 1-\\frac{4I}{(n(n-1))}\\) 2.3 Principal components analyses PCA rotation of the coordinate system Figure 2.1: Principal components are eigenvectors of the covariance or correlation matrix rotation of the coordinate system so that first component explains most variance second component explains most of the remaining variance and is perpendicular to the first one third component explains most of the remaining variance and is perpendicular to the first two … \\((x,y)\\) becomes \\((pc1, pc2)\\) where \\(pc1_i= b_{11} x_i + b_{12} y_i\\) \\(pc2_i = b_{21} x_i + b_{22} y_i\\) with \\(b_{jk}\\) being loadings pca &lt;- princomp(cbind(x,y), cor=TRUE) loadings(pca) ## ## Loadings: ## Comp.1 Comp.2 ## x 0.707 0.707 ## y 0.707 -0.707 ## ## Comp.1 Comp.2 ## SS loadings 1.0 1.0 ## Proportion Var 0.5 0.5 ## Cumulative Var 0.5 1.0 loadings of a component can be multiplied by -1 proportion of variance explained by each component number of components = number of variables summary(pca) ## Importance of components: ## Comp.1 Comp.2 ## Standard deviation 1.2471218 0.6668488 ## Proportion of Variance 0.7776563 0.2223437 ## Cumulative Proportion 0.7776563 1.0000000 outlook: components with low variance are shrinked to a higher degree in Ridge regression 2.3.1 Inferential statistics there is never a “yes-or-no” answer there will always be uncertainty Amrhein (2017)[https://peerj.com/preprints/26857] The decision whether an effect is important or not cannot not be done based on data alone. For a decision we should carefully consider the consequences of each decision, the aims we would like to achieve and the data. Consequences, needs and wishes of different stakeholders can be formally combined with the information in data by using methods of the decision theory. In most data analyses, particularly in basic research and when working on case studies, we normally do not consider consequences of decisions. In these cases, our job is extracting the information of data so that this information later can be used by other scientists, stakeholders and politicians to make decisions. Therefore, statistics is describing pattern in data and quantifying the uncertainty of the described patterns that is due to the fact that the data is just a (small) random sample from the population we would like to know. Quantification of uncertainty is only possible if the mechanisms under study are known the observations are a random sample from the population of interest Solutions: to 1. working with models and reporting assumptions to 2. study design reported uncertainties always are too small! Example: Number of stats courses before starting a PhD among all PhD students # simulate the virtual true data set.seed(235325) # set seed for random number generator # simulate fake data of the whole population statscourses &lt;- rpois(300000, rgamma(300000, 2, 3)) # draw a random sample from the population n &lt;- 12 # sample size y &lt;- sample(statscourses, 12, replace=FALSE) We observe the sample mean, what do we know about the population mean? Frequentist solution: How would the sample mean scatter, if we repeat the study many times? Bayesian solution: For any possible value, what is the probability that it is the true population mean? 2.4 Standard deviation and standard error frequentist SE = SD/sqrt(n) Bayesian SE = SD of posterior distribution 2.5 Central limit theorem / law of large numbers normal distribution = Gaussian distribution \\(p(\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(\\theta -\\mu)^2) = Normal(\\mu, \\sigma)\\) \\(E(\\theta) = \\mu\\), \\(var(\\theta) = \\sigma^2\\), \\(mode(\\theta) = \\mu\\) 2.6 Bayes theorem \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\) car flowers wine sum no 2 1 3 yes 2 2 4 ——- —————- —————- ——————- sum 4 3 7 What is the probability that the person likes wine given it has no car? \\(P(A) =\\) likes wine \\(= 0.43\\) \\(P(B) =\\) no car \\(= 0.43\\) \\(P(B|A) =\\) proportion car-free people among the wine liker \\(= 0.33\\) Knowing whether a persons owns a car increases the knowledge of the birthday preference. 2.7 Bayes theorem for continuous parameters \\(p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} = \\frac{p(y|\\theta)p(\\theta)}{\\int p(y|\\theta)p(\\theta) d\\theta}\\) \\(p(\\theta|y)\\): posterior distribution \\(p(y|\\theta)\\): likelihood, data model \\(p(\\theta)\\): prior distribution \\(p(y)\\): scaling constant 2.8 Single parameter model \\(p(y|\\theta) = Norm(\\theta, \\sigma)\\), with \\(\\sigma\\) known \\(p(\\theta) = Norm(\\mu_0, \\tau_0)\\) \\(p(\\theta|y) = Norm(\\mu_n, \\tau_n)\\), where \\(\\mu_n= \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau_0^2}+\\frac{n}{\\sigma^2}}\\) and \\(\\frac{1}{\\tau_n^2} = \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\\) \\(\\bar{y}\\) is a sufficient statistics \\(p(\\theta) = Norm(\\mu_0, \\tau_0)\\) is a conjugate prior for \\(p(y|\\theta) = Norm(\\theta, \\sigma)\\), with \\(\\sigma\\) known. Posterior mean = weighted average between prior mean and \\(\\bar{y}\\) with weights equal to the precisions (\\(\\frac{1}{\\tau_0^2}\\) and \\(\\frac{n}{\\sigma^2}\\)) 2.9 A model with two parameters \\(p(y|\\theta, \\sigma) = Norm(\\theta, \\sigma)\\) # weight (g) y &lt;- c(47.5, 43, 43, 44, 48.5, 37.5, 41.5, 45.5) n &lt;- length(y) \\(p(y|\\theta, \\sigma) = Norm(\\theta, \\sigma)\\) \\(p(\\theta, \\sigma) = N-Inv-\\chi^2(\\mu_0, \\sigma_0^2/\\kappa_0; v_0, \\sigma_0^2)\\) conjugate prior \\(p(\\theta,\\sigma|y) = \\frac{p(y|\\theta, \\sigma)p(\\theta, \\sigma)}{p(y)} = N-Inv-\\chi^2(\\mu_n, \\sigma_n^2/\\kappa_n; v_n, \\sigma_n^2)\\), with \\(\\mu_n= \\frac{\\kappa_0}{\\kappa_0+n}\\mu_0 + \\frac{n}{\\kappa_0+n}\\bar{y}\\) \\(\\kappa_n = \\kappa_0+n\\) \\(v_n = v_0 +n\\) \\(v_n\\sigma_n^2=v_0\\sigma_0^2+(n-1)s^2+\\frac{\\kappa_0n}{\\kappa_0+n}(\\bar{y}-\\mu_0)^2\\) \\(\\bar{y}\\) and \\(s^2\\) are sufficient statistics Joint, marginal and conditional posterior distributions 2.10 t-distribution marginal posterior distribution of a normal mean with unknown variance and conjugate prior distribution \\(p(\\theta|v,\\mu,\\sigma) = \\frac{\\Gamma((v+1)/2)}{\\Gamma(v/2)\\sqrt{v\\pi}\\sigma}(1+\\frac{1}{v}(\\frac{\\theta-\\mu}{\\sigma})^2)^{-(v+1)/2}\\) \\(v\\) degrees of freedom \\(\\mu\\) location \\(\\sigma\\) scale 2.11 Frequentist one-sample t-test H0: the mean weight is equal to exactly 40g. \\(t = \\frac{\\bar{y}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\) t.test(y, mu=40) ## ## One Sample t-test ## ## data: y ## t = 3.0951, df = 7, p-value = 0.01744 ## alternative hypothesis: true mean is not equal to 40 ## 95 percent confidence interval: ## 40.89979 46.72521 ## sample estimates: ## mean of x ## 43.8125 2.12 Nullhypothesis test p-value: Probability of the data or more extreme data given the null hypothesis is true. 2.13 Confidence interval # lower limit of 95% CI mean(y) + qt(0.025, df=7)*sd(y)/sqrt(n) # upper limit of 95% CI mean(y) + qt(0.975, df=7)*sd(y)/sqrt(n) 2.14 Posterior distribution Two different theories - one single result! 2.15 Posterior probability Probability \\(P(H:\\mu&lt;=40) =\\) 0.01 2.16 Monte Carlo simulation (parametric bootstrap) Monte Carlo integration: numerical solution of \\(\\int_{-1}^{1.5} F(x) dx\\) sim is solving a mathematical problem by simulation How sim is simulating to get the marginal distribution of \\(\\mu\\): 2.17 3 methods for getting the posterior distribution analytically approximation Monte Carlo simulation 2.18 Grid approximation \\(p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)}\\) For example, one coin flip (Bernoulli model) data: y=0 (a tail) likelihood: \\(p(y|\\theta)=\\theta^y(1-\\theta)^{(1-y)}\\) 2.19 Monte Carlo simulations Markov chain Monte Carlo simulation (BUGS, Jags) Hamiltonian Monte Carlo (Stan) 2.20 Comparison of the locations between two groups Boxplot: Median, 50% box, extremes observation within 1.5 times the interquartile range, outliers The uncertainties of the means do not show the uncertainty of the difference between the means! 2.21 Difference between two means mod &lt;- lm(ell~birthday, data=dat) mod ## ## Call: ## lm(formula = ell ~ birthday, data = dat) ## ## Coefficients: ## (Intercept) birthdaywine ## 37.250 1.083 bsim &lt;- sim(mod, n.sim=nsim) quantile(bsim@coef[,2], prob=c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## -6.938854 1.094108 9.176722 2.22 Two-sample t-test t.test(ell~birthday, data=dat, var.equal=TRUE) ## ## Two Sample t-test ## ## data: ell by birthday ## t = -0.33541, df = 5, p-value = 0.7509 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -9.385932 7.219266 ## sample estimates: ## mean in group flower mean in group wine ## 37.25000 38.33333 2.23 Wilxocon test wilcox.test(ell~birthday, data=dat) ## ## Wilcoxon rank sum test with continuity correction ## ## data: ell by birthday ## W = 6.5, p-value = 1 ## alternative hypothesis: true location shift is not equal to 0 2.24 Randomisation test diffH0 &lt;- numeric(nsim) for(i in 1:nsim){ randbirthday &lt;- sample(dat$birthday) rmod &lt;- lm(ell~randbirthday, data=dat) diffH0[i] &lt;- coef(rmod)[2] } mean(abs(diffH0)&gt;abs(coef(mod)[2])) # p-value ## [1] 0.7094 Produces the distribution of a test statistics given the null hypothesis. assumption: all observations are independent becomes unfeasible when data is structured 2.25 Bootstrap diffboot &lt;- numeric(nsim) for(i in 1:nsim){ nbirthday &lt;- 1 while(nbirthday==1){ bootrows &lt;- sample(1:nrow(dat), replace=TRUE) nbirthday &lt;- length(unique(dat$birthday[bootrows])) } rmod &lt;- lm(ell~birthday, data=dat[bootrows,]) diffboot[i] &lt;- coef(rmod)[2] } quantile(diffboot, prob=c(0.025, 0.975)) ## 2.5% 97.5% ## -4.200000 8.333333 result is a confidence interval assumption: all observations are independent! hist(diffboot); abline(v=coef(mod)[2], lwd=2, col=&quot;red&quot;) 2.26 F-test Comparison of two variances H0: Var(X1)=Var(X2) -&gt; \\(F = \\frac{Var(X1)}{Var(X2)} \\approx 1\\) even more complicated density function than the t-distribution! We have not yet met any Bayesian example where the F-distribution is used. is used in the frequentist version of ANOVA 2.27 Analysis of variance ANOVA Aim: comparison between means Method: comparison of between-group with within-group variance Total sum of squares (SS) = SST = \\(\\sum_1^n{(y_i-\\bar{y})^2}\\) Within-group SS = SSW = \\(\\sum_1^n{(y_i-\\bar{y_g})^2}\\): unexplained variance Between-group SS = SSB = \\(\\sum_1^n{(\\bar{y_g}-\\bar{y})^2}\\): explained variance H0: \\(\\bar{y_1}=\\bar{y_2}=\\bar{y_3}\\) Expectation given H0: Between-group variance is due to natural variation (within-group variance) SSB/df_between = SSW/df_within, where df_between= number of groups -1 and df_within = n-number of groups MSB = SSB/df_between, MSW = SSW/df_within MSB/MSW ~ F(df_between, df_within) NEED TO INSERT A BAYESIAN ANAOVA HERE 2.28 Chisquare test correlations between two categorical variables comparison of two distributions (goodness of fit) table(dat$birthday, dat$statsfeeling) ## ## negative neutral positive ## flower 3 1 0 ## wine 1 1 1 expected values \\(E_{ij}\\) given H0: rowsum*colsum/total \\(\\chi^2\\) measures the difference between the observed \\(O_{ij}\\) and expected \\(E_{ij}\\) values as: \\(\\chi^2=\\sum_{i=1}^{m}\\sum_{j=1}^{k}\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\) The \\(\\chi^2\\)-distribution has 1 parameter, the degrees of freedom \\(v\\) = \\((m-1)(k-1)\\). chisq.test(table(dat$birthday, dat$statsfeeling)) ## ## Pearson&#39;s Chi-squared test ## ## data: table(dat$birthday, dat$statsfeeling) ## X-squared = 1.8958, df = 2, p-value = 0.3875 no cell should have a count less than 5… 2.29 Bayesian way of analysing correlations between categorical variables log-linear model (Poisson model) for the counts estimating proportions using a binomial or a multinomial model # log-linear model mod &lt;- glm(count~gift+feel + gift:feel, data=datagg, family=poisson) bsim &lt;- sim(mod, n.sim=nsim) round(t(apply(bsim@coef, 2, quantile, prob=c(0.025, 0.5, 0.975))),2) ## 2.5% 50% 97.5% ## (Intercept) -0.02 1.10 2.26 ## giftwine -3.35 -1.09 1.13 ## feelneutral -3.37 -1.10 1.22 ## feelpositive -82593.73 -217.91 85128.08 ## giftwine:feelneutral -2.48 1.10 4.69 ## giftwine:feelpositive -85129.15 220.68 82595.26 the interaction parameters measure the strength of the correlation # binomial model tab &lt;- table(dat$statsfeeling,dat$birthday) mod &lt;- glm(tab~rownames(tab), family=binomial) bsim &lt;- sim(mod, n.sim=nsim) 2.30 Summary Bayesian data analysis = applying the Bayes theorem for summarizing knowledge based on data, priors and the model assumptions. Frequentist statistics = quantifying uncertainty by hypothetical repetitions p-values are not wrong per se but they lead scientists and politicians to wrong decisions. "],
["analyses-steps.html", "3 Data analysis step by step 3.1 Plausibility of Data 3.2 Relationships 3.3 Error Distribution 3.4 Preparation of Explanatory Variables 3.5 Data Structure 3.6 Fit the Model 3.7 Check Model 3.8 Model Uncertainty 3.9 Draw Conclusions Further reading", " 3 Data analysis step by step In this chapter we provide a checklist with some guidance for data analysis. However, do not expect tthe list to be complete and for different studies, a different order of the steps may make more sense. We usually repeat steps 3.2 to 3.7 until we find one or a set of models that fit the data well and that are realistic enough to be useful for the intended purpose. Data analysis is always a lot of work and, often, the following steps have to be repeated many times until we find a useful and robust model. There is a danger with this: we may find interesting results that answer different questions than we asked originally.We can report such findings, but we should state that they appeared (more or less by chance) during the data exploration and model fitting phase, and we have to be aware that the estimates may be biased because the study was not optimally designed with respect to these findings. It is important to always keep the original aim of the study in mind. Do not adjust the study question according to the data. We also recommend reporting what the model started with at the first iteration and describing the strategy and reasoning behind the model development process. 3.1 Plausibility of Data Prepare the data and check graphically, or via summary statistics, whether all the data are plausible. Prepare the data so that errors (typos, etc.) are minimal, for example, by double-checking the entries. See chapter 4 for useful R-code that can be used for data preparation and to make plausibility controls. 3.2 Relationships Think about the direct and indirect relationships among the variables of the study. We normally start a data analysis by drawing a sketch of the model including all explanatory variables and interactions that may be biologically meaningful.We will most likely repeat this step after having looked at the model fit. To make the data analysis transparent we should report every model that was considered. A short note about why a specific model was considered and why it was discarded helps make the modeling process reproducible. 3.3 Error Distribution What is the nature of the variable of interest (outcome, dependent variable)? Chapter 6 give an overview of the distributions that are most relevant for ecologists. 3.4 Preparation of Explanatory Variables Look at the distribution (histogram) of every explanatory variable: Linear models do not assume that the explanatory variables have any specific distribution. Thus there is no need to check for a normal distribution! However, very skewed distributions result in unequal weighting of the observations in the model. In extreme cases, the slope of a regression line is defined by one or a few observations only. We also need to check whether the variance is large enough, and to think about the shape of the expected effect. The following four questions may help with this step: l Is the variance (of the explanatory variable) big enough so that an effect of the variable can be measured? l Is the distribution skewed? If an explanatory variable is highly skewed, it may make sense to transform the variable. l Does it show a bimodal distribution? Consider making the variable binary. l Is it expected that a change of 1 at lower values for x has the same biological effect as a change of 1 at higher values of x? If not, a trans- formation (e.g., log) could linearize the relationship between x and y. Centering: Centering (x_centered 1⁄4 x mean(x)) is a transformation that produces a variable with a mean of 0. With centered predictors, the intercept and main effects in the linear model are better interpretable (they are measured at the center of the data instead of at the covariate value 1⁄4 0 which may be far off; Section 4.2.6); the model fitting algorithm converges faster and better. Scaling: To make the estimate of the effect sizes comparable between var- iables, the variables can be scaled, that is, x_scaled 1⁄4 x/sd(x). The unit of the scaled variable is then 1 standard deviation. Gelman and Hill (2007, p. 55 f) propose to scale the variables by two times the standard deviation (x_scaled 1⁄4 x/(2*sd(x))) to make effect sizes comparable between numeric and binary variables. Scaling can be important for model convergence, especially when polynomials are included. Consider the use of orthogonal polynomials (Section 4.2.9). Collinearity: l Look at the correlation among the explanatory variables (pairs plot or correlation matrix). l If the explanatory variables are correlated, go back to step 2 and see Section 4.2.7 for more details about collinearity. Are interactions and polynomial terms needed in the model? If not already done in step 2, think about the relationship between each explanatory variable and the dependent variable. l Is it linear or do polynomial terms have to be included in the model? If the relationship cannot be described appropriately by polynomial terms, think of a nonlinear model or a generalized additive model (GAM). l May the effect of one explanatory variable depend on the value of another explanatory variable (interaction)? 3.5 Data Structure After having taken into account all of the (fixed effect) terms from step 4: are the observations independent or grouped/structured? 3.6 Fit the Model Fit the model as described in Chapters 4, 7, 8, 9, or 14. 3.7 Check Model We assess model fit by graphical analyses of the residuals (Chapter 6), by predictive model checking (Section 10.1), or by sensitivity analysis (Chapter 15). The following is a nonexhaustive list of aspects that can be looked at in a residual analysis and posterior predictive model checking. For non-Gaussian models it is often easier to assess model fit using pos- terior predictive checks (Chapter 10) rather than residual analyses. Posterior predictive checks usually show clearly in which aspect the model failed so we can go back to step 2 of the analysis. Recognizing in what aspect a model does not fit the data based on residual plots improves with experience. Therefore, the following table lists some patterns that can appear in residual plots together with what these patterns possibly indicate. We also indicate what could be done in the specific cases. 3.8 Model Uncertainty If, while working through steps 1 to 7, possibly repeatedly, we came up with one or more models that fit the data reasonably well, we then turn to the methods presented in Chapter 11 to draw inference from more than one model. If we have only one model, we proceed to step 9. 3.9 Draw Conclusions Simulate values from the joint posterior distribution of the model parameters (sim, BUGS, Stan). Use these samples to present parameter uncertainty, to obtain posterior distributions for predictions, probabilities of specific hypotheses, and derived quantities. Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],
["datamanip.html", "4 Data preparation 4.1 Basic operations 4.2 Further reading", " 4 Data preparation 4.1 Basic operations Alle Packete laden library(tidyverse) oder nur library(dplyr). dat &lt;- iris %&gt;% as.tibble() %&gt;% filter(Sepal.Length &gt; 5) %&gt;% group_by(Species) %&gt;% summarise(n = n(), mittel = mean(Petal.Length)) 4.2 Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],
["figures.html", "5 Visualizations 5.1 Short Checklist for figures Further reading", " 5 Visualizations 5.1 Short Checklist for figures The figure should represent the answer to the study question. Often, the classical types of plots such as a scatterplot, a bar plot, or an effects plot are sufficient. However, in many cases, adding a little bit of creativity can greatly improve readability or the message of the figure. 2.Label the x- and y-axes. Make sure that the units are indicated either within the title or in the figure legend. Starty-axisatzeroifthereferencetozeroisimportantfortheinterpretationof effects. The argument ylim[c(0, max(dat$y)) in R is used for this purpose. Scale the axes so that all data are shown. Make sure that sample size is indicated either in the figure or in the legend (or, at least, easy to find in the text). Use interpretable units. That means, if the variable on the x-axis has been z-transformed to fit the model, back-transform the effects to the original scale. Give the raw data whenever possible. Sometimes, a significant effect cannot be seen in the raw data because so many other variables have an influence on the outcome. Then, you may prefer showing the effect only (e.g., a regression line with a credible interval) and give the residual standard deviation in the figure legend. Even then, we think it is important to show the raw data graphically somewhere else in the paper or in the supplementary material. A scatterplot of the data can contain structures that are lost in summary statistics. Draw the figures as simply as possible. Avoid 3D graphics. Delete all unnecessary elements. Reduce the number of different colors to a minimum necessary. A color scale from orange to blue gives a gray scale in a black-and-white print. colorRampPalette(c(&quot;orange&quot;, &quot;blue&quot;))(5) produces five colors on a scale from orange to blue. Remember that around 8% of the northern European male population have difficulties distinguishing red from green but it is easier for them to distinguish orange from blue. Further reading Data Visualization. A practical introduction: A practical introduction to data visulization in R. Good data visualizations also make it easier to communicate your ideas and findings to other people. Beyond that, producing effective plots from your own data is the best way to develop a good eye for reading and understanding graphs—good and bad—made by others. Fundamentals of Data Visualization: A guide to making visualizations that accurately reflect the data, tell a story, and look professional. […] This is an online preview of the book “Fundamentals of Data Visualization” to be published with O’Reilly Media, Inc. Completed chapters will be posted here as they become available. The book is meant as a guide to making visualizations that accurately reflect the data, tell a story, and look professional. It has grown out of my experience of working with students and postdocs in my laboratory on thousands of data visualizations. "],
["distributions.html", "6 Probability distributions 6.1 Introduction 6.2 Normal distribution 6.3 Poisson distribution 6.4 Gamma distribution", " 6 Probability distributions 6.1 Introduction hist(rnorm(1000)) Figure 6.1: Das ist ein Versuch. 6.2 Normal distribution xxx 6.3 Poisson distribution xxx 6.4 Gamma distribution xxx 6.4.1 Cauchy distribution We sometimes use the Cauchy distiribution to specify the prior distirbution of the standart deviation and similar parameters in a model. An example for this is the regression model that we used to introduce Stan (Chapter 12.3). "],
["rgis.html", "7 Spatial analyses and maps 7.1 Data types 7.2 Basic functions 7.3 Further reading", " 7 Spatial analyses and maps Almost all the things that we can do with traditional geographic information system we can also do within R. If we do it in R we get the useful benefits of a script approach that allows for reproducible analyses (see Chapter 8) and that can be scaled to many more objects or larger data sets. Here we simply introduce the packages and functions that we most often use when working with spatial data. 7.1 Data types 7.1.1 Raster data Very broadly speaking, we divide spatial data into two categories, raster data and all other types of data for points, lines or polygons. Raster data consists of a grid (i.e. a matrix) where each cell of the grid contains one or several values representing the spatial information. The R-package raster is very efficient for raster data. We can use the function raster() to load raster data from files. Most of the common file formats for raster data such as .geotiff, .tif or .grd are supported. However, raster data can also be converted from tables with one column for the x-coordinates, one column for the y-coordinates and one column for the spatial information. The coordinates must be from a regular grid. For example the freely available topographic data of Switzerland are collected on a 100m x 100m grid. In the following example the tibble elevations contains the elevation data from the canton of Aargau and is converted into raster data using the function rasterFromXYZ(). library(raster) ra &lt;- rasterFromXYZ(elevation) plot(ra) Figure 7.1: Meter above sea leavel (m) accross the canton Aargau in Switzerland. 7.1.2 Geometry data All geometry data types are composed of points. The spatial location of a point is defined by its x and y coordinates. Using several points one can then define lines (sequence of points connected by straight lines) and polygons (sequence of points that form a closed ring). Points, lines and plygons are the geometries we usually work with. We use the package sf to work with geometry data types. Its functions are very efficient to work with all spatial data other than raster data. It also links to GDAL (i.e. a computer software library for reading and writing raster and vector geospatial data formats) and proj.4 (i.e. a library for performing conversions between cartographic projections), which are important tools when woring with different sources of spatial data. We can used the function st_read() to read geometry data from file or database. In the following example, however, we convert the tibble frogs into a simple feature collection. The data file frogs, formatted as a tibble, contains different columns including the counts, variables that describe the ponds as well as the spatial coordinates of the counts. The simple feature collection looks rather similar to the original tibble, however instead of the x and y colomns it now contains the column geometry. With the simple feature collection we can work pretty much in the same way as we use to work with tibbles. For example we can filter only the data from 2011, select the geometries and plot them on the top of the raster with the elevation across the entire canton of Aargau (see 7.1.1 for the raster data). library(sf) dat &lt;- frogs %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;)) plot(ra) dat %&gt;% filter(year == 2011) %&gt;% st_geometry() %&gt;% plot(add = TRUE, pch = &quot;x&quot;) Figure 7.2: Locations of the ponds where frogs were counted in 2011. The background image shows the elevation (meter above sea level). 7.2 Basic functions In this chapter we shortly describe some functions that we often use when working with spatial data in R. 7.2.1 Coordinate systems An important aspect of spatial data is the coordinate reference system (CRS). A CRS determines for instance where the center of the map is, the units for the coordinates and others. PROJ.4 is an open source software library that is commonly used for CRS transformation. Most commonly used CRSs have been assigned a HERE IS SOMETHING MISSING. The EPSG (European Petroleum Survey Group) code is a unique ID that can be used to identify a CRS. Thus if we know the EPSG code it is rather simple to transform spatial data into other CRS. To search for the correct EPSG code we can use https://www.epsg-registry.org or http://www.spatialreference.org The following code shows how to assign the CRS of existing data and how to transform the coordinate system for raster data and sf data, respectively. # Assign CRS for raster data crs(ra) &lt;- CRS(&quot;+init=epsg:21781&quot;) # Assign CRS for sf data st_crs(dat) &lt;- 21781 # Transfrom raster data to WGS84 projectRaster(ra, crs = CRS(&quot;+init=epsg:4326&quot;)) # Transfrom sf data to WGS84 st_transform(dat, crs = 4326) 7.3 Further reading Geocomputation with R: This online-book is aimed at people who want to do spatial data analysis, visualization and modeling using open source software and reproducible workflows. Spatial Data Analysis and Modeling with R: Online introduction to do spatial analyses with R. Good introduction to coordinate systems and projections. "],
["reproducible.html", "8 Reproducible Research 8.1 Introduction 8.2 Further reading", " 8 Reproducible Research 8.1 Introduction xxx 8.2 Further reading Rmarkdown: The first official book authored by the core R Markdown developers that provides a comprehensive and accurate reference to the R Markdown ecosystem. With R Markdown, you can easily create reproducible data analysis reports, presentations, dashboards, interactive applications, books, dissertations, websites, and journal articles, while enjoying the simplicity of Markdown and the great power of R and other languages. Bookdown by Yihui Xie: A guide to authoring books with R Markdown, including how to generate figures and tables, and insert cross-references, citations, HTML widgets, and Shiny apps in R Markdown. The book can be exported to HTML, PDF, and e-books (e.g. EPUB). The book style is customizable. You can easily write and preview the book in RStudio IDE or other editors, and host the book wherever you want (e.g. bookdown.org). Our book is written using bookdown. "],
["furthertopics.html", "9 Further topics 9.1 Bioacoustic analyse 9.2 Python", " 9 Further topics This is a collection of short introductions or links with commented R code that cover other topics that might be usful for ecologists. 9.1 Bioacoustic analyse Bioacoustic analyses are nicely covered in a blog by Marcelo Araya-Salas. 9.2 Python Like R, python is a is a high-level programming language that is used by many ecologists. The reticulate package provides a comprehensive set of tools for interoperability between Python and R. "],
["PART-II.html", "10 Introduction to PART II Further reading", " 10 Introduction to PART II Further reading A really good introductory book to Bayesian data analyses is (McElreath 2016). This book starts with a thorough introduction to applying the Bayes theorem for drawing inference from data. In addition, it carefully discusses what can and what cannot be concluded from statistical results. We like this very much. We like looking up statistical methods in papers and books written by Andrew Gelman (e.g. A. Gelman et al. 2014) and Trevor Hastie (e.g. (T. Hastie, Tibshirani, and Friedman 2009, Efron and Hastie (2016))) because both explain complicated things in a concise and understandable way. "],
["priors.html", "11 Prior distributions 11.1 Introduction 11.2 How to choose a prior 11.3 Prior sensitivity", " 11 Prior distributions 11.1 Introduction 11.2 How to choose a prior Tabelle von Fränzi (CourseIII_glm_glmmm/course2018/presentations_handouts/presentations) 11.3 Prior sensitivity xxx "],
["stan.html", "12 MCMC using Stan 12.1 Background 12.2 Install rstan 12.3 Writing a Stan model 12.4 Run Stan from R Further reading", " 12 MCMC using Stan 12.1 Background Markov chain Monte Carlo (MCMC) simulation techniques were developed in the mid-1950s by physicists (Metropolis et al., 1953). Later, statisticians discovered MCMC (Hastings, 1970; Geman &amp; Geman, 1984; Tanner &amp; Wong, 1987; Gelfand et al., 1990; Gelfand &amp; Smith, 1990). MCMC methods make it possible to obtain posterior distributions for parameters and latent variables (unobserved variables) of complex models. In parallel, personal computer capacities increased in the 1990s and user-friendly software such as the different programs based on the programming language BUGS (Spiegelhalter et al., 2003) came out. These developments boosted the use of Bayesian data analyses, particularly in genetics and ecology. 12.2 Install rstan In this book we use the program Stan to draw random samples from the joint posterior distribution of the model parameters given a model, the data, prior distributions, and initial values. To do so, it uses the “no-U-turn sampler,” which is a type of Hamiltonian Monte Carlo simulation (Hoffman and Gelman 2014; Betancourt 2013), and optimization-based point estimation. These algorithms are more efficient than the ones implemented in BUGS programs and they can handle larger data sets. Stan works particularly well for hierar- chical models (Betancourt and Girolami 2013). Stan runs on Windows, Mac, and Linux and can be used via the R interface rstan. Stan is automatically installed when the R package rstan is installed. For installing rstan, it is advised to follow closely the system-specific instructions. 12.3 Writing a Stan model The statistical model is written in the Stan language and saved in a text file. The Stan language is rather strict, forcing the user to write unambiguous models. Stan is very well documented and the Stan Documentation contains a comprehensive Language Manual, a Wiki documentation and various tutorials. We here provide a normal regression with one predictor variable as a worked example. The entire Stan model is as following (saved as linreg.stan) data { int&lt;lower=0&gt; n; vector[n] y; vector[n] x; } parameters { vector[2] beta; real&lt;lower=0&gt; sigma; } model { //priors beta ~ normal(0,5); sigma ~ cauchy(0,5); // likelihood y ~ normal(beta[1] + beta[2] * x, sigma); } A Stan model consists of different named blocks. These blocks are (from first to last): data, transformed data, parameters, trans- formed parameters, model, and generated quantities. The blocks must appear in this order. The model block is mandatory; all other blocks are optional. In the data block, the type, dimension, and name of every variable has to be declared. Optionally, the range of possible values can be specified. For example, vector[N] y; means that y is a vector (type real) of length N, and int&lt;lower=0&gt; N; means that N is an integer with nonnegative values (the bounds, here 0, are included). Note that the restriction to a possible range of values is not strictly necessary but this will help specifying the correct model and it will improve speed. We also see that each line needs to be closed by a column sign. In the parameters block, all model parameters have to be defined. The coefficients of the linear predictor constitute a vector of length 2, vector[2] beta;. Alternatively, real beta[2]; could be used. The sigma parameter is a one-number parameter that has to be positive, therefore real&lt;lower=0&gt; sigma;. The model block contains the model specification. Stan functions can handle vectors and we do not have to loop over all observations as typical for BUGS . Here, we use a Cauchy distribution as a prior distribution for sigma. This distribution can have negative values, but because we defined the lower limit of sigma to be 0 in the parameters block, the prior distribution actually used in the model is a truncated Cauchy distribution (truncated at zero). In Chapter 11.2 we explain how to choose prior distributions. Further characteristics of the Stan language that are good to know include: The variance parameter for the normal distribution is specified as the standard deviation (like in R but different from BUGS, where the precision is used). If no prior is specified, Stan uses a uniform prior over the range of possible values as specified in the parameter block. Variable names must not contain periods, for example, x.z would not be allowed, but x_z is allowed. To comment out a line, use double forward-slashes //. 12.4 Run Stan from R We fit the model to simulated data. Stan needs a vector containing the names of the data objects. In our case, x, y, and N are objects that exist in the R console. The function stan() starts Stan and returns an object containing MCMCs for every model parameter. We have to specify the name of the file that contains the model specification, the data, the number of chains, and the number of iterations per chain we would like to have. The first half of the iterations of each chain is declared as the warm-up. During the warm-up, Stan is not simulating a Markov chain, because in every step the algorithm is adapted. After the warm-up the algorithm is fixed and Stan simulates Markov chains. library(rstan) # Simulate fake data n &lt;- 50 # sample size sigma &lt;- 5 # standard deviation of the residuals b0 &lt;- 2 # intercept b1 &lt;- 0.7 # slope x &lt;- runif(n, 10, 30) # random numbers of the covariate simresid &lt;- rnorm(n, 0, sd=sigma) # residuals y &lt;- b0 + b1*x + simresid # calculate y, i.e. the data # Bundle data into a list datax &lt;- list(n=length(y), y=y, x=x) # Run STAN fit &lt;- stan(file = &quot;stanmodels/linreg.stan&quot;, data=datax, verbose = FALSE) ## In file included from C:/Users/fk/Documents/R/win-library/3.5/BH/include/boost/config.hpp:39:0, ## from C:/Users/fk/Documents/R/win-library/3.5/BH/include/boost/math/tools/config.hpp:13, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/core/var.hpp:7, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/core.hpp:12, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/mat.hpp:4, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math.hpp:4, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/src/stan/model/model_header.hpp:4, ## from filee2856303bb1.cpp:8: ## C:/Users/fk/Documents/R/win-library/3.5/BH/include/boost/config/compiler/gcc.hpp:186:0: warning: &quot;BOOST_NO_CXX11_RVALUE_REFERENCES&quot; redefined ## # define BOOST_NO_CXX11_RVALUE_REFERENCES ## ^ ## &lt;command-line&gt;:0:0: note: this is the location of the previous definition ## cc1plus.exe: warning: unrecognized command line option &quot;-Wno-ignored-attributes&quot; ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 1). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2000 [ 0%] (Warmup) ## Iteration: 200 / 2000 [ 10%] (Warmup) ## Iteration: 400 / 2000 [ 20%] (Warmup) ## Iteration: 600 / 2000 [ 30%] (Warmup) ## Iteration: 800 / 2000 [ 40%] (Warmup) ## Iteration: 1000 / 2000 [ 50%] (Warmup) ## Iteration: 1001 / 2000 [ 50%] (Sampling) ## Iteration: 1200 / 2000 [ 60%] (Sampling) ## Iteration: 1400 / 2000 [ 70%] (Sampling) ## Iteration: 1600 / 2000 [ 80%] (Sampling) ## Iteration: 1800 / 2000 [ 90%] (Sampling) ## Iteration: 2000 / 2000 [100%] (Sampling) ## ## Elapsed Time: 0.08 seconds (Warm-up) ## 0.057 seconds (Sampling) ## 0.137 seconds (Total) ## ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 2). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2000 [ 0%] (Warmup) ## Iteration: 200 / 2000 [ 10%] (Warmup) ## Iteration: 400 / 2000 [ 20%] (Warmup) ## Iteration: 600 / 2000 [ 30%] (Warmup) ## Iteration: 800 / 2000 [ 40%] (Warmup) ## Iteration: 1000 / 2000 [ 50%] (Warmup) ## Iteration: 1001 / 2000 [ 50%] (Sampling) ## Iteration: 1200 / 2000 [ 60%] (Sampling) ## Iteration: 1400 / 2000 [ 70%] (Sampling) ## Iteration: 1600 / 2000 [ 80%] (Sampling) ## Iteration: 1800 / 2000 [ 90%] (Sampling) ## Iteration: 2000 / 2000 [100%] (Sampling) ## ## Elapsed Time: 0.078 seconds (Warm-up) ## 0.059 seconds (Sampling) ## 0.137 seconds (Total) ## ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 3). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2000 [ 0%] (Warmup) ## Iteration: 200 / 2000 [ 10%] (Warmup) ## Iteration: 400 / 2000 [ 20%] (Warmup) ## Iteration: 600 / 2000 [ 30%] (Warmup) ## Iteration: 800 / 2000 [ 40%] (Warmup) ## Iteration: 1000 / 2000 [ 50%] (Warmup) ## Iteration: 1001 / 2000 [ 50%] (Sampling) ## Iteration: 1200 / 2000 [ 60%] (Sampling) ## Iteration: 1400 / 2000 [ 70%] (Sampling) ## Iteration: 1600 / 2000 [ 80%] (Sampling) ## Iteration: 1800 / 2000 [ 90%] (Sampling) ## Iteration: 2000 / 2000 [100%] (Sampling) ## ## Elapsed Time: 0.078 seconds (Warm-up) ## 0.047 seconds (Sampling) ## 0.125 seconds (Total) ## ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 4). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2000 [ 0%] (Warmup) ## Iteration: 200 / 2000 [ 10%] (Warmup) ## Iteration: 400 / 2000 [ 20%] (Warmup) ## Iteration: 600 / 2000 [ 30%] (Warmup) ## Iteration: 800 / 2000 [ 40%] (Warmup) ## Iteration: 1000 / 2000 [ 50%] (Warmup) ## Iteration: 1001 / 2000 [ 50%] (Sampling) ## Iteration: 1200 / 2000 [ 60%] (Sampling) ## Iteration: 1400 / 2000 [ 70%] (Sampling) ## Iteration: 1600 / 2000 [ 80%] (Sampling) ## Iteration: 1800 / 2000 [ 90%] (Sampling) ## Iteration: 2000 / 2000 [100%] (Sampling) ## ## Elapsed Time: 0.067 seconds (Warm-up) ## 0.039 seconds (Sampling) ## 0.106 seconds (Total) Further reading Stan-Homepage: It contains the documentation for Stand a a lot of tutorials. "],
["ridge.html", "13 Ridge regression 13.1 Introduction", " 13 Ridge regression We should provide an example in Stan. 13.1 Introduction # Settings library(R2OpenBUGS) bugslocation &lt;- &quot;C:/Program Files/OpenBUGS323/OpenBugs.exe&quot; # location of OpenBUGS bugsworkingdir &lt;- file.path(getwd(), &quot;BUGS&quot;) # Bugs working directory #------------------------------------------------------------------------------- # Simulate fake data #------------------------------------------------------------------------------- library(MASS) n &lt;- 50 # sample size b0 &lt;- 1.2 b &lt;- rnorm(5, 0, 2) Sigma &lt;- matrix(c(10,3,3,2,1, 3,2,3,2,1, 3,3,5,3,2, 2,2,3,10,3, 1,1,2,3,15),5,5) Sigma x &lt;- mvrnorm(n = n, rep(0, 5), Sigma) simresid &lt;- rnorm(n, 0, sd=3) # residuals x.z &lt;- x for(i in 1:ncol(x)) x.z[,i] &lt;- (x[,i]-mean(x[,i]))/sd(x[,i]) y &lt;- b0 + x.z%*%b + simresid # calculate y, i.e. the data #------------------------------------------------------------------------------- # Function to generate initial values #------------------------------------------------------------------------------- inits &lt;- function() { list(b0=runif(1, -2, 2), b=runif(5, -2, 2), sigma=runif(1, 0.1, 2)) } #------------------------------------------------------------------------------- # Run OpenBUGS #------------------------------------------------------------------------------- parameters &lt;- c(&quot;b0&quot;, &quot;b&quot;, &quot;sigma&quot;) lambda &lt;- c(1, 2, 10, 25, 50, 100, 500, 1000, 10000) bs &lt;- matrix(ncol=length(lambda), nrow=length(b)) bse &lt;- matrix(ncol=length(lambda), nrow=length(b)) for(j in 1:length(lambda)){ datax &lt;- list(y=as.numeric(y), x=x, n=n, mb=rep(0, 5), lambda=lambda[j]) fit &lt;- bugs(datax, inits, parameters, model.file=&quot;ridge_regression.txt&quot;, n.thin=1, n.chains=2, n.burnin=5000, n.iter=10000, debug=FALSE, OpenBUGS.pgm = bugslocation, working.directory=bugsworkingdir) bs[,j] &lt;- fit$mean$b bse[,j] &lt;- fit$sd$b } range(bs) plot(1:length(lambda), seq(-2, 1, length=length(lambda)), type=&quot;n&quot;) colkey &lt;- rainbow(length(b)) for(j in 1:nrow(bs)){ lines(1:length(lambda), bs[j,], col=colkey[j], lwd=2) lines(1:length(lambda), bs[j,]-2*bse[j,], col=colkey[j], lty=3) lines(1:length(lambda), bs[j,]+2*bse[j,], col=colkey[j], lty=3) } abline(h=0) round(fit$summary,2) #------------------------------------------------------------------------------- # Run WinBUGS #------------------------------------------------------------------------------- library(R2WinBUGS) bugsdir &lt;- &quot;C:/Users/fk/WinBUGS14&quot; # mod &lt;- bugs(datax, inits= inits, parameters, model.file=&quot;normlinreg.txt&quot;, n.chains=2, n.iter=1000, n.burnin=500, n.thin=1, debug=TRUE, bugs.directory=bugsdir, program=&quot;WinBUGS&quot;, working.directory=bugsworkingdir) #------------------------------------------------------------------------------- # Test convergence and make inference #------------------------------------------------------------------------------- library(blmeco) # Make Figure 12.2 par(mfrow=c(3,1)) historyplot(fit, &quot;beta0&quot;) historyplot(fit, &quot;beta1&quot;) historyplot(fit, &quot;sigmaRes&quot;) # Parameter estimates print(fit$summary, 3) # Make predictions for covariate values between 10 and 30 newdat &lt;- data.frame(x=seq(10, 30, length=100)) Xmat &lt;- model.matrix(~x, data=newdat) predmat &lt;- matrix(ncol=fit$n.sim, nrow=nrow(newdat)) for(i in 1:fit$n.sim) predmat[,i] &lt;- Xmat%*%c(fit$sims.list$beta0[i], fit$sims.list$beta1[i]) newdat$lower.bugs &lt;- apply(predmat, 1, quantile, prob=0.025) newdat$upper.bugs &lt;- apply(predmat, 1, quantile, prob=0.975) plot(y~x, pch=16, las=1, cex.lab=1.4, cex.axis=1.2, type=&quot;n&quot;, main=&quot;&quot;) polygon(c(newdat$x, rev(newdat$x)), c(newdat$lower.bugs, rev(newdat$upper.bugs)), col=grey(0.7), border=NA) abline(c(fit$mean$beta0, fit$mean$beta1), lwd=2) box() points(x,y) "],
["sem.html", "14 Structural equation model 14.1 Introduction", " 14 Structural equation model We should provide an example in Stan. 14.1 Introduction ------------------------------------------------------------------------------------------------------ # General settings #------------------------------------------------------------------------------------------------------ library(MASS) library(rjags) library(MCMCpack) #------------------------------------------------------------------------------------------------------ # Simulation #------------------------------------------------------------------------------------------------------ n &lt;- 100 heffM &lt;- 0.6 # effect of H on M heffCS &lt;- 0.0 # effect of H on Clutch size meffCS &lt;- 0.6 # effect of M on Clutch size SigmaM &lt;- matrix(c(0.1,0.04,0.04,0.1),2,2) meffm1 &lt;- 0.6 meffm2 &lt;- 0.7 SigmaH &lt;- matrix(c(0.1,0.04,0.04,0.1),2,2) meffh1 &lt;- 0.6 meffh2 &lt;- -0.7 # Latente Variablen H &lt;- rnorm(n, 0, 1) M &lt;- rnorm(n, heffM * H, 0.1) # Clutch size CS &lt;- rnorm(n, heffCS * H + meffCS * M, 0.1) # Indicators eM &lt;- cbind(meffm1 * M, meffm2 * M) datM &lt;- matrix(NA, ncol = 2, nrow = n) eH &lt;- cbind(meffh1 * H, meffh2 * H) datH &lt;- matrix(NA, ncol = 2, nrow = n) for(i in 1:n) { datM[i,] &lt;- mvrnorm(1, eM[i,], SigmaM) datH[i,] &lt;- mvrnorm(1, eH[i,], SigmaH) } #------------------------------------------------------------------------------ # JAGS Model #------------------------------------------------------------------------------ dat &lt;- list(datM = datM, datH = datH, n = n, CS = CS, #H = H, M = M, S3 = matrix(c(1,0,0,1),nrow=2)/1) # Function to create initial values inits &lt;- function() { list( meffh = runif(2, 0, 0.1), meffm = runif(2, 0, 0.1), heffM = runif(1, 0, 0.1), heffCS = runif(1, 0, 0.1), meffCS = runif(1, 0, 0.1), tauCS = runif(1, 0.1, 0.3), tauMH = runif(1, 0.1, 0.3), tauH = rwish(3,matrix(c(.02,0,0,.04),nrow=2)), tauM = rwish(3,matrix(c(.02,0,0,.04),nrow=2)) # M = as.numeric(rep(0, n)) ) } t.n.thin &lt;- 50 t.n.chains &lt;- 2 t.n.burnin &lt;- 20000 t.n.iter &lt;- 50000 # Run JAGS jagres &lt;- jags.model(&#39;JAGS/BUGSmod1.R&#39;,data = dat, n.chains = t.n.chains, inits = inits, n.adapt = t.n.burnin) params &lt;- c(&quot;meffh&quot;, &quot;meffm&quot;, &quot;heffM&quot;, &quot;heffCS&quot;, &quot;meffCS&quot;) mod &lt;- coda.samples(jagres, params, n.iter=t.n.iter, thin=t.n.thin) res &lt;- round(data.frame(summary(mod)$quantiles[, c(3, 1, 5)]), 3) res$TRUEVALUE &lt;- c(heffCS, heffM, meffCS, meffh1, meffh2, meffm1, meffm2) res # Traceplots post &lt;- data.frame(rbind(mod[[1]], mod[[2]])) names(post) &lt;- dimnames(mod[[1]])[[2]] par(mfrow = c(3,3)) param &lt;- c(&quot;meffh[1]&quot;, &quot;meffh[2]&quot;, &quot;meffm[1]&quot;, &quot;meffm[2]&quot;, &quot;heffM&quot;, &quot;heffCS&quot;, &quot;meffCS&quot;) traceplot(mod[, match(param, names(post))]) "],
["PART-III.html", "15 Introduction to PART III 15.1 Model notations", " 15 Introduction to PART III This part is a collection of more complicated ecological models to analyse data that may not be analysed with the traditional linear models that we covered in PART I of this book. 15.1 Model notations It is unavoidable that different authors use different notations for the same thing, or that the same notation is used for different things. We try to use, whenever possible, notations that is commonly used at the International Statistical Ecology Congress ISEC. Resulting from an earlier ISEC, Betancourt et al. (2016) give guidelines on what letter should be used for which parameter in order to achieve a standard notation at least among people working with classical mark-recapture models. However, the alphabet has fewer letters compared to the number of ecological parameters. Therefore, the same letter cannot stand for the same parameter across all papers, books and chapters. Here, we try to use the same letter for the same parameter within the same chapter. "],
["zeroinflated-poisson-lmm.html", "16 Zero-inflated Poisson Mixed Model 16.1 Introduction 16.2 Example data 16.3 Model", " 16 Zero-inflated Poisson Mixed Model 16.1 Introduction Usually we describe the outcome variable with a single distribution, such as the normal distribution in the case of linear (mixed) models, and Poisson or binomial distributions in the case of generalized linear (mixed) models. In life sciences, however, quite often the data are actually generated by more than one process. In such cases the distribution of the data could be the result of two or more different distributions. If we do not account for these different processes our inferences are likely to be biased. In this chapter, we introduce a mixture model that explicitly include two processes that generated the data. The zero-inflated Poisson model is a mixture of a binomial and a Poisson distribution. We belief that two (or more)-level models are very useful tools in life sciences because they can help uncover the different processes that generate the data we observe. 16.2 Example data We used the blackstork data from the blmeco-package. They contain the breeding success of Black-stork in Latvia. The data was collected and kindly provided by Maris Stradz. The data contains the number of nestlings of more then 300 Black-stork nests in different years. Counting animals or plants is a typical example of data that contain a lot of zero counts. For example, the number of nestlings produced by a breeding pair is often zero because the whole nest was depredated or because a catastrophic event occurred such as a flood. However, when the nest succeeds, the number of nestlings varies among the successful nests depending on how many eggs the female has laid, how much food the parents could bring to the nest, or other factors that affect the survival of a nestling in an intact nest. Thus the factors that determine how many zero counts there are in the data differ from the factors that determine how many nestlings there are, if a nest survives. Count data that are produced by two different processes–one produces the zero counts and the other the variance in the count for the ones that were not zero in the first process–are called zero-inflated data. Histograms of zero-inflated data look bimodal, with one peak at zero (Figure 16.1). Figure 16.1: Histogram of the number of nestlings counted in black stork nests Ciconia nigra in Latvia (n = 1130 observations of 279 nests). 16.3 Model The Poisson distribution does not fit well to such data, because the data contain more zero counts than expected under the Poisson distribution. Mullahy (1986) and Lambert (1992) formulated two different types of models that combine the two processes in one model and therefore account for the zero excess in the data and allow the analysis of the two processes separately. The hurdle model (Mullahy, 1986) combines a left-truncated count data model (Poisson or negative binomial distribution that only describes the distribution of data larger than zero) with a zero-hurdle model that describes the distribution of the data that are either zero or nonzero. In other words, the hurdle model divides the data into two data subsets, the zero counts and the nonzero counts, and fits two separate models to each subset of the data. To account for this division of the data, the two models assume left truncation (all measurements below 1 are missing in the data) and right censoring (all measurements larger than 1 have the value 1), respectively, in their error distributions. A hurdle model can be fitted in R using the function hurdle from the package pscl (Jackman, 2008). See the tutorial by Zeileis et al. (2008) for an introduction. In contrast to the hurdle model, the zero-inflated models (Mullahy, 1986; Lambert, 1992) combine a Bernoulli model (zero vs. nonzero) with a conditional Poisson model; conditional on the Bernoulli process being nonzero. Thus this model allows for a mixture of zero counts: some zero counts are zero because the outcome of the Bernoulli process was zero (these zero counts are sometimes called structural zero values), and others are zero because their outcome from the Poisson process was zero. The function `zeroinfl from the package pscl fits zero-inflated models (Zeileis et al., 2008). The zero-inflated model may seem to reflect the true process that has generated the data closer than the hurdle model. However, sometimes the fit of zero-inflated models is impeded because of high correlation of the model parameters between the zero model and the count model. In such cases, a hurdle model may cause less troubles. Both functions (hurdle and zeroinfl) from the package pscl do not allow the inclusion of random factors. The functions MCMCglmm from the package MCMCglmm (Hadfield, 2010) and glmmadmb from the package glmmADMB (http://glmmadmb.r-forge.r-project.org/) provide the possibility to account for zero-inflation with a GLMM. However, these functions are not very flexible in the types of zero-inflated models they can fit; for example, glmmadmb only includes a constant proportion of zero values. A zero-inflation model using BUGS is described in Ke ́ry and Schaub (2012). Here we use Stan to fit a zero- inflated model. Once we understand the basic model code, it is easy to add predictors and/or random effects to both the zero and the count model. The example data contain numbers of nestlings in black stork Ciconia nigra nests in Latvia collected by Maris Stradz and collaborators at 279 nests be- tween 1979 and 2010. Black storks build solid and large aeries on branches of large trees. The same aerie is used for up to 17 years until it collapses. The black stork population in Latvia has drastically declined over the last decades. Here, we use the nestling data as presented in Figure 14-2 to describe whether the number of black stork nestlings produced in Latvia decreased over time. We use a zero-inflated Poisson model to separately estimate temporal trends for nest survival and the number of nestlings in successful nests. Since the same nests have been measured repeatedly over 1 to 17 years, we add nest ID as a random factor to both models, the Bernoulli and the Poisson model. After the first model fit, we saw that the between-nest variance in the number of nest- lings for the successful nests was close to zero. Therefore, we decide to delete the random effect from the Poisson model. Here is our final model: zit is a latent (unobserved) variable that takes the values 0 or 1 for each nest i during year t. It indicates a “structural zero”, that is, if zit 1⁄4 1 the number of nestlings yit always is zero, because the expected value in the Poisson model lit(1 zit) becomes zero. If zit 1⁄4 0, the expected value in the Poisson model becomes lit. To fit this model in Stan, we first write the Stan model code and save it in a separated text-file with name “zeroinfl.stan”. "],
["cjs-with-mix.html", "17 Fränzi Modell 17.1 Introduction", " 17 Fränzi Modell 17.1 Introduction "],
["referenzen.html", "Referenzen", " Referenzen "]
]
