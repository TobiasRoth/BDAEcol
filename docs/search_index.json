[
["index.html", "Data Analysis in Ecology with R and Stan Preface Why this book? About this book How to contribute? Acknowledgments", " Data Analysis in Ecology with R and Stan Fränzi Korner-Nievergelt and Tobias Roth 2019-01-01 Preface Why this book? In 2015, we wrote a statistics book for Master/PhD level Bayesian data analyses in ecology (F. Korner-Nievergelt et al. 2015). You can order it here. People seemed to like it (e.g. (Harju 2016)). Since then, two parallel processes happen. First, we learn more and we become more confident in what we do, or what we do not, and why we do what we do. Second, several really clever people develop software that broaden the spectrum of ecological models that now easily can be applied by ecologists used to work with R. With this e-book, we open the possibility to add new or substantially revised material. In most of the time, it should be in a state that it can be printed and used together with the book as handout for our stats courses. About this book We do not copy text from the book into the e-book. Therefore, we refer to the book (F. Korner-Nievergelt et al. 2015) for reading about the basic theory on doing Bayesian data analyses using linear models. However, Chapters 1 to 17 of this dynamic e-book correspond to the book chapters. In each chapter, we may provide updated R-codes and/or additional material. The following chapters contain completely new material that we think may be useful for ecologists. While we show the R-code behind most of the analyses, we sometimes choose not to show all the code in the html version of the book. This is particularly the case for some of the illustrations. An intrested reader can always consult the public GitHub repository with the rmarkdown-files that were used to generate the book. How to contribute? It is open so that everybody with a GitHub account can make comments and suggestions for improvement. Readers can contribute in two ways. One way is to add an issue. The second way is to contribute content directly through the edit button at the top of the page (i.e. a symbol showing a pencil in a square). That button is linked to the rmarkdown source file of each page. You can correct typos or add new text and then submit a GitHub pull request. We try to respond to you as quickly as possible. We are looking forward to your contribution! Acknowledgments We thank Yihui Xie for providing bookdown which makes it much fun to write open books such as ours. "],
["PART-I.html", "1 Introduction to PART I 1.1 Further reading", " 1 Introduction to PART I During our courses we are sometimes asked to give an introduction to some R-related stuff covering data analysis, presentation of results or rather specialist topics in ecology. In this part we present collected these introduction and try to keep them updated. This is also a commented collection of R-code that we documented for our own work. We hope this might be useful olso for other readers. 1.1 Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],
["basics.html", "2 Prerequisits: Basic statistical terms 2.1 Scale of measurement 2.2 Correlations 2.3 Principal components analyses PCA 2.4 Standard deviation and standard error 2.5 Central limit theorem / law of large numbers 2.6 Bayes theorem 2.7 Bayes theorem for continuous parameters 2.8 Single parameter model 2.9 A model with two parameters 2.10 t-distribution 2.11 Frequentist one-sample t-test 2.12 Nullhypothesis test 2.13 Confidence interval 2.14 Posterior distribution 2.15 Posterior probability 2.16 Monte Carlo simulation (parametric bootstrap) 2.17 3 methods for getting the posterior distribution 2.18 Grid approximation 2.19 Monte Carlo simulations 2.20 Comparison of the locations between two groups 2.21 Difference between two means 2.22 Two-sample t-test 2.23 Wilxocon test 2.24 Randomisation test 2.25 Bootstrap 2.26 F-distribution 2.27 Chisquare test 2.28 Bayesian way of analysing correlations between categorical variables 2.29 Summary", " 2 Prerequisits: Basic statistical terms This chapter introduces some important terms useful for doing data analyses. It also introduces the essentials of the classical frequentist tests such as t- and Chisquare test. We will not use them later but we think it is important to know how to interpret the results in order to be able to understand 100 years of scientific literature. For each classical test, we provide a suggestion how to do it in a Bayesian way and we discuss some differences between the Bayesian and frequentist statistics. 2.1 Scale of measurement Scale Examples Properties Coding in R Nominal Sex, genotype, habitat Identity (values have a unique meaning) factor() Ordinal Elevational zones Identity and magnitude (values have an ordered relationship) ordered() Numeric Discrete: counts; continuous: body weight, wing length Identity, magnitude, and equal intervals intgeger() numeric() 2.2 Correlations 2.2.1 Basics of variances, covariances and correlations variance \\(\\hat{\\sigma^2} = s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\) The term \\((n-1)\\) is called the degrees of freedom. standard deviation \\(\\hat{\\sigma} = s = \\sqrt{s^2}\\) covariance \\(q = \\frac{1}{n-1}\\sum_{i=1}^{n}((x_i-\\bar{x})*(y_i-\\bar{y}))\\) 2.2.2 Pearson correlation coefficient standardized covariance \\[r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\] 2.2.3 Spearman correlation coefficient rank correlation correlation between rank(x) and rank(y) robust against outliers 2.2.4 Kendall’s tau rank correlation I = number of pairs (i,k) for which \\((x_i &lt; x_k)\\) &amp; \\((y_i &gt; y_k)\\) or viceversa \\(\\tau = 1-\\frac{4I}{(n(n-1))}\\) 2.3 Principal components analyses PCA rotation of the coordinate system (see Fig. 2.1) Figure 2.1: Principal components are eigenvectors of the covariance or correlation matrix rotation of the coordinate system so that first component explains most variance second component explains most of the remaining variance and is perpendicular to the first one third component explains most of the remaining variance and is perpendicular to the first two … \\((x,y)\\) becomes \\((pc1, pc2)\\) where \\(pc1_i= b_{11} x_i + b_{12} y_i\\) \\(pc2_i = b_{21} x_i + b_{22} y_i\\) with \\(b_{jk}\\) being loadings pca &lt;- princomp(cbind(x,y), cor=TRUE) loadings(pca) ## ## Loadings: ## Comp.1 Comp.2 ## x 0.707 0.707 ## y 0.707 -0.707 ## ## Comp.1 Comp.2 ## SS loadings 1.0 1.0 ## Proportion Var 0.5 0.5 ## Cumulative Var 0.5 1.0 loadings of a component can be multiplied by -1 proportion of variance explained by each component number of components = number of variables summary(pca) ## Importance of components: ## Comp.1 Comp.2 ## Standard deviation 1.2910463 0.5772344 ## Proportion of Variance 0.8334002 0.1665998 ## Cumulative Proportion 0.8334002 1.0000000 outlook: components with low variance are shrinked to a higher degree in Ridge regression 2.3.1 Inferential statistics there is never a “yes-or-no” answer there will always be uncertainty Amrhein (2017)[https://peerj.com/preprints/26857] The decision whether an effect is important or not cannot not be done based on data alone. For a decision we should carefully consider the consequences of each decision, the aims we would like to achieve and the data. Consequences, needs and wishes of different stakeholders can be formally combined with the information in data by using methods of the decision theory. In most data analyses, particularly in basic research and when working on case studies, we normally do not consider consequences of decisions. In these cases, our job is extracting the information of data so that this information later can be used by other scientists, stakeholders and politicians to make decisions. Therefore, statistics is describing pattern in data and quantifying the uncertainty of the described patterns that is due to the fact that the data is just a (small) random sample from the population we would like to know. Quantification of uncertainty is only possible if the mechanisms under study are known the observations are a random sample from the population of interest Solutions: to 1. working with models and reporting assumptions to 2. study design reported uncertainties always are too small! Example: Number of stats courses before starting a PhD among all PhD students # simulate the virtual true population set.seed(235325) # set seed for random number generator # simulate fake data of the whole population statscourses &lt;- rpois(300000, rgamma(300000, 2, 3)) # draw a random sample from the population n &lt;- 12 # sample size y &lt;- sample(statscourses, 12, replace=FALSE) We observe the sample mean, what do we know about the population mean? Frequentist solution: How would the sample mean scatter, if we repeat the study many times? Bayesian solution: For any possible value, what is the probability that it is the true population mean? 2.4 Standard deviation and standard error frequentist SE = SD/sqrt(n) Bayesian SE = SD of posterior distribution 2.5 Central limit theorem / law of large numbers normal distribution = Gaussian distribution \\(p(\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(\\theta -\\mu)^2) = Normal(\\mu, \\sigma)\\) \\(E(\\theta) = \\mu\\), \\(var(\\theta) = \\sigma^2\\), \\(mode(\\theta) = \\mu\\) 2.6 Bayes theorem \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\) car flowers wine sum no 5 3 8 yes 1 4 5 ——- —————- —————- ——————- sum 6 7 13 What is the probability that the person likes wine given it has no car? \\(P(A) =\\) likes wine \\(= 0.54\\) \\(P(B) =\\) no car \\(= 0.62\\) \\(P(B|A) =\\) proportion car-free people among the wine liker \\(= 0.43\\) Knowing whether a persons owns a car increases the knowledge of the birthday preference. 2.7 Bayes theorem for continuous parameters \\(p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} = \\frac{p(y|\\theta)p(\\theta)}{\\int p(y|\\theta)p(\\theta) d\\theta}\\) \\(p(\\theta|y)\\): posterior distribution \\(p(y|\\theta)\\): likelihood, data model \\(p(\\theta)\\): prior distribution \\(p(y)\\): scaling constant 2.8 Single parameter model \\(p(y|\\theta) = Norm(\\theta, \\sigma)\\), with \\(\\sigma\\) known \\(p(\\theta) = Norm(\\mu_0, \\tau_0)\\) \\(p(\\theta|y) = Norm(\\mu_n, \\tau_n)\\), where \\(\\mu_n= \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau_0^2}+\\frac{n}{\\sigma^2}}\\) and \\(\\frac{1}{\\tau_n^2} = \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\\) \\(\\bar{y}\\) is a sufficient statistics \\(p(\\theta) = Norm(\\mu_0, \\tau_0)\\) is a conjugate prior for \\(p(y|\\theta) = Norm(\\theta, \\sigma)\\), with \\(\\sigma\\) known. Posterior mean = weighted average between prior mean and \\(\\bar{y}\\) with weights equal to the precisions (\\(\\frac{1}{\\tau_0^2}\\) and \\(\\frac{n}{\\sigma^2}\\)) 2.9 A model with two parameters \\(p(y|\\theta, \\sigma) = Norm(\\theta, \\sigma)\\) # weight (g) y &lt;- c(47.5, 43, 43, 44, 48.5, 37.5, 41.5, 45.5) n &lt;- length(y) \\(p(y|\\theta, \\sigma) = Norm(\\theta, \\sigma)\\) \\(p(\\theta, \\sigma) = N-Inv-\\chi^2(\\mu_0, \\sigma_0^2/\\kappa_0; v_0, \\sigma_0^2)\\) conjugate prior \\(p(\\theta,\\sigma|y) = \\frac{p(y|\\theta, \\sigma)p(\\theta, \\sigma)}{p(y)} = N-Inv-\\chi^2(\\mu_n, \\sigma_n^2/\\kappa_n; v_n, \\sigma_n^2)\\), with \\(\\mu_n= \\frac{\\kappa_0}{\\kappa_0+n}\\mu_0 + \\frac{n}{\\kappa_0+n}\\bar{y}\\) \\(\\kappa_n = \\kappa_0+n\\) \\(v_n = v_0 +n\\) \\(v_n\\sigma_n^2=v_0\\sigma_0^2+(n-1)s^2+\\frac{\\kappa_0n}{\\kappa_0+n}(\\bar{y}-\\mu_0)^2\\) \\(\\bar{y}\\) and \\(s^2\\) are sufficient statistics Joint, marginal and conditional posterior distributions 2.10 t-distribution marginal posterior distribution of a normal mean with unknown variance and conjugate prior distribution \\(p(\\theta|v,\\mu,\\sigma) = \\frac{\\Gamma((v+1)/2)}{\\Gamma(v/2)\\sqrt{v\\pi}\\sigma}(1+\\frac{1}{v}(\\frac{\\theta-\\mu}{\\sigma})^2)^{-(v+1)/2}\\) \\(v\\) degrees of freedom \\(\\mu\\) location \\(\\sigma\\) scale 2.11 Frequentist one-sample t-test H0: the mean weight is equal to exactly 40g. \\(t = \\frac{\\bar{y}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\) t.test(y, mu=40) ## ## One Sample t-test ## ## data: y ## t = 3.0951, df = 7, p-value = 0.01744 ## alternative hypothesis: true mean is not equal to 40 ## 95 percent confidence interval: ## 40.89979 46.72521 ## sample estimates: ## mean of x ## 43.8125 2.12 Nullhypothesis test p-value: Probability of the data or more extreme data given the null hypothesis is true. 2.13 Confidence interval # lower limit of 95% CI mean(y) + qt(0.025, df=7)*sd(y)/sqrt(n) # upper limit of 95% CI mean(y) + qt(0.975, df=7)*sd(y)/sqrt(n) 2.14 Posterior distribution Two different theories - one single result! 2.15 Posterior probability Probability \\(P(H:\\mu&lt;=40) =\\) 0.01 2.16 Monte Carlo simulation (parametric bootstrap) Monte Carlo integration: numerical solution of \\(\\int_{-1}^{1.5} F(x) dx\\) sim is solving a mathematical problem by simulation How sim is simulating to get the marginal distribution of \\(\\mu\\): 2.17 3 methods for getting the posterior distribution analytically approximation Monte Carlo simulation 2.18 Grid approximation \\(p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)}\\) For example, one coin flip (Bernoulli model) data: y=0 (a tail) likelihood: \\(p(y|\\theta)=\\theta^y(1-\\theta)^{(1-y)}\\) 2.19 Monte Carlo simulations Markov chain Monte Carlo simulation (BUGS, Jags) Hamiltonian Monte Carlo (Stan) 2.20 Comparison of the locations between two groups Boxplot: Median, 50% box, extremes observation within 1.5 times the interquartile range, outliers The uncertainties of the means do not show the uncertainty of the difference between the means! 2.21 Difference between two means mod &lt;- lm(ell~birthday, data=dat) mod ## ## Call: ## lm(formula = ell ~ birthday, data = dat) ## ## Coefficients: ## (Intercept) birthdaywine ## 43.3333 0.6667 bsim &lt;- sim(mod, n.sim=nsim) quantile(bsim@coef[,2], prob=c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## -1.6524462 0.6698085 2.9975599 2.22 Two-sample t-test t.test(ell~birthday, data=dat, var.equal=TRUE) ## ## Two Sample t-test ## ## data: ell by birthday ## t = -0.63369, df = 11, p-value = 0.5392 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.982185 1.648851 ## sample estimates: ## mean in group flowers mean in group wine ## 43.33333 44.00000 2.23 Wilxocon test wilcox.test(ell~birthday, data=dat) ## ## Wilcoxon rank sum test with continuity correction ## ## data: ell by birthday ## W = 18, p-value = 0.7172 ## alternative hypothesis: true location shift is not equal to 0 2.24 Randomisation test diffH0 &lt;- numeric(nsim) for(i in 1:nsim){ randbirthday &lt;- sample(dat$birthday) rmod &lt;- lm(ell~randbirthday, data=dat) diffH0[i] &lt;- coef(rmod)[2] } mean(abs(diffH0)&gt;abs(coef(mod)[2])) # p-value ## [1] 0.4898 Produces the distribution of a test statistics given the null hypothesis. assumption: all observations are independent becomes unfeasible when data is structured 2.25 Bootstrap diffboot &lt;- numeric(nsim) for(i in 1:nsim){ nbirthday &lt;- 1 while(nbirthday==1){ bootrows &lt;- sample(1:nrow(dat), replace=TRUE) nbirthday &lt;- length(unique(dat$birthday[bootrows])) } rmod &lt;- lm(ell~birthday, data=dat[bootrows,]) diffboot[i] &lt;- coef(rmod)[2] } quantile(diffboot, prob=c(0.025, 0.975)) ## 2.5% 97.5% ## -1.214286 2.725327 result is a confidence interval assumption: all observations are independent! hist(diffboot); abline(v=coef(mod)[2], lwd=2, col=&quot;red&quot;) 2.26 F-distribution Ratios of sample variances drawn from populations with equal variances follow an F-distribution. The density function of the F-distribution is even more complicated than the one of the t-distribution! We do not copy it here. Further, we have not yet met any Bayesian example where the F-distribution is used (that does not mean that there is no). It is used in frequentist analyses in order to compare variances, and, within the ANOVA, to compare means between groups. If two variances only differ because of natural variance in the data (nullhypothesis) then \\(\\frac{Var(X_1)}{Var(X_2)}\\sim F_{df_1,df_2}\\). Figure 2.2: Different density functions of the F statistics 2.26.1 Analysis of variance ANOVA The aim of an ANOVA is to compare means of groups. In a frequentist analysis, this is done by comparing the between-group with the within-group variance. The result of a Bayesian analysis is the joint posterior distribution of the group means. Figure 2.3: Number of stats courses students have taken before starting a PhD in relation to their feeling about statistics. In the frequentist ANOVA, the following three sum of squared distances (SS) are used to calculate the total, the between- and within-group variances: Total sum of squares = SST = \\(\\sum_1^n{(y_i-\\bar{y})^2}\\) Within-group SS = SSW = \\(\\sum_1^n{(y_i-\\bar{y_g})^2}\\): unexplained variance Between-group SS = SSB = \\(\\sum_1^g{n_g(\\bar{y_g}-\\bar{y})^2}\\): explained variance The between-group and within-group SS sum to the total sum of squares: SST=SSB+SSW. Attention: this equation is only true in any case for a simple one-way ANOVA (just one grouping factor). If the data are grouped according to more than one factor (such as in a two- or three-way ANOVA), then there is one single solution for the equation only when the data is completely balanced, i.e. when there are the same number of observations in all combinations of factor levels. For non-balanced data with more than one grouping factor, there are different ways of calculating the SSBs, and the result of the F-test described below depends on the order of the predictors in the model. Figure 2.4: Visualisation of the total, between-group and within-group sum of squares. Points are observations; long horizontal line is the overall mean; short horizontal lines are group specific means. In order to make SSB and SSW comparable, we have to divide them by their degrees of freedoms. For the within-group SS, SSW, the degrees of freedom is the number of obervations minus the number of groups (\\(g\\)), because \\(g\\) means have been estimated from the data. If the \\(g\\) means are fixed and \\(n-g\\) data points are known, then the last \\(g\\) data points are defined, i.e., they cannot be chosen freely. For the between-group SS, SSB, the degrees of freedom is the number of groups minus 1 (the minus 1 stands for the overall mean). MSB = SSB/df_between, MSW = SSW/df_within It can be shown (by mathematicians) that, given the nullhypothesis, the mean of all groups are equal \\(m_1 = m_2 = m_3\\), then the mean squared errors between groups (MSB) is expected to be equal to the mean squared errors within the groups (MSW). Therefore, the ration MSB/MSW is expected to follow an F-distribution given the nullhypothesis is true. MSB/MSW ~ F(df_between, df_within) The Bayesian analysis for comparing group means consists of calculating the posterior distribution for each group mean and then drawing inference from these posterior distributions. A Bayesian one-way ANOVA involves the following steps: 1. Decide for a data model: We, here, assume that the measurements are normally distributed around the group means. In this example here, we transform the outcome variable in order to better meet the normal assumption. Note: the frequentist ANOVA makes exactly the same assumptions. We can write the data model: \\(y_i\\sim Norm(\\mu_i,\\sigma)\\) with \\(mu_i= \\beta_0 + \\beta_1I(group=2) +\\beta_1I(group=3)\\), where the \\(I()\\)-function is an indicator function taking on 1 if the expression is true and 0 otherwise. This model has 4 parameters: \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\sigma\\). # fit a normal model with 3 different means mod &lt;- lm(log(nrcourses+1)~statsfeeling, data=dat) Choose a prior distribution for each model parameter: In this example, we choose flat prior distributions for each parameter. By using these priors, the result should not remarkably be affected by the prior distributions but almost only reflect the information in the data. We choose so-called improper prior distributions. These are completely flat distributions that give all parameter values the same probability. Such distributions are called improper because the area under the curve is not summing to 1 and therefore, they cannot be considered to be proper probability distributions. However, they can still be used to solve the Bayesian theorem. Solve the Bayes theorem: The solution of the Bayes theorem for the above priors and model is implemented in the function sim of the package arm. # calculate numerically the posterior distributions of the model # parameters using flat prior distributions nsim &lt;- 5000 set.seed(346346) bsim &lt;- sim(mod, n.sim=nsim) Display the joint posterior distributions of the group means # calculate group means from the model parameters newdat &lt;- data.frame(statsfeeling=levels(dat$statsfeeling)) X &lt;- model.matrix(~statsfeeling, data=newdat) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- X%*%bsim@coef[i,] hist(fitmat[1,], freq=FALSE, breaks=seq(-2.5, 4.2, by=0.1), main=NA, xlab=&quot;Group mean of log(number of courses +1)&quot;, las=1, ylim=c(0, 2.2)) hist(fitmat[2,], freq=FALSE, breaks=seq(-2.5, 4.2, by=0.1), main=NA, xlab=&quot;&quot;, las=1, add=TRUE, col=rgb(0,0,1,0.5)) hist(fitmat[3,], freq=FALSE, breaks=seq(-2.5, 4.2, by=0.1), main=NA, xlab=&quot;&quot;, las=1, add=TRUE, col=rgb(1,0,0,0.5)) legend(2,2, fill=c(&quot;white&quot;,rgb(0,0,1,0.5), rgb(1,0,0,0.5)), legend=levels(dat$statsfeeling)) Figure 2.5: Posterior distributions of the mean number of stats courses PhD students visited before starting the PhD grouped according to their feelings about statistics. Based on the posterior distributions of the group means, we can extract derived quantities depending on our interest and questions. Here, for example, we could extract the posterior probability of the hypothesis that students with a positive feeling about statistics have a better education in statistics than those with a neutral or negative feeling about statistics. # P(mean(positive)&gt;mean(neutral)) mean(fitmat[3,]&gt;fitmat[2,]) ## [1] 0.9004 # P(mean(positive)&gt;mean(negative)) mean(fitmat[3,]&gt;fitmat[1,]) ## [1] 0.953 2.27 Chisquare test The chisquare test is used for two frequentist purposes. 1. Testing for correlations between two categorical variables. 2. Comparison of two distributions (goodness of fit test) When testing for correlations between two categorical variables, then the nullhypothesis is “there is no correlation”. The data can be displayed in cross-tables. # Example: correlation between birthday preference and car ownership table(dat$birthday, dat$car) ## ## N Y ## flowers 5 1 ## wine 3 4 Given the nullhypothesis is true, we expect that the distribution of the data in each column of the cross-table is similar to the distribution of the row-sums. And, the distribution of the data in each row should be similar to the distribution of the column-sums. The chisquare test statistics \\(\\chi^2\\) measures the deviation of the data from this expected distribution of the data in the cross-table. For calculating the chisquare test statistics \\(\\chi^2\\), we first have to obtain for each cell in the cross-table the expected value \\(E_{ij}\\) = rowsum*colsum/total. \\(\\chi^2\\) measures the difference between the observed \\(O_{ij}\\) and expected \\(E_{ij}\\) values as: \\(\\chi^2=\\sum_{i=1}^{m}\\sum_{j=1}^{k}\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\) where \\(m\\) is the number of rows and \\(k\\) is the number of columns. The \\(\\chi^2\\)-distribution has 1 parameter, the degrees of freedom \\(v\\) = \\((m-1)(k-1)\\). R is calculating the \\(\\chi^2\\) value for specific cross-tables, and it is also giving the p-values, i.e., the probability of obtaining the observed or a higher \\(\\chi^2\\) value given the nullhypothesis is true by comparing the observed \\(\\chi^2\\) with the corresponding chisquare distribution. chisq.test(table(dat$birthday, dat$car)) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: table(dat$birthday, dat$car) ## X-squared = 0.85312, df = 1, p-value = 0.3557 The warning (that is suppressed in the rmarkdown version, but that you will see if you run the code on your own computer) is given, because in our example some cells have counts less than 5. In such cases, the Fisher’s exact test should be preferred. This test calculates the p-value analytically using probability theory, whereas the chisquare test relies on the assumption that the \\(\\chi^2\\) value follows a chisquare distribution. The latter assumption holds better for larger sample sizes. fisher.test(table(dat$birthday, dat$car)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(dat$birthday, dat$car) ## p-value = 0.2657 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.3369761 391.2320030 ## sample estimates: ## odds ratio ## 5.699654 2.28 Bayesian way of analysing correlations between categorical variables For a Bayesian analysis of cross-table data, a data model has to be found. There are several possibilities that could be used: a so-called log-linear model (Poisson model) for the counts in each cell of the cross-table. a binomial or a multinomial model for obtaining estimates of the proportions of data in each cell These models provide possibilities to explore the patterns in the data in more details than a chisquare test. # log-linear model mod &lt;- glm(count~birthday+car + birthday:car, data=datagg, family=poisson) bsim &lt;- sim(mod, n.sim=nsim) round(t(apply(bsim@coef, 2, quantile, prob=c(0.025, 0.5, 0.975))),2) ## 2.5% 50% 97.5% ## (Intercept) 0.74 1.61 2.47 ## birthdaywine -1.91 -0.50 0.90 ## carY -3.71 -1.62 0.52 ## birthdaywine:carY -0.67 1.88 4.40 The interaction parameter measures the strength of the correlation. To quantitatively understand what a parameter value of 1.90 means, we have to look at the interpretation of all parameter values. We do that here quickly without a thorough explanation, because we already explained the Poisson model in chapter 8 of (F. Korner-Nievergelt et al. 2015). The intercept 1.61 corresponds to the logarithm of the count in the cell “flowers” and “N” (number of students who prefer flowers as a birthday present and who do not have a car), i.e., \\(exp(\\beta_0)\\) = 5.00. The exponent of the second parameter corresponds to the multiplicative difference between the counts in the cells “flowers and N” and “wine and N”, i.e., count in the cell “wine and N” = \\(exp(\\beta_0)exp(\\beta_1)\\) = exp(1.61)exp(-0.51) = 3.00. The third parameter measures the multiplicative difference in the counts between the cells “flowers and N” and “flowers and Y”, i.e., count in the cell “flowers and Y” = \\(exp(\\beta_0)exp(\\beta_2)\\) = exp(1.61)exp(-1.61) = 1.00. Thus, the third parameter is the difference in the logarithm of the counts between the car owners and the car-free students for those who prefer flowers. The interaction parameter is the difference of this difference between the students who prefer wine and those who prefer flowers. This is difficult to intuitively understand. Here is another try to formulate it: The interaction parameter measures the difference in the logarithm of the counts in the cross-table between the row-differences between the columns. Maybe it becomes clear, when we extract the count in the cell “wine and Y” from the model parameters: \\(exp(\\beta_0)exp(\\beta_1)exp(\\beta_2)exp(\\beta_3)\\) = exp(1.61)exp(-0.51)exp(-1.61)exp(1.90) = 4.00. Alternatively, we could estimate the proportions of flower and wine preferers within each group of car owners and car-free students using a binomial model. For an explanation of the binomial model, see chapter 8 of (F. Korner-Nievergelt et al. 2015). # binomial model tab &lt;- table(dat$car,dat$birthday) mod &lt;- glm(tab~rownames(tab), family=binomial) bsim &lt;- sim(mod, n.sim=nsim) Figure 2.6: Estimated proportion of students that prefer flowers over wine as a birthday present among the car-free students (N) and the car owners (Y). Given are the median of the posterior distribution (circle). The bar extends between the 2.5% and 97.5% quantiles of the posterior distribution. 2.29 Summary Bayesian data analysis = applying the Bayes theorem for summarising knowledge based on data, priors and the model assumptions. Frequentist statistics = quantifying uncertainty by hypothetical repetitions "],
["analyses-steps.html", "3 Data analysis step by step 3.1 Plausibility of Data 3.2 Relationships 3.3 Data Distribution 3.4 Preparation of Explanatory Variables 3.5 Data Structure 3.6 Define Prior Distributions 3.7 Fit the Model 3.8 Check Model 3.9 Model Uncertainty 3.10 Draw Conclusions Further reading", " 3 Data analysis step by step In this chapter we provide a checklist with some guidance for data analysis. However, do not expect the list to be complete and for different studies, a different order of the steps may make more sense. We usually repeat steps 3.2 to 3.8 until we find a model that fit the data well and that is realistic enough to be useful for the intended purpose. Data analysis is always a lot of work and, often, the following steps have to be repeated many times until we find a useful model. There is a chance and danger at the same time: we may find interesting results that answer different questions than we asked originally. They may be very exciting and important, however they may be biased. We can report such findings, but we should state that they appeared (more or less by chance) during the data exploration and model fitting phase, and we have to be aware that the estimates may be biased because the study was not optimally designed with respect to these findings. It is important to always keep the original aim of the study in mind. Do not adjust the study question according to the data. We also recommend reporting what the model started with at the first iteration and describing the strategy and reasoning behind the model development process. 3.1 Plausibility of Data Prepare the data and check graphically, or via summary statistics, whether all the data are plausible. Prepare the data so that errors (typos, etc.) are minimal, for example, by double-checking the entries. See chapter 4 for useful R-code that can be used for data preparation and to make plausibility controls. 3.2 Relationships Think about the direct and indirect relationships among the variables of the study. We normally start a data analysis by drawing a sketch of the model including all explanatory variables and interactions that may be biologically meaningful. We will most likely repeat this step after having looked at the model fit. To make the data analysis transparent we should report every model that was considered. A short note about why a specific model was considered and why it was discarded helps make the modeling process reproducible. 3.3 Data Distribution What is the nature of the variable of interest (outcome, dependent variable)? At this stage, there is no use of formally comparing the distribution of the outcome variable to a statistical distribution, because the rawdata is not required to follow a specific distribution. The models assume that conditional on the explanatory variables and the model structure, the outcome variable follows a specific distribution. Therefore, checking how well the chosen distribution fits to the data is done after the model fit 3.8. This first choice is solely done based on the nature of the data. Normally, our first choice is one of the classical distributions for which robust software for model fitting is available. Here is a rough guideline for this first choice: continuous measurements \\(\\Longrightarrow\\) normal distribution &gt; exceptions: time-to-event data \\(\\Longrightarrow\\) see survival analysis count \\(\\Longrightarrow\\) Poisson or negative-binomial distribution count with upper bound (proportion) \\(\\Longrightarrow\\) binomial distribution binary \\(\\Longrightarrow\\) Bernoully distribution rate (count by a reference) \\(\\Longrightarrow\\) Poisson including an offset nominal \\(\\Longrightarrow\\) multinomial distribution Chapter 5 gives an overview of the distributions that are most relevant for ecologists. 3.4 Preparation of Explanatory Variables Look at the distribution (histogram) of every explanatory variable: Linear models do not assume that the explanatory variables have any specific distribution. Thus there is no need to check for a normal distribution! However, very skewed distributions result in unequal weighting of the observations in the model. In extreme cases, the slope of a regression line is defined by one or a few observations only. We also need to check whether the variance is large enough, and to think about the shape of the expected effect. The following four questions may help with this step: Is the variance (of the explanatory variable) big enough so that an effect of the variable can be measured? Is the distribution skewed? If an explanatory variable is highly skewed, it may make sense to transform the variable (e.g., log, square-root). Does it show a bimodal distribution? Consider making the variable binary. Is it expected that a change of 1 at lower values for x has the same biological effect as a change of 1 at higher values of x? If not, a trans- formation (e.g., log) could linearize the relationship between x and y. Centering: Centering (\\(x.c = x-mean(x)\\)) is a transformation that produces a variable with a mean of 0. Centering is optional. We have two reasons to center a predictor variable. First, it helps the model fitting algorithm to better converge because it reduces correlations among model parameters. Second, with centered predictors, the intercept and main effects in the linear model are better interpretable (they are measured at the center of the data instead of at the covariate value of 0 which may be far off). Scaling: Scaling (\\(x.s = x/c\\), where \\(c\\) is a constant) is a transformation that changes the unit of the variable. Also scaling is optional. We have three reasons to scale an predictor variable. First, to make the effect sizes better understandable. For example, a population change from one year to the next may be very small and hard to interpret. When we give the change for a 10-year period, its ecological meaning is better understandable. Second, to make the estimate of the effect sizes comparable between variables, we may use \\(x.s = x/sd(x)\\). The resulting variable has a unit of one standard deviation. A standard deviation may be comparable between variables that oritinally are measured in different units (meters, seconds etc). Andrew Gelman and Hill (2007) (p. 55 f) propose to scale the variables by two times the standard deviation (\\(x.s = x/(2*sd(x))\\)) to make effect sizes comparable between numeric and binary variables. Third, scaling can be important for model convergence, especially when polynomials are included. Also, consider the use of orthogonal polynomials, see Chapter 4.2.9 in F. Korner-Nievergelt et al. (2015). Collinearity: Look at the correlation among the explanatory variables (pairs plot or correlation matrix). If the explanatory variables are correlated, go back to step 2. Also, Chapter 4.2.7 in F. Korner-Nievergelt et al. (2015) discusses collinearity. Are interactions and polynomial terms needed in the model? If not already done in step 2, think about the relationship between each explanatory variable and the dependent variable. Is it linear or do polynomial terms have to be included in the model? If the relationship cannot be described appropriately by polynomial terms, think of a nonlinear model or a generalized additive model (GAM). May the effect of one explanatory variable depend on the value of another explanatory variable (interaction)? 3.5 Data Structure After having taken into account all of the (fixed effect) terms from step 4: are the observations independent or grouped/structured? What random factors are needed in the model? Are the data obviously temporally or spatially correlated? Or, are other correlation structures present, such as phylogenetic relationships? Our strategy is to start with a rather simple model that may not account for all correlation structures that in fact are present in the data. We first, only include those that are known to be important a priory. Only when residual analyses reveals important additional correlation structures, we include them in the model. 3.6 Define Prior Distributions Decide whether we would like to use informative prior distributions or whether we would like use priors that only have a negligible effect on the results. When the results are later used for informing authorities or for making a decision (as usual in applied sciences), then we would like to base the results on all information available. Information from the literature is then used to construct informative prior distributions. In contrast to applied sciences, in basic research we often would like to show only the information in the data that should not be influenced by earlier results. Therefore, in basic research we look for priors that do not influence the results. 3.7 Fit the Model Fit the model. 3.8 Check Model We assess model fit by graphical analyses of the residuals (Chapter 6 in F. Korner-Nievergelt et al. (2015)), by predictive model checking (Section 10.1 in F. Korner-Nievergelt et al. (2015)), or by sensitivity analysis (Chapter 15 in F. Korner-Nievergelt et al. (2015)). For non-Gaussian models it is often easier to assess model fit using pos- terior predictive checks (Chapter 10 in F. Korner-Nievergelt et al. (2015)) rather than residual analyses. Posterior predictive checks usually show clearly in which aspect the model failed so we can go back to step 2 of the analysis. Recognizing in what aspect a model does not fit the data based on residual plots improves with experience. Therefore, we list in Chapter 16 of F. Korner-Nievergelt et al. (2015) some patterns that can appear in residual plots together with what these patterns possibly indicate. We also indicate what could be done in the specific cases. 3.9 Model Uncertainty If, while working through steps 1 to 8, possibly repeatedly, we came up with one or more models that fit the data reasonably well, we then turn to the methods presented in Chapter 11 (F. Korner-Nievergelt et al. (2015)) to draw inference from more than one model. If we have only one model, we proceed to 3.10. 3.10 Draw Conclusions Simulate values from the joint posterior distribution of the model parameters (sim or Stan). Use these samples to present parameter uncertainty, to obtain posterior distributions for predictions, probabilities of specific hypotheses, and derived quantities. Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],
["datamanip.html", "4 Data preparation 4.1 Basic operations 4.2 Joining tables 4.3 Further reading", " 4 Data preparation 4.1 Basic operations Alle Packete laden library(tidyverse) oder nur library(dplyr). dat &lt;- iris %&gt;% as.tibble() %&gt;% filter(Sepal.Length &gt; 5) %&gt;% group_by(Species) %&gt;% summarise(n = n(), mittel = mean(Petal.Length)) 4.2 Joining tables Beschreiben wie left_join funktioniert. 4.3 Further reading R for Data Science by Garrett Grolemund and Hadley Wickham: Introduces the tidyverse framwork. It explains how to get data into R, get it into the most useful structure, transform it, visualise it and model it. "],
["distributions.html", "5 Probability distributions 5.1 Introduction 5.2 Normal distribution 5.3 Poisson distribution 5.4 Gamma distribution", " 5 Probability distributions 5.1 Introduction hist(rnorm(1000)) Figure 5.1: Das ist ein Versuch. 5.2 Normal distribution xxx 5.3 Poisson distribution xxx 5.4 Gamma distribution xxx 5.4.1 Cauchy distribution We sometimes use the Cauchy distiribution to specify the prior distirbution of the standart deviation and similar parameters in a model. An example for this is the regression model that we used to introduce Stan (Chapter 12.3). "],
["figures.html", "6 Visualizations 6.1 Short Checklist for figures Further reading", " 6 Visualizations 6.1 Short Checklist for figures The figure should represent the answer to the study question. Often, the classical types of plots such as a scatterplot, a bar plot, or an effects plot are sufficient. However, in many cases, adding a little bit of creativity can greatly improve readability or the message of the figure. 2.Label the x- and y-axes. Make sure that the units are indicated either within the title or in the figure legend. Starty-axisatzeroifthereferencetozeroisimportantfortheinterpretationof effects. The argument ylim[c(0, max(dat$y)) in R is used for this purpose. Scale the axes so that all data are shown. Make sure that sample size is indicated either in the figure or in the legend (or, at least, easy to find in the text). Use interpretable units. That means, if the variable on the x-axis has been z-transformed to fit the model, back-transform the effects to the original scale. Give the raw data whenever possible. Sometimes, a significant effect cannot be seen in the raw data because so many other variables have an influence on the outcome. Then, you may prefer showing the effect only (e.g., a regression line with a credible interval) and give the residual standard deviation in the figure legend. Even then, we think it is important to show the raw data graphically somewhere else in the paper or in the supplementary material. A scatterplot of the data can contain structures that are lost in summary statistics. Draw the figures as simply as possible. Avoid 3D graphics. Delete all unnecessary elements. Reduce the number of different colors to a minimum necessary. A color scale from orange to blue gives a gray scale in a black-and-white print. colorRampPalette(c(&quot;orange&quot;, &quot;blue&quot;))(5) produces five colors on a scale from orange to blue. Remember that around 8% of the northern European male population have difficulties distinguishing red from green but it is easier for them to distinguish orange from blue. Further reading Data Visualization. A practical introduction: A practical introduction to data visulization in R. Good data visualizations also make it easier to communicate your ideas and findings to other people. Beyond that, producing effective plots from your own data is the best way to develop a good eye for reading and understanding graphs—good and bad—made by others. Fundamentals of Data Visualization: A guide to making visualizations that accurately reflect the data, tell a story, and look professional. […] This is an online preview of the book “Fundamentals of Data Visualization” to be published with O’Reilly Media, Inc. Completed chapters will be posted here as they become available. The book is meant as a guide to making visualizations that accurately reflect the data, tell a story, and look professional. It has grown out of my experience of working with students and postdocs in my laboratory on thousands of data visualizations. "],
["rgis.html", "7 Spatial analyses and maps 7.1 Data types 7.2 Basic functions 7.3 Further reading", " 7 Spatial analyses and maps Almost all the things that we can do with traditional geographic information system we can also do within R. If we do it in R we get the useful benefits of a script approach that allows for reproducible analyses (see Chapter 8) and that can be scaled to many more objects or larger data sets. Here we simply introduce the packages and functions that we most often use when working with spatial data. 7.1 Data types 7.1.1 Raster data Very broadly speaking, we divide spatial data into two categories, raster data and all other types of data for points, lines or polygons. Raster data consists of a grid (i.e. a matrix) where each cell of the grid contains one or several values representing the spatial information. The R-package raster is very efficient for raster data. We can use the function raster() to load raster data from files. Most of the common file formats for raster data such as .geotiff, .tif or .grd are supported. However, raster data can also be converted from tables with one column for the x-coordinates, one column for the y-coordinates and one column for the spatial information. The coordinates must be from a regular grid. For example the freely available topographic data of Switzerland are collected on a 100m x 100m grid. In the following example the tibble elevations contains the elevation data from the canton of Aargau and is converted into raster data using the function rasterFromXYZ(). library(raster) ra &lt;- rasterFromXYZ(elevation) plot(ra) Figure 7.1: Meter above sea leavel (m) accross the canton Aargau in Switzerland. 7.1.2 Geometry data All geometry data types are composed of points. The spatial location of a point is defined by its x and y coordinates. Using several points one can then define lines (sequence of points connected by straight lines) and polygons (sequence of points that form a closed ring). Points, lines and plygons are the geometries we usually work with. We use the package sf to work with geometry data types. Its functions are very efficient to work with all spatial data other than raster data. It also links to GDAL (i.e. a computer software library for reading and writing raster and vector geospatial data formats) and proj.4 (i.e. a library for performing conversions between cartographic projections), which are important tools when woring with different sources of spatial data. We can used the function st_read() to read geometry data from file or database. In the following example, however, we convert the tibble frogs into a simple feature collection. The data file frogs, formatted as a tibble, contains different columns including the counts, variables that describe the ponds as well as the spatial coordinates of the counts. The simple feature collection looks rather similar to the original tibble, however instead of the x and y colomns it now contains the column geometry. With the simple feature collection we can work pretty much in the same way as we use to work with tibbles. For example we can filter only the data from 2011, select the geometries and plot them on the top of the raster with the elevation across the entire canton of Aargau (see 7.1.1 for the raster data). library(sf) dat &lt;- frogs %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs = 21781) plot(ra) dat %&gt;% filter(year == 2011) %&gt;% st_geometry() %&gt;% plot(add = TRUE, pch = &quot;x&quot;) Figure 7.2: Locations of the ponds where frogs were counted in 2011. The background image shows the elevation (meter above sea level). 7.2 Basic functions In this chapter we shortly describe some functions that we often use when working with spatial data in R. 7.2.1 Coordinate systems An important aspect of spatial data is the coordinate reference system (CRS). A CRS determines for instance where the center of the map is, the units for the coordinates and others. PROJ.4 is an open source software library that is commonly used for CRS transformation. Most commonly used CRSs have been assigned a HERE IS SOMETHING MISSING. The EPSG (European Petroleum Survey Group) code is a unique ID that can be used to identify a CRS. Thus if we know the EPSG code it is rather simple to transform spatial data into other CRS. To search for the correct EPSG code we can use https://www.epsg-registry.org or http://www.spatialreference.org The following code shows how to assign the CRS of existing data and how to transform the coordinate system for raster data and sf data, respectively. # Assign CRS for raster data crs(ra) &lt;- CRS(&quot;+init=epsg:21781&quot;) # Assign CRS for sf data st_crs(dat) &lt;- 21781 # Transfrom raster data to WGS84 projectRaster(ra, crs = CRS(&quot;+init=epsg:4326&quot;)) # Transfrom sf data to WGS84 st_transform(dat, crs = 4326) 7.2.2 Joining spatial data Joining two non-spatial datasets relies on a shared variable (key) using for instance the function left_join(), as described in chapter 4.2. In a spatial context, we apply the exact same concept except for the key being a shared areas of geographic space. Note, however, that people with a background in geographic information systems may call these operations differently, such as spatial overlay or intersection. For geometry data we can use the function st_join(). As an example, we aim to add the biogegraphic Region to the counts of the number of frogs in ponds of the Canton Aargau. We transform the frogs data into a spatial object (i.e. a simple feature collection). The polygons of the biogegraphic regions in Switzerland are available in the object bgr. To each of the polygons additional data are available and we aim to extract the information from the column BIOGREG_R6, which contains the name of the biogeographic region. We add this column to the frogs data using the function st_join() as following. load(&quot;RData/bgr.RData&quot;) bgr &lt;- bgr %&gt;% st_transform(21781) dat &lt;- frogs %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs = 21781) dat &lt;- dat %&gt;% st_join(bgr[&quot;BIOGREG_R6&quot;]) 7.3 Further reading Geocomputation with R: This online-book is aimed at people who want to do spatial data analysis, visualization and modeling using open source software and reproducible workflows. Spatial Data Analysis and Modeling with R: Online introduction to do spatial analyses with R. Good introduction to coordinate systems and projections. "],
["reproducible.html", "8 Reproducible Research 8.1 Introduction 8.2 Further reading", " 8 Reproducible Research 8.1 Introduction xxx 8.2 Further reading Rmarkdown: The first official book authored by the core R Markdown developers that provides a comprehensive and accurate reference to the R Markdown ecosystem. With R Markdown, you can easily create reproducible data analysis reports, presentations, dashboards, interactive applications, books, dissertations, websites, and journal articles, while enjoying the simplicity of Markdown and the great power of R and other languages. Bookdown by Yihui Xie: A guide to authoring books with R Markdown, including how to generate figures and tables, and insert cross-references, citations, HTML widgets, and Shiny apps in R Markdown. The book can be exported to HTML, PDF, and e-books (e.g. EPUB). The book style is customizable. You can easily write and preview the book in RStudio IDE or other editors, and host the book wherever you want (e.g. bookdown.org). Our book is written using bookdown. "],
["furthertopics.html", "9 Further topics 9.1 Bioacoustic analyse 9.2 Python", " 9 Further topics This is a collection of short introductions or links with commented R code that cover other topics that might be usful for ecologists. 9.1 Bioacoustic analyse Bioacoustic analyses are nicely covered in a blog by Marcelo Araya-Salas. 9.2 Python Like R, python is a is a high-level programming language that is used by many ecologists. The reticulate package provides a comprehensive set of tools for interoperability between Python and R. "],
["PART-II.html", "10 Introduction to PART II Further reading", " 10 Introduction to PART II Further reading A really good introductory book to Bayesian data analyses is (McElreath 2016). This book starts with a thorough introduction to applying the Bayes theorem for drawing inference from data. In addition, it carefully discusses what can and what cannot be concluded from statistical results. We like this very much. We like looking up statistical methods in papers and books written by Andrew Gelman (e.g. A Gelman et al. 2014) and Trevor Hastie (e.g. (T. Hastie, Tibshirani, and Friedman 2009, Efron and Hastie (2016))) because both explain complicated things in a concise and understandable way. "],
["priors.html", "11 Prior distributions 11.1 Introduction 11.2 How to choose a prior 11.3 Prior sensitivity", " 11 Prior distributions 11.1 Introduction 11.2 How to choose a prior Tabelle von Fränzi (CourseIII_glm_glmmm/course2018/presentations_handouts/presentations) 11.3 Prior sensitivity xxx "],
["stan.html", "12 MCMC using Stan 12.1 Background 12.2 Install rstan 12.3 Writing a Stan model 12.4 Run Stan from R Further reading", " 12 MCMC using Stan 12.1 Background Markov chain Monte Carlo (MCMC) simulation techniques were developed in the mid-1950s by physicists (Metropolis et al., 1953). Later, statisticians discovered MCMC (Hastings, 1970; Geman &amp; Geman, 1984; Tanner &amp; Wong, 1987; Gelfand et al., 1990; Gelfand &amp; Smith, 1990). MCMC methods make it possible to obtain posterior distributions for parameters and latent variables (unobserved variables) of complex models. In parallel, personal computer capacities increased in the 1990s and user-friendly software such as the different programs based on the programming language BUGS (Spiegelhalter et al., 2003) came out. These developments boosted the use of Bayesian data analyses, particularly in genetics and ecology. 12.2 Install rstan In this book we use the program Stan to draw random samples from the joint posterior distribution of the model parameters given a model, the data, prior distributions, and initial values. To do so, it uses the “no-U-turn sampler,” which is a type of Hamiltonian Monte Carlo simulation (Hoffman and Gelman 2014; Betancourt 2013), and optimization-based point estimation. These algorithms are more efficient than the ones implemented in BUGS programs and they can handle larger data sets. Stan works particularly well for hierar- chical models (Betancourt and Girolami 2013). Stan runs on Windows, Mac, and Linux and can be used via the R interface rstan. Stan is automatically installed when the R package rstan is installed. For installing rstan, it is advised to follow closely the system-specific instructions. 12.3 Writing a Stan model The statistical model is written in the Stan language and saved in a text file. The Stan language is rather strict, forcing the user to write unambiguous models. Stan is very well documented and the Stan Documentation contains a comprehensive Language Manual, a Wiki documentation and various tutorials. We here provide a normal regression with one predictor variable as a worked example. The entire Stan model is as following (saved as linreg.stan) data { int&lt;lower=0&gt; n; vector[n] y; vector[n] x; } parameters { vector[2] beta; real&lt;lower=0&gt; sigma; } model { //priors beta ~ normal(0,5); sigma ~ cauchy(0,5); // likelihood y ~ normal(beta[1] + beta[2] * x, sigma); } A Stan model consists of different named blocks. These blocks are (from first to last): data, transformed data, parameters, trans- formed parameters, model, and generated quantities. The blocks must appear in this order. The model block is mandatory; all other blocks are optional. In the data block, the type, dimension, and name of every variable has to be declared. Optionally, the range of possible values can be specified. For example, vector[N] y; means that y is a vector (type real) of length N, and int&lt;lower=0&gt; N; means that N is an integer with nonnegative values (the bounds, here 0, are included). Note that the restriction to a possible range of values is not strictly necessary but this will help specifying the correct model and it will improve speed. We also see that each line needs to be closed by a column sign. In the parameters block, all model parameters have to be defined. The coefficients of the linear predictor constitute a vector of length 2, vector[2] beta;. Alternatively, real beta[2]; could be used. The sigma parameter is a one-number parameter that has to be positive, therefore real&lt;lower=0&gt; sigma;. The model block contains the model specification. Stan functions can handle vectors and we do not have to loop over all observations as typical for BUGS . Here, we use a Cauchy distribution as a prior distribution for sigma. This distribution can have negative values, but because we defined the lower limit of sigma to be 0 in the parameters block, the prior distribution actually used in the model is a truncated Cauchy distribution (truncated at zero). In Chapter 11.2 we explain how to choose prior distributions. Further characteristics of the Stan language that are good to know include: The variance parameter for the normal distribution is specified as the standard deviation (like in R but different from BUGS, where the precision is used). If no prior is specified, Stan uses a uniform prior over the range of possible values as specified in the parameter block. Variable names must not contain periods, for example, x.z would not be allowed, but x_z is allowed. To comment out a line, use double forward-slashes //. 12.4 Run Stan from R We fit the model to simulated data. Stan needs a vector containing the names of the data objects. In our case, x, y, and N are objects that exist in the R console. The function stan() starts Stan and returns an object containing MCMCs for every model parameter. We have to specify the name of the file that contains the model specification, the data, the number of chains, and the number of iterations per chain we would like to have. The first half of the iterations of each chain is declared as the warm-up. During the warm-up, Stan is not simulating a Markov chain, because in every step the algorithm is adapted. After the warm-up the algorithm is fixed and Stan simulates Markov chains. library(rstan) # Simulate fake data n &lt;- 50 # sample size sigma &lt;- 5 # standard deviation of the residuals b0 &lt;- 2 # intercept b1 &lt;- 0.7 # slope x &lt;- runif(n, 10, 30) # random numbers of the covariate simresid &lt;- rnorm(n, 0, sd=sigma) # residuals y &lt;- b0 + b1*x + simresid # calculate y, i.e. the data # Bundle data into a list datax &lt;- list(n=length(y), y=y, x=x) # Run STAN fit &lt;- stan(file = &quot;stanmodels/linreg.stan&quot;, data=datax, verbose = FALSE) ## In file included from C:/Users/fk/Documents/R/win-library/3.5/BH/include/boost/config.hpp:39:0, ## from C:/Users/fk/Documents/R/win-library/3.5/BH/include/boost/math/tools/config.hpp:13, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/core/var.hpp:7, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/core.hpp:12, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/mat.hpp:4, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math.hpp:4, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/src/stan/model/model_header.hpp:4, ## from filee2856303bb1.cpp:8: ## C:/Users/fk/Documents/R/win-library/3.5/BH/include/boost/config/compiler/gcc.hpp:186:0: warning: &quot;BOOST_NO_CXX11_RVALUE_REFERENCES&quot; redefined ## # define BOOST_NO_CXX11_RVALUE_REFERENCES ## ^ ## &lt;command-line&gt;:0:0: note: this is the location of the previous definition ## cc1plus.exe: warning: unrecognized command line option &quot;-Wno-ignored-attributes&quot; ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 1). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2000 [ 0%] (Warmup) ## Iteration: 200 / 2000 [ 10%] (Warmup) ## Iteration: 400 / 2000 [ 20%] (Warmup) ## Iteration: 600 / 2000 [ 30%] (Warmup) ## Iteration: 800 / 2000 [ 40%] (Warmup) ## Iteration: 1000 / 2000 [ 50%] (Warmup) ## Iteration: 1001 / 2000 [ 50%] (Sampling) ## Iteration: 1200 / 2000 [ 60%] (Sampling) ## Iteration: 1400 / 2000 [ 70%] (Sampling) ## Iteration: 1600 / 2000 [ 80%] (Sampling) ## Iteration: 1800 / 2000 [ 90%] (Sampling) ## Iteration: 2000 / 2000 [100%] (Sampling) ## ## Elapsed Time: 0.08 seconds (Warm-up) ## 0.057 seconds (Sampling) ## 0.137 seconds (Total) ## ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 2). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2000 [ 0%] (Warmup) ## Iteration: 200 / 2000 [ 10%] (Warmup) ## Iteration: 400 / 2000 [ 20%] (Warmup) ## Iteration: 600 / 2000 [ 30%] (Warmup) ## Iteration: 800 / 2000 [ 40%] (Warmup) ## Iteration: 1000 / 2000 [ 50%] (Warmup) ## Iteration: 1001 / 2000 [ 50%] (Sampling) ## Iteration: 1200 / 2000 [ 60%] (Sampling) ## Iteration: 1400 / 2000 [ 70%] (Sampling) ## Iteration: 1600 / 2000 [ 80%] (Sampling) ## Iteration: 1800 / 2000 [ 90%] (Sampling) ## Iteration: 2000 / 2000 [100%] (Sampling) ## ## Elapsed Time: 0.078 seconds (Warm-up) ## 0.059 seconds (Sampling) ## 0.137 seconds (Total) ## ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 3). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2000 [ 0%] (Warmup) ## Iteration: 200 / 2000 [ 10%] (Warmup) ## Iteration: 400 / 2000 [ 20%] (Warmup) ## Iteration: 600 / 2000 [ 30%] (Warmup) ## Iteration: 800 / 2000 [ 40%] (Warmup) ## Iteration: 1000 / 2000 [ 50%] (Warmup) ## Iteration: 1001 / 2000 [ 50%] (Sampling) ## Iteration: 1200 / 2000 [ 60%] (Sampling) ## Iteration: 1400 / 2000 [ 70%] (Sampling) ## Iteration: 1600 / 2000 [ 80%] (Sampling) ## Iteration: 1800 / 2000 [ 90%] (Sampling) ## Iteration: 2000 / 2000 [100%] (Sampling) ## ## Elapsed Time: 0.078 seconds (Warm-up) ## 0.047 seconds (Sampling) ## 0.125 seconds (Total) ## ## ## SAMPLING FOR MODEL &#39;linreg&#39; NOW (CHAIN 4). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2000 [ 0%] (Warmup) ## Iteration: 200 / 2000 [ 10%] (Warmup) ## Iteration: 400 / 2000 [ 20%] (Warmup) ## Iteration: 600 / 2000 [ 30%] (Warmup) ## Iteration: 800 / 2000 [ 40%] (Warmup) ## Iteration: 1000 / 2000 [ 50%] (Warmup) ## Iteration: 1001 / 2000 [ 50%] (Sampling) ## Iteration: 1200 / 2000 [ 60%] (Sampling) ## Iteration: 1400 / 2000 [ 70%] (Sampling) ## Iteration: 1600 / 2000 [ 80%] (Sampling) ## Iteration: 1800 / 2000 [ 90%] (Sampling) ## Iteration: 2000 / 2000 [100%] (Sampling) ## ## Elapsed Time: 0.067 seconds (Warm-up) ## 0.039 seconds (Sampling) ## 0.106 seconds (Total) Further reading Stan-Homepage: It contains the documentation for Stand a a lot of tutorials. "],
["ridge.html", "13 Ridge regression 13.1 Introduction", " 13 Ridge regression We should provide an example in Stan. 13.1 Introduction # Settings library(R2OpenBUGS) bugslocation &lt;- &quot;C:/Program Files/OpenBUGS323/OpenBugs.exe&quot; # location of OpenBUGS bugsworkingdir &lt;- file.path(getwd(), &quot;BUGS&quot;) # Bugs working directory #------------------------------------------------------------------------------- # Simulate fake data #------------------------------------------------------------------------------- library(MASS) n &lt;- 50 # sample size b0 &lt;- 1.2 b &lt;- rnorm(5, 0, 2) Sigma &lt;- matrix(c(10,3,3,2,1, 3,2,3,2,1, 3,3,5,3,2, 2,2,3,10,3, 1,1,2,3,15),5,5) Sigma x &lt;- mvrnorm(n = n, rep(0, 5), Sigma) simresid &lt;- rnorm(n, 0, sd=3) # residuals x.z &lt;- x for(i in 1:ncol(x)) x.z[,i] &lt;- (x[,i]-mean(x[,i]))/sd(x[,i]) y &lt;- b0 + x.z%*%b + simresid # calculate y, i.e. the data #------------------------------------------------------------------------------- # Function to generate initial values #------------------------------------------------------------------------------- inits &lt;- function() { list(b0=runif(1, -2, 2), b=runif(5, -2, 2), sigma=runif(1, 0.1, 2)) } #------------------------------------------------------------------------------- # Run OpenBUGS #------------------------------------------------------------------------------- parameters &lt;- c(&quot;b0&quot;, &quot;b&quot;, &quot;sigma&quot;) lambda &lt;- c(1, 2, 10, 25, 50, 100, 500, 1000, 10000) bs &lt;- matrix(ncol=length(lambda), nrow=length(b)) bse &lt;- matrix(ncol=length(lambda), nrow=length(b)) for(j in 1:length(lambda)){ datax &lt;- list(y=as.numeric(y), x=x, n=n, mb=rep(0, 5), lambda=lambda[j]) fit &lt;- bugs(datax, inits, parameters, model.file=&quot;ridge_regression.txt&quot;, n.thin=1, n.chains=2, n.burnin=5000, n.iter=10000, debug=FALSE, OpenBUGS.pgm = bugslocation, working.directory=bugsworkingdir) bs[,j] &lt;- fit$mean$b bse[,j] &lt;- fit$sd$b } range(bs) plot(1:length(lambda), seq(-2, 1, length=length(lambda)), type=&quot;n&quot;) colkey &lt;- rainbow(length(b)) for(j in 1:nrow(bs)){ lines(1:length(lambda), bs[j,], col=colkey[j], lwd=2) lines(1:length(lambda), bs[j,]-2*bse[j,], col=colkey[j], lty=3) lines(1:length(lambda), bs[j,]+2*bse[j,], col=colkey[j], lty=3) } abline(h=0) round(fit$summary,2) #------------------------------------------------------------------------------- # Run WinBUGS #------------------------------------------------------------------------------- library(R2WinBUGS) bugsdir &lt;- &quot;C:/Users/fk/WinBUGS14&quot; # mod &lt;- bugs(datax, inits= inits, parameters, model.file=&quot;normlinreg.txt&quot;, n.chains=2, n.iter=1000, n.burnin=500, n.thin=1, debug=TRUE, bugs.directory=bugsdir, program=&quot;WinBUGS&quot;, working.directory=bugsworkingdir) #------------------------------------------------------------------------------- # Test convergence and make inference #------------------------------------------------------------------------------- library(blmeco) # Make Figure 12.2 par(mfrow=c(3,1)) historyplot(fit, &quot;beta0&quot;) historyplot(fit, &quot;beta1&quot;) historyplot(fit, &quot;sigmaRes&quot;) # Parameter estimates print(fit$summary, 3) # Make predictions for covariate values between 10 and 30 newdat &lt;- data.frame(x=seq(10, 30, length=100)) Xmat &lt;- model.matrix(~x, data=newdat) predmat &lt;- matrix(ncol=fit$n.sim, nrow=nrow(newdat)) for(i in 1:fit$n.sim) predmat[,i] &lt;- Xmat%*%c(fit$sims.list$beta0[i], fit$sims.list$beta1[i]) newdat$lower.bugs &lt;- apply(predmat, 1, quantile, prob=0.025) newdat$upper.bugs &lt;- apply(predmat, 1, quantile, prob=0.975) plot(y~x, pch=16, las=1, cex.lab=1.4, cex.axis=1.2, type=&quot;n&quot;, main=&quot;&quot;) polygon(c(newdat$x, rev(newdat$x)), c(newdat$lower.bugs, rev(newdat$upper.bugs)), col=grey(0.7), border=NA) abline(c(fit$mean$beta0, fit$mean$beta1), lwd=2) box() points(x,y) "],
["sem.html", "14 Structural equation model 14.1 Introduction", " 14 Structural equation model We should provide an example in Stan. 14.1 Introduction ------------------------------------------------------------------------------------------------------ # General settings #------------------------------------------------------------------------------------------------------ library(MASS) library(rjags) library(MCMCpack) #------------------------------------------------------------------------------------------------------ # Simulation #------------------------------------------------------------------------------------------------------ n &lt;- 100 heffM &lt;- 0.6 # effect of H on M heffCS &lt;- 0.0 # effect of H on Clutch size meffCS &lt;- 0.6 # effect of M on Clutch size SigmaM &lt;- matrix(c(0.1,0.04,0.04,0.1),2,2) meffm1 &lt;- 0.6 meffm2 &lt;- 0.7 SigmaH &lt;- matrix(c(0.1,0.04,0.04,0.1),2,2) meffh1 &lt;- 0.6 meffh2 &lt;- -0.7 # Latente Variablen H &lt;- rnorm(n, 0, 1) M &lt;- rnorm(n, heffM * H, 0.1) # Clutch size CS &lt;- rnorm(n, heffCS * H + meffCS * M, 0.1) # Indicators eM &lt;- cbind(meffm1 * M, meffm2 * M) datM &lt;- matrix(NA, ncol = 2, nrow = n) eH &lt;- cbind(meffh1 * H, meffh2 * H) datH &lt;- matrix(NA, ncol = 2, nrow = n) for(i in 1:n) { datM[i,] &lt;- mvrnorm(1, eM[i,], SigmaM) datH[i,] &lt;- mvrnorm(1, eH[i,], SigmaH) } #------------------------------------------------------------------------------ # JAGS Model #------------------------------------------------------------------------------ dat &lt;- list(datM = datM, datH = datH, n = n, CS = CS, #H = H, M = M, S3 = matrix(c(1,0,0,1),nrow=2)/1) # Function to create initial values inits &lt;- function() { list( meffh = runif(2, 0, 0.1), meffm = runif(2, 0, 0.1), heffM = runif(1, 0, 0.1), heffCS = runif(1, 0, 0.1), meffCS = runif(1, 0, 0.1), tauCS = runif(1, 0.1, 0.3), tauMH = runif(1, 0.1, 0.3), tauH = rwish(3,matrix(c(.02,0,0,.04),nrow=2)), tauM = rwish(3,matrix(c(.02,0,0,.04),nrow=2)) # M = as.numeric(rep(0, n)) ) } t.n.thin &lt;- 50 t.n.chains &lt;- 2 t.n.burnin &lt;- 20000 t.n.iter &lt;- 50000 # Run JAGS jagres &lt;- jags.model(&#39;JAGS/BUGSmod1.R&#39;,data = dat, n.chains = t.n.chains, inits = inits, n.adapt = t.n.burnin) params &lt;- c(&quot;meffh&quot;, &quot;meffm&quot;, &quot;heffM&quot;, &quot;heffCS&quot;, &quot;meffCS&quot;) mod &lt;- coda.samples(jagres, params, n.iter=t.n.iter, thin=t.n.thin) res &lt;- round(data.frame(summary(mod)$quantiles[, c(3, 1, 5)]), 3) res$TRUEVALUE &lt;- c(heffCS, heffM, meffCS, meffh1, meffh2, meffm1, meffm2) res # Traceplots post &lt;- data.frame(rbind(mod[[1]], mod[[2]])) names(post) &lt;- dimnames(mod[[1]])[[2]] par(mfrow = c(3,3)) param &lt;- c(&quot;meffh[1]&quot;, &quot;meffh[2]&quot;, &quot;meffm[1]&quot;, &quot;meffm[2]&quot;, &quot;heffM&quot;, &quot;heffCS&quot;, &quot;meffCS&quot;) traceplot(mod[, match(param, names(post))]) "],
["PART-III.html", "15 Introduction to PART III 15.1 Model notations", " 15 Introduction to PART III This part is a collection of more complicated ecological models to analyse data that may not be analysed with the traditional linear models that we covered in PART I of this book. 15.1 Model notations It is unavoidable that different authors use different notations for the same thing, or that the same notation is used for different things. We try to use, whenever possible, notations that is commonly used at the International Statistical Ecology Congress ISEC. Resulting from an earlier ISEC, Betancourt et al. (2016) give guidelines on what letter should be used for which parameter in order to achieve a standard notation at least among people working with classical mark-recapture models. However, the alphabet has fewer letters compared to the number of ecological parameters. Therefore, the same letter cannot stand for the same parameter across all papers, books and chapters. Here, we try to use the same letter for the same parameter within the same chapter. "],
["zeroinflated-poisson-lmm.html", "16 Zero-inflated Poisson Mixed Model 16.1 Introduction 16.2 Example data 16.3 Model", " 16 Zero-inflated Poisson Mixed Model 16.1 Introduction Usually we describe the outcome variable with a single distribution, such as the normal distribution in the case of linear (mixed) models, and Poisson or binomial distributions in the case of generalized linear (mixed) models. In life sciences, however, quite often the data are actually generated by more than one process. In such cases the distribution of the data could be the result of two or more different distributions. If we do not account for these different processes our inferences are likely to be biased. In this chapter, we introduce a mixture model that explicitly include two processes that generated the data. The zero-inflated Poisson model is a mixture of a binomial and a Poisson distribution. We belief that two (or more)-level models are very useful tools in life sciences because they can help uncover the different processes that generate the data we observe. 16.2 Example data We used the blackstork data from the blmeco-package. They contain the breeding success of Black-stork in Latvia. The data was collected and kindly provided by Maris Stradz. The data contains the number of nestlings of more then 300 Black-stork nests in different years. Counting animals or plants is a typical example of data that contain a lot of zero counts. For example, the number of nestlings produced by a breeding pair is often zero because the whole nest was depredated or because a catastrophic event occurred such as a flood. However, when the nest succeeds, the number of nestlings varies among the successful nests depending on how many eggs the female has laid, how much food the parents could bring to the nest, or other factors that affect the survival of a nestling in an intact nest. Thus the factors that determine how many zero counts there are in the data differ from the factors that determine how many nestlings there are, if a nest survives. Count data that are produced by two different processes–one produces the zero counts and the other the variance in the count for the ones that were not zero in the first process–are called zero-inflated data. Histograms of zero-inflated data look bimodal, with one peak at zero (Figure 16.1). Figure 16.1: Histogram of the number of nestlings counted in black stork nests Ciconia nigra in Latvia (n = 1130 observations of 279 nests). 16.3 Model The Poisson distribution does not fit well to such data, because the data contain more zero counts than expected under the Poisson distribution. Mullahy (1986) and Lambert (1992) formulated two different types of models that combine the two processes in one model and therefore account for the zero excess in the data and allow the analysis of the two processes separately. The hurdle model (Mullahy, 1986) combines a left-truncated count data model (Poisson or negative binomial distribution that only describes the distribution of data larger than zero) with a zero-hurdle model that describes the distribution of the data that are either zero or nonzero. In other words, the hurdle model divides the data into two data subsets, the zero counts and the nonzero counts, and fits two separate models to each subset of the data. To account for this division of the data, the two models assume left truncation (all measurements below 1 are missing in the data) and right censoring (all measurements larger than 1 have the value 1), respectively, in their error distributions. A hurdle model can be fitted in R using the function hurdle from the package pscl (Jackman, 2008). See the tutorial by Zeileis et al. (2008) for an introduction. In contrast to the hurdle model, the zero-inflated models (Mullahy, 1986; Lambert, 1992) combine a Bernoulli model (zero vs. nonzero) with a conditional Poisson model; conditional on the Bernoulli process being nonzero. Thus this model allows for a mixture of zero counts: some zero counts are zero because the outcome of the Bernoulli process was zero (these zero counts are sometimes called structural zero values), and others are zero because their outcome from the Poisson process was zero. The function `zeroinfl from the package pscl fits zero-inflated models (Zeileis et al., 2008). The zero-inflated model may seem to reflect the true process that has generated the data closer than the hurdle model. However, sometimes the fit of zero-inflated models is impeded because of high correlation of the model parameters between the zero model and the count model. In such cases, a hurdle model may cause less troubles. Both functions (hurdle and zeroinfl) from the package pscl do not allow the inclusion of random factors. The functions MCMCglmm from the package MCMCglmm (Hadfield, 2010) and glmmadmb from the package glmmADMB (http://glmmadmb.r-forge.r-project.org/) provide the possibility to account for zero-inflation with a GLMM. However, these functions are not very flexible in the types of zero-inflated models they can fit; for example, glmmadmb only includes a constant proportion of zero values. A zero-inflation model using BUGS is described in Ke ́ry and Schaub (2012). Here we use Stan to fit a zero- inflated model. Once we understand the basic model code, it is easy to add predictors and/or random effects to both the zero and the count model. The example data contain numbers of nestlings in black stork Ciconia nigra nests in Latvia collected by Maris Stradz and collaborators at 279 nests be- tween 1979 and 2010. Black storks build solid and large aeries on branches of large trees. The same aerie is used for up to 17 years until it collapses. The black stork population in Latvia has drastically declined over the last decades. Here, we use the nestling data as presented in Figure 14-2 to describe whether the number of black stork nestlings produced in Latvia decreased over time. We use a zero-inflated Poisson model to separately estimate temporal trends for nest survival and the number of nestlings in successful nests. Since the same nests have been measured repeatedly over 1 to 17 years, we add nest ID as a random factor to both models, the Bernoulli and the Poisson model. After the first model fit, we saw that the between-nest variance in the number of nest- lings for the successful nests was close to zero. Therefore, we decide to delete the random effect from the Poisson model. Here is our final model: zit is a latent (unobserved) variable that takes the values 0 or 1 for each nest i during year t. It indicates a “structural zero”, that is, if zit 1⁄4 1 the number of nestlings yit always is zero, because the expected value in the Poisson model lit(1 zit) becomes zero. If zit 1⁄4 0, the expected value in the Poisson model becomes lit. To fit this model in Stan, we first write the Stan model code and save it in a separated text-file with name “zeroinfl.stan”. Here is a handy package: https://cran.r-project.org/web/packages/GLMMadaptive/vignettes/ZeroInflated_and_TwoPart_Models.html "],
["dailynestsurv.html", "17 Daily nest survival 17.1 Background 17.2 Models for estimating daily nest survival 17.3 The Stan model 17.4 Prepare data and run Stan 17.5 Check convergence 17.6 Look at results Further reading", " 17 Daily nest survival 17.1 Background Analyses of nest survival is important for understanding the mechanisms of population dynamics. The life-span of a nest could be used as a measure of nest survival. However, this measure very often is biased towards nests that survived longer because these nests are detected by the ornithologists with higher probability (Mayfield 1975). Therefore, instead of estimating overall nest survival it is better to estimate daily nest survival conditional on survival to the previous day. 17.2 Models for estimating daily nest survival Using a simple logistic regression with an indicator for whether a nest is alive or not at day \\(t\\) is not appropriate for estimating nest survival because its result is directly dependent on the time point when the nest has been detected. This time point is not biologically relevant. The conditional logistic regression clogit from the survival package could be used to describe correlations of the mortality with environmental variables but it does not provide an estimate for the daily survival probability. A natural model that allows estimating daily nest survival is the known-fate survival model. It is a Markov model that models the state of a nest \\(i\\) at day \\(t\\) (whether it is alive, \\(y_{it}=1\\) or not \\(y_{it}=0\\)) as a Bernoulli variable dependent on the state of the nest the day before. \\[ y_{it} \\sim Bernoulli(y_{it-1}S_{it})\\] The daily nest survival \\(S_{it}\\) can be linearly related to predictor variables that are measured on the nest or on the day level. \\[logit(S_{it}) = \\textbf{X} \\beta\\] It is also possible to add random effects if needed. 17.3 The Stan model The following Stan model code is saved as daily_nest_survival.stan. data { int&lt;lower=0&gt; Nnests; // number of nests int&lt;lower=0&gt; last[Nnests]; // day of last observation (alive or dead) int&lt;lower=0&gt; first[Nnests]; // day of first observation (alive or dead) int&lt;lower=0&gt; maxage; // maximum of last int&lt;lower=0&gt; y[Nnests, maxage]; // indicator of alive nests real cover[Nnests]; // a covariate of the nest real age[maxage]; // a covariate of the date } parameters { vector[4] b; // coef of linear pred for S } model { real S[Nnests, maxage-1]; // survival probability for(i in 1:Nnests){ for(t in first[i]:(last[i]-1)){ S[i,t] = inv_logit(b[1] + b[2]*cover[i] + b[3]*age[t] + b[4]*pow(age[t], 2)); } } // priors b[1]~normal(0,5); b[2]~normal(0,3); b[3]~normal(0,3); b[4]~normal(0,3); // likelihood for (i in 1:Nnests) { for(t in (first[i]+1):last[i]){ y[i,t]~bernoulli(y[i,t-1]*S[i,t-1]); } } } 17.4 Prepare data and run Stan Read data DESCRIPTION OF ORIGIN INCL REFERENCE MISSING load(&quot;RData/nest_surv_data.rda&quot;) str(datax) ## List of 7 ## $ y : int [1:157, 1:38] 1 NA 1 NA 1 NA NA 1 1 1 ... ## $ Nnests: int 157 ## $ last : int [1:157] 26 30 32 27 31 30 31 31 33 31 ... ## $ first : int [1:157] 1 14 1 3 1 24 18 1 1 1 ... ## $ cover : num [1:157] -0.944 -0.226 0.133 0.133 -0.226 ... ## $ age : num [1:38] -1.66 -1.57 -1.48 -1.39 -1.3 ... ## $ maxage: int 38 datax$y[is.na(datax$y)] &lt;- 0 # Stan does not allow for NA&#39;s in the outcome # Run STAN library(rstan) mod &lt;- stan(file = &quot;stanmodels/daily_nest_survival.stan&quot;, data=datax, chains=5, iter=2500, control=list(adapt_delta=0.9), verbose = FALSE) ## In file included from C:/Users/fk/Documents/R/win-library/3.5/BH/include/boost/config.hpp:39:0, ## from C:/Users/fk/Documents/R/win-library/3.5/BH/include/boost/math/tools/config.hpp:13, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/core/var.hpp:7, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/core.hpp:12, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math/rev/mat.hpp:4, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/stan/math.hpp:4, ## from C:/Users/fk/Documents/R/win-library/3.5/StanHeaders/include/src/stan/model/model_header.hpp:4, ## from filedc281745c.cpp:8: ## C:/Users/fk/Documents/R/win-library/3.5/BH/include/boost/config/compiler/gcc.hpp:186:0: warning: &quot;BOOST_NO_CXX11_RVALUE_REFERENCES&quot; redefined ## # define BOOST_NO_CXX11_RVALUE_REFERENCES ## ^ ## &lt;command-line&gt;:0:0: note: this is the location of the previous definition ## cc1plus.exe: warning: unrecognized command line option &quot;-Wno-ignored-attributes&quot; ## ## SAMPLING FOR MODEL &#39;daily_nest_survival&#39; NOW (CHAIN 1). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2500 [ 0%] (Warmup) ## Iteration: 250 / 2500 [ 10%] (Warmup) ## Iteration: 500 / 2500 [ 20%] (Warmup) ## Iteration: 750 / 2500 [ 30%] (Warmup) ## Iteration: 1000 / 2500 [ 40%] (Warmup) ## Iteration: 1250 / 2500 [ 50%] (Warmup) ## Iteration: 1251 / 2500 [ 50%] (Sampling) ## Iteration: 1500 / 2500 [ 60%] (Sampling) ## Iteration: 1750 / 2500 [ 70%] (Sampling) ## Iteration: 2000 / 2500 [ 80%] (Sampling) ## Iteration: 2250 / 2500 [ 90%] (Sampling) ## Iteration: 2500 / 2500 [100%] (Sampling) ## ## Elapsed Time: 7.441 seconds (Warm-up) ## 7.933 seconds (Sampling) ## 15.374 seconds (Total) ## ## ## SAMPLING FOR MODEL &#39;daily_nest_survival&#39; NOW (CHAIN 2). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2500 [ 0%] (Warmup) ## Iteration: 250 / 2500 [ 10%] (Warmup) ## Iteration: 500 / 2500 [ 20%] (Warmup) ## Iteration: 750 / 2500 [ 30%] (Warmup) ## Iteration: 1000 / 2500 [ 40%] (Warmup) ## Iteration: 1250 / 2500 [ 50%] (Warmup) ## Iteration: 1251 / 2500 [ 50%] (Sampling) ## Iteration: 1500 / 2500 [ 60%] (Sampling) ## Iteration: 1750 / 2500 [ 70%] (Sampling) ## Iteration: 2000 / 2500 [ 80%] (Sampling) ## Iteration: 2250 / 2500 [ 90%] (Sampling) ## Iteration: 2500 / 2500 [100%] (Sampling) ## ## Elapsed Time: 7.172 seconds (Warm-up) ## 7.558 seconds (Sampling) ## 14.73 seconds (Total) ## ## ## SAMPLING FOR MODEL &#39;daily_nest_survival&#39; NOW (CHAIN 3). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2500 [ 0%] (Warmup) ## Iteration: 250 / 2500 [ 10%] (Warmup) ## Iteration: 500 / 2500 [ 20%] (Warmup) ## Iteration: 750 / 2500 [ 30%] (Warmup) ## Iteration: 1000 / 2500 [ 40%] (Warmup) ## Iteration: 1250 / 2500 [ 50%] (Warmup) ## Iteration: 1251 / 2500 [ 50%] (Sampling) ## Iteration: 1500 / 2500 [ 60%] (Sampling) ## Iteration: 1750 / 2500 [ 70%] (Sampling) ## Iteration: 2000 / 2500 [ 80%] (Sampling) ## Iteration: 2250 / 2500 [ 90%] (Sampling) ## Iteration: 2500 / 2500 [100%] (Sampling) ## ## Elapsed Time: 6.991 seconds (Warm-up) ## 8.303 seconds (Sampling) ## 15.294 seconds (Total) ## ## ## SAMPLING FOR MODEL &#39;daily_nest_survival&#39; NOW (CHAIN 4). ## ## Gradient evaluation took 0.016 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 160 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2500 [ 0%] (Warmup) ## Iteration: 250 / 2500 [ 10%] (Warmup) ## Iteration: 500 / 2500 [ 20%] (Warmup) ## Iteration: 750 / 2500 [ 30%] (Warmup) ## Iteration: 1000 / 2500 [ 40%] (Warmup) ## Iteration: 1250 / 2500 [ 50%] (Warmup) ## Iteration: 1251 / 2500 [ 50%] (Sampling) ## Iteration: 1500 / 2500 [ 60%] (Sampling) ## Iteration: 1750 / 2500 [ 70%] (Sampling) ## Iteration: 2000 / 2500 [ 80%] (Sampling) ## Iteration: 2250 / 2500 [ 90%] (Sampling) ## Iteration: 2500 / 2500 [100%] (Sampling) ## ## Elapsed Time: 7.237 seconds (Warm-up) ## 7.497 seconds (Sampling) ## 14.734 seconds (Total) ## ## ## SAMPLING FOR MODEL &#39;daily_nest_survival&#39; NOW (CHAIN 5). ## ## Gradient evaluation took 0 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Adjust your expectations accordingly! ## ## ## Iteration: 1 / 2500 [ 0%] (Warmup) ## Iteration: 250 / 2500 [ 10%] (Warmup) ## Iteration: 500 / 2500 [ 20%] (Warmup) ## Iteration: 750 / 2500 [ 30%] (Warmup) ## Iteration: 1000 / 2500 [ 40%] (Warmup) ## Iteration: 1250 / 2500 [ 50%] (Warmup) ## Iteration: 1251 / 2500 [ 50%] (Sampling) ## Iteration: 1500 / 2500 [ 60%] (Sampling) ## Iteration: 1750 / 2500 [ 70%] (Sampling) ## Iteration: 2000 / 2500 [ 80%] (Sampling) ## Iteration: 2250 / 2500 [ 90%] (Sampling) ## Iteration: 2500 / 2500 [100%] (Sampling) ## ## Elapsed Time: 7.642 seconds (Warm-up) ## 7.129 seconds (Sampling) ## 14.771 seconds (Total) 17.5 Check convergence We love exploring the performance of the Markov chains by using the function launch_shinystan from the package shinystan. 17.6 Look at results It looks like cover does not affect daily nest survival, but daily nest survival decreases with the age of the nestlings. print(mod) ## Inference for Stan model: daily_nest_survival. ## 5 chains, each with iter=2500; warmup=1250; thin=1; ## post-warmup draws per chain=1250, total post-warmup draws=6250. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff ## b[1] 3.88 0.00 0.19 3.53 3.75 3.87 4.00 4.26 3630 ## b[2] 0.00 0.00 0.13 -0.24 -0.09 0.00 0.08 0.26 5627 ## b[3] -0.90 0.00 0.19 -1.26 -1.03 -0.90 -0.77 -0.53 4301 ## b[4] -0.18 0.00 0.24 -0.64 -0.34 -0.19 -0.03 0.29 3556 ## lp__ -306.91 0.03 1.46 -310.67 -307.61 -306.57 -305.85 -305.10 2620 ## Rhat ## b[1] 1 ## b[2] 1 ## b[3] 1 ## b[4] 1 ## lp__ 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Jan 01 13:43:07 2019. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # effect plot bsim &lt;- as.data.frame(mod) nsim &lt;- nrow(bsim) newdat &lt;- data.frame(age=seq(1, datax$maxage, length=100)) newdat$age.z &lt;- (newdat$age-mean(1:datax$maxage))/sd((1:datax$maxage)) Xmat &lt;- model.matrix(~age.z+I(age.z^2), data=newdat) fitmat &lt;- matrix(ncol=nsim, nrow=nrow(newdat)) for(i in 1:nsim) fitmat[,i] &lt;- plogis(Xmat%*%as.numeric(bsim[i,c(1,3,4)])) newdat$fit &lt;- apply(fitmat, 1, median) newdat$lwr &lt;- apply(fitmat, 1, quantile, prob=0.025) newdat$upr &lt;- apply(fitmat, 1, quantile, prob=0.975) plot(newdat$age, newdat$fit, ylim=c(0.8,1), type=&quot;l&quot;, las=1, ylab=&quot;Daily nest survival&quot;, xlab=&quot;Age [d]&quot;) lines(newdat$age, newdat$lwr, lty=3) lines(newdat$age, newdat$upr, lty=3) Figure 17.1: Estimated daily nest survival probability in relation to nest age. Dotted lines are 95% uncertainty intervals of the regression line. Further reading ADD EXAMPLE STUDIES!! "],
["cjs-with-mix.html", "18 Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals 18.1 Introduction 18.2 Data description 18.3 Model description 18.4 The Stan code 18.5 Call Stan from R, check convergence and look at results", " 18 Capture-mark recapture model with a mixture structure to account for missing sex-variable for parts of the individuals 18.1 Introduction In some species the identification of the sex is not possible for all individuals without sampling DNA. For example, morphological dimorphism is absent or so weak that parts of the individuals cannot be assigned to one of the sexes. Particularly in ornithological long-term capture recapture data sets that typically are obtained by voluntary bird ringers who do normaly not have the possibilities to analyse DNA, often the sex identification is missing in parts of the individuals. For estimating survival, it would nevertheless be valuable to include data of all individuals, use the information on sex-specific effects on survival wherever possible but account for the fact that of parts of the individuals the sex is not known. We here explain how a Cormack-Jolly-Seber model can be integrated with a mixture model in oder to allow for a combined analyses of individuals with and without sex identified. An introduction to the Cormack-Jolly-Seber model we gave in Chapter 14.5 of the book F. Korner-Nievergelt et al. (2015). We here expand this model by a mixture structure that allows including individuals with a missing categorical predictor variable, such as sex. 18.2 Data description ## simulate data # true parameter values theta &lt;- 0.6 # proportion of males nocc &lt;- 15 # number of years in the data set b0 &lt;- matrix(NA, ncol=nocc-1, nrow=2) b0[1,] &lt;- rbeta((nocc-1), 3, 4) # capture probability of males b0[2,] &lt;- rbeta((nocc-1), 2, 4) # capture probability of females a0 &lt;- matrix(NA, ncol=2, nrow=2) a1 &lt;- matrix(NA, ncol=2, nrow=2) a0[1,1]&lt;- qlogis(0.7) # average annual survival for adult males a0[1,2]&lt;- qlogis(0.3) # average annual survival for juveniles a0[2,1] &lt;- qlogis(0.55) # average annual survival for adult females a0[2,2] &lt;- a0[1,2] a1[1,1] &lt;- 0 a1[1,2] &lt;- -0.5 a1[2,1] &lt;- -0.8 a1[2,2] &lt;- a1[1,2] nindi &lt;- 1000 # number of individuals with identified sex nindni &lt;- 1500 # number of individuals with non-identified sex nind &lt;- nindi + nindni # total number of individuals y &lt;- matrix(ncol=nocc, nrow=nind) z &lt;- matrix(ncol=nocc, nrow=nind) first &lt;- sample(1:(nocc-1), nind, replace=TRUE) sex &lt;- sample(c(1,2), nind, prob=c(theta, 1-theta), replace=TRUE) juvfirst &lt;- sample(c(0,1), nind, prob=c(0.5, 0.5), replace=TRUE) juv &lt;- matrix(0, nrow=nind, ncol=nocc) for(i in 1:nind) juv[i,first[i]] &lt;- juv[i] x &lt;- runif(nocc-1, -2, 2) # a time dependent covariate covariate p &lt;- b0 # recapture probability phi &lt;- array(NA, dim=c(2, 2, nocc-1)) # for ad males phi[1,1,] &lt;- plogis(a0[1,1]+a1[1,1]*x) # for ad females phi[2,1,] &lt;- plogis(a0[2,1]+a1[2,1]*x) # for juvs phi[1,2,] &lt;- phi[2,2,] &lt;- plogis(a0[2,2]+a1[2,2]*x) for(i in 1:nind){ z[i,first[i]] &lt;- 1 y[i, first[i]] &lt;- 1 for(t in (first[i]+1):nocc){ z[i, t] &lt;- rbinom(1, size=1, prob=z[i,t-1]*phi[sex[i],juv[i,t-1]+1, t-1]) y[i, t] &lt;- rbinom(1, size=1, prob=z[i,t]*p[sex[i],t-1]) } } y[is.na(y)] &lt;- 0 The mark-recapture data set consists of capture histories of 2500.00 individuals over 15.00 time periods. For each time period \\(t\\) and individual \\(i\\) the capture history matrix \\(y\\) contains \\(y_{it}=1\\) if the individual \\(i\\) is captured during time period \\(t\\), or \\(y_{it}=0\\) if the individual \\(i\\) is not captured during time period \\(t\\). The marking time period varies between individuals from 1 to 14.00. At the marking time period, the age of the individuals was classified either as juvenile or as adult. Juveniles turn into adults after one time period, thus age is known for all individuals during all time periods after marking. For 1000.00 individuals of the 2500.00 individuals, the sex is identified, whereas for 1500.00 individuals, the sex is unknown. The example data contain one covariate \\(x\\) that takes on one value for each time period. # bundle the data for Stan i &lt;- 1:nindi ni &lt;- (nindi+1):nind datax &lt;- list(yi=y[i,], nindi=nindi, sex=sex[i], nocc=nocc, yni=y[ni,], nindni=nindni, firsti=first[i], firstni=first[ni], juvi=juv[i,]+1, juvni=juv[ni,]+1, year=1:nocc, x=x) 18.3 Model description The observations \\(y_{it}\\), an indicator of whether individual i was recaptured during time period \\(t\\) is modelled conditional on the latent true state of the individual birds \\(z_{it}\\) (0 = dead or permanently emigrated, 1 = alive and at the study site) as a Bernoulli variable. The probability \\(P(y_{it} = 1)\\) is the product of the probability that an alive individual is recaptured, \\(p_{it}\\), and the state of the bird \\(z_{it}\\) (alive = 1, dead = 0). Thus, a dead bird cannot be recaptured, whereas for a bird alive during time period \\(t\\), the recapture probability equals \\(p_{it}\\): \\[y_{it} \\sim Bernoulli(z_{it}p_{it})\\] The latent state variable \\(z_{it}\\) is a Markovian variable with the state at time \\(t\\) being dependent on the state at time \\(t-1\\) and the apparent survival probability \\[\\phi_{it}\\]: \\[z_{it} \\sim Bernoulli(z_{it-1}\\phi_{it})\\] We use the term apparent survival in order to indicate that the parameter \\(\\phi\\) is a product of site fidelity and survival. Thus, individuals that permanently emigrated from the study area cannot be distinguished from dead individuals. In both models, the parameters \\(\\phi\\) and \\(p\\) were modelled as sex-specific. However, for parts of the individuals, sex could not be identified, i.e. sex was missing. Ignoring these missing values would most likely lead to a bias because they were not missing at random. The probability that sex can be identified is increasing with age and most likely differs between sexes. Therefore, we included a mixture model for the sex: \\[Sex_i \\sim Categorical(q_i)\\] where \\(q_i\\) is a vector of length 2, containing the probability of being a male and a female, respectively. In this way, the sex of the non-identified individuals was assumed to be male or female with probability \\(q[1]\\) and \\(q[2]=1-q[1]\\), respectively. This model corresponds to the finite mixture model introduced by Pledger, Pollock, and Norris (2003) in order to account for unknown classes of birds (heterogeneity). However, in our case, for parts of the individuals the class (sex) was known. In the example model, we constrain apparent survival to be linearly dependent on a covariate x with different slopes for males, females and juveniles using the logit link function. \\[logit(\\phi_{it}) = a0_{sex-age-class[it]} + a1_{sex-age-class[it]}x_i\\] Annual recapture probability was modelled for each year and age and sex class independently: \\[p_{it} = b0_{t,sex-age-class[it]}\\] Uniform prior distributions were used for all parameters with a parameter space limited to values between 0 and 1 (probabilities) and a normal distribution with a mean of 0 and a standard deviation of 1.5 for the intercept \\(a0\\), and a standard deviation of 5 was used for \\(a1\\). 18.4 The Stan code The trick for coding the CMR-mixture model in Stan is to formulate the model 3 times: 1. For the individuals with identified sex 2. For the males that were not identified 3. For the females that were not identified Then for the non-identified individuals a mixture model is formulated that assigns a probability of being a female or a male to each individual. data { int&lt;lower=2&gt; nocc; // number of capture events int&lt;lower=0&gt; nindi; // number of individuals with identified sex int&lt;lower=0&gt; nindni; // number of individuals with non-identified sex int&lt;lower=0,upper=2&gt; yi[nindi,nocc]; // CH[i,k]: individual i captured at k int&lt;lower=0,upper=nocc-1&gt; firsti[nindi]; // year of first capture int&lt;lower=0,upper=2&gt; yni[nindni,nocc]; // CH[i,k]: individual i captured at k int&lt;lower=0,upper=nocc-1&gt; firstni[nindni]; // year of first capture int&lt;lower=1, upper=2&gt; sex[nindi]; int&lt;lower=1, upper=2&gt; juvi[nindi, nocc]; int&lt;lower=1, upper=2&gt; juvni[nindni, nocc]; int&lt;lower=1&gt; year[nocc]; real x[nocc-1]; // a covariate } transformed data { int&lt;lower=0,upper=nocc+1&gt; lasti[nindi]; // last[i]: ind i last capture int&lt;lower=0,upper=nocc+1&gt; lastni[nindni]; // last[i]: ind i last capture lasti = rep_array(0,nindi); lastni = rep_array(0,nindni); for (i in 1:nindi) { for (k in firsti[i]:nocc) { if (yi[i,k] == 1) { if (k &gt; lasti[i]) lasti[i] = k; } } } for (ii in 1:nindni) { for (kk in firstni[ii]:nocc) { if (yni[ii,kk] == 1) { if (kk &gt; lastni[ii]) lastni[ii] = kk; } } } } parameters { real&lt;lower=0, upper=1&gt; theta[nindni]; // probability of being male for non-identified individuals real&lt;lower=0, upper=1&gt; b0[2,nocc-1]; // intercept of p real a0[2,2]; // intercept for phi real a1[2,2]; // coefficient for phi } transformed parameters { real&lt;lower=0,upper=1&gt;p_male[nindni,nocc]; // capture probability real&lt;lower=0,upper=1&gt;p_female[nindni,nocc]; // capture probability real&lt;lower=0,upper=1&gt;p[nindi,nocc]; // capture probability real&lt;lower=0,upper=1&gt;phi_male[nindni,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi_male[nindni,nocc+1]; // probability that an individual // is never recaptured after its // last capture real&lt;lower=0,upper=1&gt;phi_female[nindni,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi_female[nindni,nocc+1]; // probability that an individual // is never recaptured after its // last capture real&lt;lower=0,upper=1&gt;phi[nindi,nocc-1]; // survival probability real&lt;lower=0,upper=1&gt;chi[nindi,nocc+1]; // probability that an individual // is never recaptured after its // last capture { int k; int kk; for(ii in 1:nindi){ if (firsti[ii]&gt;1) { for (z in 1:(firsti[ii]-1)){ phi[ii,z] = 1; } } for(tt in firsti[ii]:(nocc-1)) { // linear predictor for phi: phi[ii,tt] = inv_logit(a0[sex[ii], juvi[ii,tt]] + a1[sex[ii], juvi[ii,tt]]*x[tt]); } } for(ii in 1:nindni){ if (firstni[ii]&gt;1) { for (z in 1:(firstni[ii]-1)){ phi_female[ii,z] = 1; phi_male[ii,z] = 1; } } for(tt in firstni[ii]:(nocc-1)) { // linear predictor for phi: phi_male[ii,tt] = inv_logit(a0[1, juvni[ii,tt]] + a1[1, juvni[ii,tt]]*x[tt]); phi_female[ii,tt] = inv_logit(a0[2, juvni[ii,tt]]+ a1[2, juvni[ii,tt]]*x[tt]); } } for(i in 1:nindi) { // linear predictor for p for identified individuals for(w in 1:firsti[i]){ p[i,w] = 1; } for(kkk in (firsti[i]+1):nocc) p[i,kkk] = b0[sex[i],year[kkk-1]]; chi[i,nocc+1] = 1.0; k = nocc; while (k &gt; firsti[i]) { chi[i,k] = (1 - phi[i,k-1]) + phi[i,k-1] * (1 - p[i,k]) * chi[i,k+1]; k = k - 1; } if (firsti[i]&gt;1) { for (u in 1:(firsti[i]-1)){ chi[i,u] = 0; } } chi[i,firsti[i]] = (1 - p[i,firsti[i]]) * chi[i,firsti[i]+1]; }// close definition of transformed parameters for identified individuals for(i in 1:nindni) { // linear predictor for p for non-identified individuals for(w in 1:firstni[i]){ p_male[i,w] = 1; p_female[i,w] = 1; } for(kkkk in (firstni[i]+1):nocc){ p_male[i,kkkk] = b0[1,year[kkkk-1]]; p_female[i,kkkk] = b0[2,year[kkkk-1]]; } chi_male[i,nocc+1] = 1.0; chi_female[i,nocc+1] = 1.0; k = nocc; while (k &gt; firstni[i]) { chi_male[i,k] = (1 - phi_male[i,k-1]) + phi_male[i,k-1] * (1 - p_male[i,k]) * chi_male[i,k+1]; chi_female[i,k] = (1 - phi_female[i,k-1]) + phi_female[i,k-1] * (1 - p_female[i,k]) * chi_female[i,k+1]; k = k - 1; } if (firstni[i]&gt;1) { for (u in 1:(firstni[i]-1)){ chi_male[i,u] = 0; chi_female[i,u] = 0; } } chi_male[i,firstni[i]] = (1 - p_male[i,firstni[i]]) * chi_male[i,firstni[i]+1]; chi_female[i,firstni[i]] = (1 - p_female[i,firstni[i]]) * chi_female[i,firstni[i]+1]; } // close definition of transformed parameters for non-identified individuals } // close block of transformed parameters exclusive parameter declarations } // close transformed parameters model { // priors theta ~ beta(1, 1); for (g in 1:(nocc-1)){ b0[1,g]~beta(1,1); b0[2,g]~beta(1,1); } a0[1,1]~normal(0,1.5); a0[1,2]~normal(0,1.5); a1[1,1]~normal(0,3); a1[1,2]~normal(0,3); a0[2,1]~normal(0,1.5); a0[2,2]~normal(a0[1,2],0.01); // for juveniles, we assume that the effect of the covariate is independet of sex a1[2,1]~normal(0,3); a1[2,2]~normal(a1[1,2],0.01); // likelihood for identified individuals for (i in 1:nindi) { if (lasti[i]&gt;0) { for (k in firsti[i]:lasti[i]) { if(k&gt;1) target+= (log(phi[i, k-1])); if (yi[i,k] == 1) target+=(log(p[i,k])); else target+=(log1m(p[i,k])); } } target+=(log(chi[i,lasti[i]+1])); } // likelihood for non-identified individuals for (i in 1:nindni) { real log_like_male = 0; real log_like_female = 0; if (lastni[i]&gt;0) { for (k in firstni[i]:lastni[i]) { if(k&gt;1){ log_like_male += (log(phi_male[i, k-1])); log_like_female += (log(phi_female[i, k-1])); } if (yni[i,k] == 1){ log_like_male+=(log(p_male[i,k])); log_like_female+=(log(p_female[i,k])); } else{ log_like_male+=(log1m(p_male[i,k])); log_like_female+=(log1m(p_female[i,k])); } } } log_like_male += (log(chi_male[i,lastni[i]+1])); log_like_female += (log(chi_female[i,lastni[i]+1])); target += log_mix(theta[i], log_like_male, log_like_female); } } 18.5 Call Stan from R, check convergence and look at results # Run STAN library(rstan) fit &lt;- stan(file = &quot;stanmodels/cmr_mixture_model.stan&quot;, data=datax, verbose = FALSE) # for above simulated data (25000 individuals x 15 time periods) # computing time is around 48 hours on an intel corei7 laptop # for larger data sets, we recommed moving the transformed parameters block # to the model block in order to avoid monitoring of p_male, p_female, # phi_male and phi_female producing memory problems # launch_shinystan(fit) # diagnostic plots summary(fit) ## mean se_mean sd 2.5% 25% ## b0[1,1] 0.60132367 0.0015709423 0.06173884 0.48042366 0.55922253 ## b0[1,2] 0.70098709 0.0012519948 0.04969428 0.60382019 0.66806698 ## b0[1,3] 0.50293513 0.0010904085 0.04517398 0.41491848 0.47220346 ## b0[1,4] 0.28118209 0.0008809447 0.03577334 0.21440931 0.25697691 ## b0[1,5] 0.34938289 0.0009901335 0.03647815 0.27819918 0.32351323 ## b0[1,6] 0.13158569 0.0006914740 0.02627423 0.08664129 0.11286629 ## b0[1,7] 0.61182981 0.0010463611 0.04129602 0.53187976 0.58387839 ## b0[1,8] 0.48535193 0.0010845951 0.04155762 0.40559440 0.45750793 ## b0[1,9] 0.52531291 0.0008790063 0.03704084 0.45247132 0.50064513 ## b0[1,10] 0.87174780 0.0007565552 0.03000936 0.80818138 0.85259573 ## b0[1,11] 0.80185454 0.0009425675 0.03518166 0.73173810 0.77865187 ## b0[1,12] 0.33152443 0.0008564381 0.03628505 0.26380840 0.30697293 ## b0[1,13] 0.42132288 0.0012174784 0.04140382 0.34062688 0.39305210 ## b0[1,14] 0.65180372 0.0015151039 0.05333953 0.55349105 0.61560493 ## b0[2,1] 0.34237039 0.0041467200 0.12925217 0.12002285 0.24717176 ## b0[2,2] 0.18534646 0.0023431250 0.07547704 0.05924694 0.12871584 ## b0[2,3] 0.61351083 0.0024140550 0.07679100 0.46647727 0.56242546 ## b0[2,4] 0.37140208 0.0024464965 0.06962399 0.24693888 0.32338093 ## b0[2,5] 0.19428215 0.0034618302 0.11214798 0.02800056 0.11146326 ## b0[2,6] 0.27371336 0.0026553769 0.09054020 0.11827243 0.20785316 ## b0[2,7] 0.18611173 0.0014387436 0.05328492 0.09122869 0.14789827 ## b0[2,8] 0.25648337 0.0018258589 0.05287800 0.16255769 0.21913271 ## b0[2,9] 0.20378754 0.0021367769 0.07380004 0.07777998 0.15215845 ## b0[2,10] 0.52679548 0.0024625568 0.08696008 0.36214334 0.46594844 ## b0[2,11] 0.47393354 0.0032593161 0.10555065 0.28843967 0.39781278 ## b0[2,12] 0.22289155 0.0017082729 0.05551514 0.12576797 0.18203335 ## b0[2,13] 0.26191486 0.0024159794 0.07016314 0.14106495 0.21234017 ## b0[2,14] 0.65111737 0.0055743944 0.18780555 0.29279480 0.50957591 ## a0[1,1] 0.95440670 0.0013771881 0.04808748 0.86301660 0.92146330 ## a0[1,2] 0.01529770 0.0469699511 1.46995922 -2.82218067 -0.95533706 ## a0[2,1] 0.16384995 0.0049928331 0.12634422 -0.06399631 0.07533962 ## a0[2,2] 0.01535679 0.0469634175 1.47006964 -2.81864060 -0.95515751 ## a1[1,1] 0.15937249 0.0028992587 0.08864790 -0.01288607 0.10017613 ## a1[1,2] 0.08055953 0.1007089857 3.02148727 -5.95525636 -1.96662599 ## a1[2,1] -0.83614134 0.0074143920 0.18655882 -1.21033848 -0.95698565 ## a1[2,2] 0.08071668 0.1006904255 3.02145647 -5.94617355 -1.96508733 ## 50% 75% 97.5% n_eff Rhat ## b0[1,1] 0.60206306 0.6431566 0.7206343 1544.5301 1.002331 ## b0[1,2] 0.70165494 0.7355204 0.7946280 1575.4617 1.001482 ## b0[1,3] 0.50367411 0.5330078 0.5898079 1716.3196 1.001183 ## b0[1,4] 0.27997512 0.3046483 0.3544592 1649.0040 1.000760 ## b0[1,5] 0.34936442 0.3751935 0.4191138 1357.3073 1.002072 ## b0[1,6] 0.12987449 0.1481661 0.1873982 1443.8040 1.003676 ## b0[1,7] 0.61203228 0.6397577 0.6933929 1557.5904 1.001458 ## b0[1,8] 0.48513822 0.5134314 0.5672066 1468.1355 1.002511 ## b0[1,9] 0.52534212 0.5501747 0.5994060 1775.7335 1.000824 ## b0[1,10] 0.87324112 0.8934047 0.9258033 1573.3747 1.000719 ## b0[1,11] 0.80300311 0.8261868 0.8675033 1393.1817 1.001172 ## b0[1,12] 0.33044476 0.3552199 0.4052902 1794.9956 1.000566 ## b0[1,13] 0.42116690 0.4492297 0.5026942 1156.5339 1.000289 ## b0[1,14] 0.64956850 0.6864706 0.7607107 1239.4056 1.004061 ## b0[2,1] 0.33493631 0.4251416 0.6150923 971.5524 1.004049 ## b0[2,2] 0.17981663 0.2358847 0.3446097 1037.6210 1.001474 ## b0[2,3] 0.61326419 0.6644156 0.7628427 1011.8737 1.005727 ## b0[2,4] 0.36837778 0.4158585 0.5190457 809.8949 1.003803 ## b0[2,5] 0.17910449 0.2591418 0.4533117 1049.4733 1.001499 ## b0[2,6] 0.26739172 0.3299594 0.4685139 1162.6006 1.001170 ## b0[2,7] 0.18254607 0.2198969 0.3003156 1371.6455 1.000878 ## b0[2,8] 0.25280556 0.2895585 0.3704113 838.7174 1.005624 ## b0[2,9] 0.19724053 0.2501298 0.3694806 1192.8747 1.003687 ## b0[2,10] 0.52587075 0.5845730 0.7061694 1247.0027 1.002851 ## b0[2,11] 0.46874445 0.5392302 0.7046892 1048.7425 0.999473 ## b0[2,12] 0.21961656 0.2580782 0.3397127 1056.1081 1.000907 ## b0[2,13] 0.25601959 0.3056204 0.4142888 843.3960 1.003130 ## b0[2,14] 0.65824835 0.7973674 0.9698829 1135.0669 1.003838 ## a0[1,1] 0.95368445 0.9862439 1.0515747 1219.2071 1.003898 ## a0[1,2] 0.01633534 0.9911055 2.9717839 979.4231 1.003726 ## a0[2,1] 0.15519648 0.2472483 0.4230776 640.3489 1.004625 ## a0[2,2] 0.01587281 0.9898084 2.9659552 979.8429 1.003744 ## a1[1,1] 0.15647489 0.2205720 0.3354845 934.8953 1.007190 ## a1[1,2] 0.06683287 2.1568781 6.0295208 900.1297 1.003701 ## a1[2,1] -0.83503982 -0.7075691 -0.4814539 633.1119 1.010568 ## a1[2,2] 0.06586905 2.1557247 6.0239735 900.4432 1.003704 "],
["referenzen.html", "Referenzen", " Referenzen "]
]
