<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and Stan</title>
  <meta name="description" content="Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN introduces Bayesian software, using R for the simple modes, and flexible Bayesian software (BUGS and Stan) for the more complicated ones. Guiding the ready from easy toward more complex (real) data analyses ina step-by-step manner, the book presents problems and solutions—including all R codes—that are most often applicable to other data and questions, making it an invaluable resource for analyzing a variety of data types.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.jpg" />
  <meta property="og:description" content="Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN introduces Bayesian software, using R for the simple modes, and flexible Bayesian software (BUGS and Stan) for the more complicated ones. Guiding the ready from easy toward more complex (real) data analyses ina step-by-step manner, the book presents problems and solutions—including all R codes—that are most often applicable to other data and questions, making it an invaluable resource for analyzing a variety of data types." />
  <meta name="github-repo" content="TobiasRoth/BDAEcology" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and Stan" />
  
  <meta name="twitter:description" content="Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and STAN introduces Bayesian software, using R for the simple modes, and flexible Bayesian software (BUGS and Stan) for the more complicated ones. Guiding the ready from easy toward more complex (real) data analyses ina step-by-step manner, the book presents problems and solutions—including all R codes—that are most often applicable to other data and questions, making it an invaluable resource for analyzing a variety of data types." />
  <meta name="twitter:image" content="images/cover.jpg" />

<meta name="author" content="Fränzi Korner-Nievergelt, Tobias Roth, Stefanie von Felten, Jérôme Guéla, Bettina Almasi and Pius Korner-Nievergelt">


<meta name="date" content="2018-04-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="prerequisites.html">
<link rel="next" href="lm.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="Settings\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>1</b> Why do we Need Statistical Models and What is this Book About?</a><ul>
<li class="chapter" data-level="1.1" data-path="about.html"><a href="about.html#why-we-need-statistical-models"><i class="fa fa-check"></i><b>1.1</b> WHY WE NEED STATISTICAL MODELS</a></li>
<li class="chapter" data-level="1.2" data-path="about.html"><a href="about.html#what-this-book-is-about"><i class="fa fa-check"></i><b>1.2</b> WHAT THIS BOOK IS ABOUT</a></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html#further-reading"><i class="fa fa-check"></i>FURTHER READING</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites and Vocabulary</a><ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#software"><i class="fa fa-check"></i><b>2.1</b> SOFTWARE</a><ul>
<li class="chapter" data-level="2.1.1" data-path="prerequisites.html"><a href="prerequisites.html#what-is-r"><i class="fa fa-check"></i><b>2.1.1</b> What Is R?</a></li>
<li class="chapter" data-level="2.1.2" data-path="prerequisites.html"><a href="prerequisites.html#working-with-r"><i class="fa fa-check"></i><b>2.1.2</b> Working with R</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#important-statistical-terms-and-how-to-handle-them-in-r"><i class="fa fa-check"></i><b>2.2</b> IMPORTANT STATISTICAL TERMS AND HOW TO HANDLE THEM IN R</a><ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#data-sets-variables-and-observations"><i class="fa fa-check"></i><b>2.2.1</b> Data Sets, Variables, and Observations</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#distributions-and-summary-statistics"><i class="fa fa-check"></i><b>2.2.2</b> Distributions and Summary Statistics</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#more-on-r-objects"><i class="fa fa-check"></i><b>2.2.3</b> More on R Objects</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#r-functions-for-graphics"><i class="fa fa-check"></i><b>2.2.4</b> R Functions for Graphics</a></li>
<li class="chapter" data-level="2.2.5" data-path="prerequisites.html"><a href="prerequisites.html#writing-our-own-r-functions"><i class="fa fa-check"></i><b>2.2.5</b> Writing Our Own R Functions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html#further-reading-1"><i class="fa fa-check"></i>FURTHER READING</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>3</b> The Bayesian and the Frequentist Ways of Analyzing Data</a><ul>
<li class="chapter" data-level="3.1" data-path="bayes.html"><a href="bayes.html#short-historical-overview"><i class="fa fa-check"></i><b>3.1</b> SHORT HISTORICAL OVERVIEW</a></li>
<li class="chapter" data-level="3.2" data-path="bayes.html"><a href="bayes.html#the-bayesian-way"><i class="fa fa-check"></i><b>3.2</b> THE BAYESIAN WAY</a><ul>
<li class="chapter" data-level="3.2.1" data-path="bayes.html"><a href="bayes.html#estimating-the-mean-of-a-normal-distribution-with-a-known-variance"><i class="fa fa-check"></i><b>3.2.1</b> Estimating the Mean of a Normal Distribution with a Known Variance</a></li>
<li class="chapter" data-level="3.2.2" data-path="bayes.html"><a href="bayes.html#estimating-mean-and-variance-of-a-normal-distribution-using-simulation"><i class="fa fa-check"></i><b>3.2.2</b> Estimating Mean and Variance of a Normal Distribution Using Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bayes.html"><a href="bayes.html#frequentist"><i class="fa fa-check"></i><b>3.3</b> THE FREQUENTIST WAY</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>4</b> Normal Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="lm.html"><a href="lm.html#linear-regression"><i class="fa fa-check"></i><b>4.1</b> LINEAR REGRESSION</a><ul>
<li class="chapter" data-level="4.1.1" data-path="lm.html"><a href="lm.html#background"><i class="fa fa-check"></i><b>4.1.1</b> Background</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="likelihood.html"><a href="likelihood.html"><i class="fa fa-check"></i><b>5</b> Likelihood</a><ul>
<li class="chapter" data-level="5.1" data-path="likelihood.html"><a href="likelihood.html#theory"><i class="fa fa-check"></i><b>5.1</b> THEORY</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelchecking.html"><a href="modelchecking.html"><i class="fa fa-check"></i><b>6</b> Assessing Model Assumptions</a><ul>
<li class="chapter" data-level="6.1" data-path="modelchecking.html"><a href="modelchecking.html#model-assumptions"><i class="fa fa-check"></i><b>6.1</b> MODEL ASSUMPTIONS</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lmm.html"><a href="lmm.html"><i class="fa fa-check"></i><b>7</b> Linear Mixed Effects Models</a><ul>
<li class="chapter" data-level="7.1" data-path="lmm.html"><a href="lmm.html#background-1"><i class="fa fa-check"></i><b>7.1</b> BACKGROUND</a><ul>
<li class="chapter" data-level="7.1.1" data-path="lmm.html"><a href="lmm.html#why-mixed-effects-models"><i class="fa fa-check"></i><b>7.1.1</b> Why Mixed Effects Models?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>8</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="8.1" data-path="glm.html"><a href="glm.html#background-2"><i class="fa fa-check"></i><b>8.1</b> BACKGROUND</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="glmm.html"><a href="glmm.html"><i class="fa fa-check"></i><b>9</b> Generalized Linear Mixed Models</a><ul>
<li class="chapter" data-level="9.1" data-path="glmm.html"><a href="glmm.html#binomial-mixed-model"><i class="fa fa-check"></i><b>9.1</b> BINOMIAL MIXED MODEL</a><ul>
<li class="chapter" data-level="9.1.1" data-path="glmm.html"><a href="glmm.html#background-3"><i class="fa fa-check"></i><b>9.1.1</b> Background</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="predictivemodcheck.html"><a href="predictivemodcheck.html"><i class="fa fa-check"></i><b>10</b> Posterior Predictive Model Checking and Proportion of Explained Variance</a><ul>
<li class="chapter" data-level="10.1" data-path="predictivemodcheck.html"><a href="predictivemodcheck.html#posterior-predictive-model-checking"><i class="fa fa-check"></i><b>10.1</b> POSTERIOR PREDICTIVE MODEL CHECKING</a><ul>
<li class="chapter" data-level="10.1.1" data-path="predictivemodcheck.html"><a href="predictivemodcheck.html#why-mixed-effects-models-1"><i class="fa fa-check"></i><b>10.1.1</b> Why Mixed Effects Models?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="moddelection.html"><a href="moddelection.html"><i class="fa fa-check"></i><b>11</b> Model Selection and Multimodel Inference</a><ul>
<li class="chapter" data-level="11.1" data-path="moddelection.html"><a href="moddelection.html#when-and-why-we-select-models-and-why-this-is-difficult"><i class="fa fa-check"></i><b>11.1</b> WHEN AND WHY WE SELECT MODELS AND WHY THIS IS DIFFICULT</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>12</b> Markov Chain Monte Carlo Simulation</a><ul>
<li class="chapter" data-level="12.1" data-path="mcmc.html"><a href="mcmc.html#background-4"><i class="fa fa-check"></i><b>12.1</b> BACKGROUND</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="spatial.html"><a href="spatial.html"><i class="fa fa-check"></i><b>13</b> Modeling Spatial Data Using GLMM</a><ul>
<li class="chapter" data-level="13.1" data-path="spatial.html"><a href="spatial.html#background-5"><i class="fa fa-check"></i><b>13.1</b> BACKGROUND</a><ul>
<li class="chapter" data-level="13.1.1" data-path="spatial.html"><a href="spatial.html#why-mixed-effects-models-2"><i class="fa fa-check"></i><b>13.1.1</b> Why Mixed Effects Models?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="advancedmodels.html"><a href="advancedmodels.html"><i class="fa fa-check"></i><b>14</b> Advanced Ecological Models</a><ul>
<li class="chapter" data-level="14.1" data-path="advancedmodels.html"><a href="advancedmodels.html#hierarchical-multinomial-model-to-analyze-habitat-selection-using-bugs"><i class="fa fa-check"></i><b>14.1</b> HIERARCHICAL MULTINOMIAL MODEL TO ANALYZE HABITAT SELECTION USING BUGS</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="estimability.html"><a href="estimability.html"><i class="fa fa-check"></i><b>15</b> Prior Influence and Parameter Estimability</a><ul>
<li class="chapter" data-level="15.1" data-path="estimability.html"><a href="estimability.html#how-to-specify-prior-distributions"><i class="fa fa-check"></i><b>15.1</b> HOW TO SPECIFY PRIOR DISTRIBUTIONS</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="checklist.html"><a href="checklist.html"><i class="fa fa-check"></i><b>16</b> Checklist</a><ul>
<li class="chapter" data-level="16.1" data-path="checklist.html"><a href="checklist.html#data-analysis-step-by-step"><i class="fa fa-check"></i><b>16.1</b> DATA ANALYSIS STEP BY STEP</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="reportforpaper.html"><a href="reportforpaper.html"><i class="fa fa-check"></i><b>17</b> What Should I Report in a Paper</a><ul>
<li class="chapter" data-level="17.1" data-path="reportforpaper.html"><a href="reportforpaper.html#how-to-present-the-results"><i class="fa fa-check"></i><b>17.1</b> HOW TO PRESENT THE RESULTS</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referenzen.html"><a href="referenzen.html"><i class="fa fa-check"></i>Referenzen</a></li>
<li class="divider"></li>
</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayes" class="section level1">
<h1><span class="header-section-number">3</span> The Bayesian and the Frequentist Ways of Analyzing Data</h1>
<div id="short-historical-overview" class="section level2">
<h2><span class="header-section-number">3.1</span> SHORT HISTORICAL OVERVIEW</h2>
<p>Reverend Thomas Bayes (1701 or 1702e1761) developed the Bayes theorem. Based on this theorem, he described how to obtain the probability of a hy- pothesis given an observation, that is, data. However, he was so worried whether it would be acceptable to apply his theory to real-world examples that he did not dare to publish it. His methods were only published posthumously (Bayes, 1763). Without the help of computers, Bayes’ methods were appli- cable to just simple problems.</p>
<p>Much later, the concept of null hypothesis testing was introduced by Ronald A. Fisher (1890e1962) in his book Statistical Methods for Research Workers (Fisher, 1925) and many other publications. Egon Pearson (1895e1980) and others developed the frequentist statistical methods, which are based on the probability of the data given a null hypothesis. These methods are solvable for many simple and some moderately complex examples.</p>
<p>The rapidly improving capacity of computers in recent years now enables us to use Bayesian methods also for more (and even very) complex problems using simulation techniques (Smith et al., 1985; Gelfand &amp; Smith, 1990; Gilks et al., 1996).</p>
</div>
<div id="the-bayesian-way" class="section level2">
<h2><span class="header-section-number">3.2</span> THE BAYESIAN WAY</h2>
<p>Bayesian methods use Bayes’ theorem to update prior knowledge about a parameter with information coming from the data to obtain posterior knowl- edge. The prior and posterior knowledge are mathematically described by a probability distribution (prior and posterior distributions of the parameter).</p>
<p>Bayes’ theorem for discrete events says that the probability of event A given event B has occurred, P(A|B), equals the probability of event A, P(A), times the probability of event B conditional on A, P(B|A), divided by the probability of event B, P(B):</p>
<span class="math display" id="eq:bayesiantheorem">\[\begin{align} 
P(A|B) = \frac{P(A)P(B|A)}{P(B)} \tag{3.1}
\end{align}\]</span>
<p>When using Bayes’ theorem for drawing inference from data, we are interested in the probability distribution of one or several parameters, <span class="math inline">\(\theta\)</span> (called “theta”), after having looked at the data y, that is, the posterior distribution, <span class="math inline">\(p(\theta|y)\)</span>. To this end, Bayes’ theorem <a href="bayes.html#eq:bayesiantheorem">(3.1)</a> is reformulated for continuous parameters using probability distributions rather than probabilities for discrete events:</p>
<span class="math display" id="eq:bayesiantheoremc">\[\begin{align} 
p(\theta|y) = \frac{p(\theta)p(y|\theta)}{p(y)} \tag{3.2}
\end{align}\]</span>
<p>The <em>posterior distribution</em>, <span class="math inline">\(p(\theta|y)\)</span>, describes what we know about the parameter (or about the set of parameters), <span class="math inline">\(\theta\)</span>, after having looked at the data and given the prior knowledge and the model. The <em>prior distribution</em> of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(p(\theta)\)</span>, describes what we know about <span class="math inline">\(\theta\)</span> before having looked at the data. This is often very little but it can include information from earlier studies. The probability of the data conditional on <span class="math inline">\(\theta\)</span>, <span class="math inline">\(p(y|\theta)\)</span>, is called likelihood.</p>
<p>The word <em>likelihood</em> is used in Bayesian statistics with a slightly different meaning than it is used in frequentist statistics. The frequentist likelihood, <span class="math inline">\(L(\theta|y)\)</span>, is a relative measure for the probability of the observed data given specific parameter values. The likelihood is a number often close to zero (see also Chapter <a href="likelihood.html#likelihood">5</a>). In contrast, Bayesians use likelihood for the density distribution of the data conditional on the parameters of a model. Thus, in Bayesian statistics the likelihood is a distribution (i.e., the area under the curve is 1) whereas in frequentist statistics it is a scalar. The <em>prior probability</em> of the data, <span class="math inline">\(p(y)\)</span>, equals the integral of <span class="math inline">\(p(y|\theta)p(\theta)\)</span> over all possible values of <span class="math inline">\(\theta\)</span>; thus <span class="math inline">\(p(y)\)</span> is a constant.</p>
<p>The integral can be solved numerically only for a few simple cases. For this reason, Bayesian statistics were not widely applied before the computer age. Nowadays, a variety of different simulation algorithms exist that allow sam- pling from distributions that are only known to proportionality (Gilks et al., 1996; Bre ́maud, 1999). Dropping the term p(y) in Equation<a href="bayes.html#eq:bayesiantheoremc">(3.2)</a> leads to a term that is proportional to the posterior distribution: <span class="math inline">\(p(\theta|y) \propto p(\theta)p(y|\theta)\)</span>. Simulation algorithms such as Markov chain Monte Carlo simulation (MCMC) can, therefore, sample from the posterior distribution without having to know <span class="math inline">\(p(y)\)</span>. A large enough sample of the posterior distribution can then be used to draw inference about the parameters of interest.</p>
<div id="estimating-the-mean-of-a-normal-distribution-with-a-known-variance" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Estimating the Mean of a Normal Distribution with a Known Variance</h3>
<p>The purpose of this section is to illustrate the Bayesian method using a theoretical example. It has only limited practical value. Therefore, feel free to skip this chapter if you are afraid of mathematical meditations. One of the simplest examples is to estimate the mean of a normal distribution with known variance based on a sample of n measurements.</p>
<p>The model of the data is <span class="math inline">\(y \sim Norm(\theta, \sigma^2)\)</span>, which means “y is normally distributed with mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(\sigma^2\)</span>”. <span class="math inline">\(\Theta\)</span> is the true (population) mean and <span class="math inline">\(\sigma^2\)</span> the variance of the population from which the data are a random sample. Given the data contain three measurements, y1 = 27.1, y2 = 14.6, y3 = 14.6, and we know that the variance, <span class="math inline">\(\sigma^2\)</span>, is 20, what do we know about the mean <span class="math inline">\(\theta\)</span> of the population from which the data are a random sample? Based on Bayes’ theorem and the normal prior distribution it is possible to see that the posterior distribution of the mean <span class="math inline">\(\theta\)</span> is itself a normal distribution with mean <span class="math inline">\(\mu_n\)</span> and variance <span class="math inline">\(\sigma_n\)</span>, <span class="math inline">\(p(\theta|y) \sim Norm(\mu_n, \sigma_n)\)</span>, where</p>
<span class="math display">\[\begin{align} 
\mu_n = \frac{\frac{\mu_o}{\sigma^2_0}+\frac{n\overline{y}}{\sigma^2}}{\frac{1}{\sigma^2_0}+\frac{n}{\sigma^2}}\text{, and } \frac{1}{\sigma2_n}=\frac{1}{\sigma^2_0}+\frac{n}{\sigma^2}.
\end{align}\]</span>
<p><span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\sigma^2_0\)</span> are the mean and the variance of the prior distribution for <span class="math inline">\(\theta\)</span>. Gelman et al. (2014) provide the derivation of these formulas. When we know in advance that m cannot be very far from 0, but we have very little knowledge about <span class="math inline">\(\theta\)</span>, we might assume a flat normal distribution around 0 – for example, <span class="math inline">\(Norm(0, \sigma^2_0 = 200)\)</span> – as the prior distribution for <span class="math inline">\(\theta\)</span> (the dotted line in Figure <a href="bayes.html#fig:triplot">3.1</a>; only part of the right tail is visible). This results in a posterior distribution $p(q |y) Norm(18.2, ^2_0 = 6.5) (the solid line in Figure <a href="bayes.html#fig:triplot">3.1</a>). This posterior distribution expresses what we know about q based on our prior knowledge, the data, and assuming our model adequately describes the process that generated the data (the data and the model are formalized in the likelihood, which, in this example, is a normal distribution with mean equal to the arithmetic mean of the data and variance equal to 20, the dashed line in Figure <a href="bayes.html#fig:triplot">3.1</a>).</p>
<div class="figure"><span id="fig:triplot"></span>
<img src="03-bayes_files/figure-html/triplot-1.png" alt="Prior distribution, likelihood, and posterior distribution of the mean q. The plot has been drawn using the R function `triplot.normal.knownvariance` provided in the R package blmeco." width="480" />
<p class="caption">
Figure 3.1: Prior distribution, likelihood, and posterior distribution of the mean q. The plot has been drawn using the R function <code>triplot.normal.knownvariance</code> provided in the R package blmeco.
</p>
</div>
<p>The posterior distribution is a combination of the information of the data and the prior; the data and the prior are weighted according to the information they contain. This information content is measured by the precision (which is equal to the inverse of the variance). If we use a completely flat (i.e., non- informative) prior distribution, the posterior distribution equals the likelihood (upper left panel in Figure <a href="bayes.html#fig:triplottheta">3.2</a>). In this case the inference drawn does not differ from the inference drawn with frequentist methods. The more we know a priori about q, the stronger the influence the prior has on the posterior (Figure <a href="bayes.html#fig:triplottheta">3.2</a>). When informative prior distributions are used, the inference drawn with Bayesian methods differ from the ones drawn with frequentist methods (instead of saying “prior distribution” and “posterior distribution” one often only says “prior” and “posterior”).</p>
<p>Note that the likelihood is the same in all panels of Figure <a href="bayes.html#fig:triplottheta">3.2</a> because the same data set and the same model is used.</p>
<div class="figure"><span id="fig:triplottheta"></span>
<img src="03-bayes_files/figure-html/triplottheta-1.png" alt="Prior, likelihood, and posterior distribution of the mean $\theta$ using different prior distributions for $\theta$." width="768" />
<p class="caption">
Figure 3.2: Prior, likelihood, and posterior distribution of the mean <span class="math inline">\(\theta\)</span> using different prior distributions for <span class="math inline">\(\theta\)</span>.
</p>
</div>
</div>
<div id="estimating-mean-and-variance-of-a-normal-distribution-using-simulation" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Estimating Mean and Variance of a Normal Distribution Using Simulation</h3>
<p>The estimation of a mean $and a variance <span class="math inline">\(\sigma^2\)</span> of a normal distribution, where both parameters are unknown, is more complicated than estimating the mean only, because the posterior distribution is two-dimensional. Such a posterior distribution is called a <em>joint posterior distribution</em>. Nevertheless, it is still possible to obtain the joint posterior distribution analytically (Albert, 2007; Gelman et al., 2014). Rather than following the analytical route, we now demonstrate how we can simulate a posterior distribution using the <code>sim</code> function in the package arm.</p>
<p>First, we obtain parameter estimates using classical methods such as least-squares or maximum likelihood. Then, <code>sim</code> uses the results from the model fit to calculate the posterior distribution assuming flat prior distributions (Gelman and Hill, 2007). Because the rather complicated formula of the joint posterior distribution of the model parameters is not of much help for many users (such as us biologists), <code>sim</code> samples pairs of random values of $and ^2 from this distribution.</p>
<p>iven enough random samples, uncertainty measurements for each model parameter can be obtained. Often we calculate an interval within which we expect the true parameter value to be with a probability of 0.95, the so-called 95% credible interval (CrI). Note that the frequentist confidence interval does not allow such a straightforward interpretation (see Section <a href="bayes.html#frequentist">3.3</a>). Credible intervals can be defined in different ways: two commonly used CrIs are the symmetric CrI and the highest posterior density interval. The symmetric 95% CrI is the interval between the 2.5% and 97.5% quantiles of the posterior distribution. For skewed posterior distributions, the density values at the 2.5% and 97.5% quantiles can be very different. The highest posterior density 95% interval is a 95% interval that is chosen so that, for any value within the interval, the density function is equal to or higher than for any value outside the interval.</p>
<p>Let’s use a simple example. What is the mean height of humans? A random sample of 10 people was drawn and their height measured. A normal model with unknown mean and unknown variance, <span class="math inline">\(y \sim Norm(\theta, \sigma^2)\)</span>, can be assumed for human height, and the model is fitted to the data by the least-squares method in R using the <code>lm</code> function to get estimates for the mean and standard deviation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate hypothetical body height measurements</span>
true.mean &lt;-<span class="st"> </span><span class="dv">165</span>     <span class="co"># population mean</span>
true.sd &lt;-<span class="st"> </span><span class="dv">10</span>        <span class="co"># standard deviation</span>
y &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">rnorm</span>(<span class="dv">10</span>, <span class="dt">mean=</span>true.mean, <span class="dt">sd=</span>true.sd))
mod &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span><span class="dv">1</span>)       <span class="co"># least-squares fit</span>
mod                  <span class="co"># least-squares estimate of mean</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ 1)
## 
## Coefficients:
## (Intercept)  
##       168.6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mod)<span class="op">$</span>sigma   <span class="co"># least-squares estimate of standard deviation</span></code></pre></div>
<pre><code>## [1] 8.946756</code></pre>
<p>From the R output we obtain least-squares estimates for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\sigma\)</span>. We can describe our data distribution as <span class="math inline">\(y \sim Norm(\hat{q} = 168.60 \text{, } \sigma^2 = 8.95)\)</span>. The hats above the model parameters indicate that these parameters were estimated from data, that is, their true values are unknown. Estimates should always be given together with a measurement of their uncertainty. In Bayesian statistics, the uncertainty is measured by the variance of the posterior distribution <span class="math inline">\(p(\theta,\sigma|y)\)</span>. It describes which pairs of values of q and s are plausible given the data, the prior, and the model. The function sim draws pairs of values from the joint posterior distribution of the two parameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(arm)
nsim &lt;-<span class="st"> </span><span class="dv">5000</span>
bsim &lt;-<span class="st"> </span><span class="kw">sim</span>(mod, <span class="dt">n.sim=</span>nsim)
<span class="kw">str</span>(bsim)</code></pre></div>
<pre><code>## Formal class &#39;sim&#39; [package &quot;arm&quot;] with 2 slots
##   ..@ coef : num [1:5000, 1] 169 168 166 165 168 ...
##   .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. ..$ : NULL
##   .. .. ..$ : chr &quot;(Intercept)&quot;
##   ..@ sigma: num [1:5000] 9.78 7.28 7.23 8.22 11.34 ...</code></pre>
<p>The function <code>sim</code> produces an object of class “sim” that contains two slots, “coef” (containing 5000 simulated values for <span class="math inline">\(\theta\)</span>) and “sigma” (containing the 5000 corresponding values for <span class="math inline">\(\sigma\)</span>). Note that the order matters: the coef and sigma values form pairs of reasonable combinations of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\sigma\)</span> values; we are talking about a joint posterior distribution of the two parameters. A scatterplot allows us to visualize the joint posterior distribution, <span class="math inline">\(p(\theta,\sigma|y)\)</span> (lower left panel in Figure <a href="bayes.html#fig:jointpost">3.3</a>). We can use the simulated values to draw our conclusions (see the following).</p>
<div class="figure"><span id="fig:jointpost"></span>
<img src="03-bayes_files/figure-html/jointpost-1.png" alt="There are 5000 draws from the joint posterior distribution of $\theta$ and $\sigma$. Lower left panel: every dot is one draw from the joint posterior distribution of $\theta$ and $\sigma$. The histograms in the upper and right panels give the marginal posterior distributions of $\theta$ and $\sigma$, respectively." width="480" />
<p class="caption">
Figure 3.3: There are 5000 draws from the joint posterior distribution of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\sigma\)</span>. Lower left panel: every dot is one draw from the joint posterior distribution of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\sigma\)</span>. The histograms in the upper and right panels give the marginal posterior distributions of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\sigma\)</span>, respectively.
</p>
</div>
</div>
</div>
<div id="frequentist" class="section level2">
<h2><span class="header-section-number">3.3</span> THE FREQUENTIST WAY</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="prerequisites.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/TobiasRoth/BDAEcology/edit/master/03-bayes.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
