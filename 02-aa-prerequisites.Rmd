# Prerequisites: What is R and basic statistical terms


## What is R?
* download for free: www.r-project.org 
* authors of first version: Robert Gentleman and Ross Ihaka, University of Auckland
* since 1997: R development core team
* R is a programming language



## Why using R?
* we program the statistical analyses
* we have to know what we do (no clicking)
* analyses are documented and reproducible
* we can write own functions
* methods can be shared (CRAN)

R is easy to learn (intuitive), less strict syntax, slow  
e.g. compared to Python or Julia


## Working with R
### Console and Editor
R Console and Editor (mostly RStudio)

\begin{center}
  \includegraphics[width=1.00\textwidth]{images/RConsole.png}
\end{center}




## Scale of measurement


Scale   | Examples          | Properties        | Coding in R | 
:-------|:------------------|:------------------|:--------------------|
Nominal | Sex, genotype, habitat  | Identity (values have a unique meaning) | `factor()` |
Ordinal | Elevational zones | Identity and magnitude (values have an ordered relationship) | `ordered()` |
Numeric | Discrete: counts;  continuous: body weight, wing length | Identity, magnitude, and equal intervals | `intgeger()` `numeric()` |


## Correlations

###Basics
  
- variance $\hat{\sigma^2} = s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2$  
  
    
      
- standard deviation $\hat{\sigma} = s = \sqrt{s^2}$  
  
    
  
- covariance $q = \frac{1}{n-1}\sum_{i=1}^{n}((x_i-\bar{x})*(y_i-\bar{y}))$  


### Pearson correlation coefficient
  
standardized covariance


  $r=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2\sum_{i=1}^{n}(y_i-\bar{y})^2}}$



### Spearman correlation coefficient
rank correlation  
correlation between rank(x) and rank(y)  
  
  robust against outliers

### Kendall's tau
rank correlation  

I = number of pairs (i,k) for which $(x_i < x_k)$ & $(y_i > y_k)$ or viceversa  
$\tau = 1-\frac{4I}{(n(n-1))}$



## Principal components analyses PCA
rotation of the coordinate system

```{r, fig.cap='Principal components are eigenvectors of the covariance or correlation matrix'}
x <- rnorm(100, 5, 1)
y <- rnorm(100, 0.8*x, 1)
pca <- princomp(cbind(x,y), cor=TRUE)
par(mar=c(3,3,0.3,0.3))
plot(x-mean(x),y-mean(y), asp=1, pch=16, col="blue")
pc1 <- eigen(cov(cbind(x,y)))$vectors[,1]
pc2 <- eigen(cov(cbind(x,y)))$vectors[,2]
abline(0, pc1[2]/pc1[1], col="green", lwd=2)
abline(0, pc2[2]/pc2[1], col="green", lwd=2)
```


rotation of the coordinate system so that   

* first component explains most variance  
* second component explains most of the remaining variance and is perpendicular to the first one  
* third component explains most of the remaining variance and is perpendicular to the first two  
* ...  

$(x,y)$ becomes $(pc1, pc2)$  
where  
$pc1_i= b_{11} x_i + b_{12} y_i$  
$pc2_i = b_{21} x_i + b_{22} y_i$ with $b_{jk}$ being loadings

```{r}
pca <- princomp(cbind(x,y), cor=TRUE)
loadings(pca)
```
loadings of a component can be multiplied by -1


proportion of variance explained by each component  
number of components = number of variables
```{r}
summary(pca)
```
outlook: components with low variance are shrinked to a higher degree in Ridge regression


## Inferential statistics
\begin{center}
  \includegraphics[width=1.00\textwidth]{images/Amrhein_PeerJ2018.png}
\end{center}

https://peerj.com/preprints/26857

> there is never a "yes-or-no" answer  
> there will always be uncertainty  
Amrhein (2017)

Statistical analyses is quantifying uncertainty.


Quantification of uncertainty only possible if  
  
1. the mechanisms under study are known
2. the observations are a random sample from the population of interest

Solutions:  

1. working with models and reporting assumptions  
2. study design

> reported uncertainties always are too small!


Example: Number of stats courses before starting a PhD among all PhD students

```{r}
# simulate the virtual true data
set.seed(235325)   # set seed for random number generator

# simulate fake data of the whole population
statscourses <- rpois(300000, rgamma(300000, 2, 3))  

# draw a random sample from the population
n <- 12            # sample size
y <- sample(statscourses, 12, replace=FALSE)         
```


```{r, fig.cap='', echo=FALSE}
par(mar=c(4,3,1,1))
hist(statscourses, breaks=seq(-0.5, 10.5, by=1), main=NA)     # draw histogram of the population
rug(jitter(y))                                       # add the sample
abline(v=mean(y), col="blue", lwd=2)                 # add the mean of the sample
abline(v=mean(statscourses), lwd=2)                  # add the true mean of the population
text(4, 150000, "Sample mean", col="blue", adj=c(0,1))
text(4, 120000, "True mean", adj=c(0,1))

```



We observe the sample mean, what do we know about the population mean?  

Frequentist solution: How would the sample mean scatter, if we repeat the study many times?  

Bayesian solution: For any possible value, what is the probability that it is the true population mean?  

```{r, fig.cap='',echo=FALSE, message=FALSE}
library(arm)
nsim <- 5000
# frequentist theoretical uncertainty of mean
my <- numeric(nsim)
for(i in 1:nsim) my[i] <- mean(sample(statscourses, 12, replace=FALSE))

# Bayesian uncertainty of mean
mod <- lm(y~1)
bsim <- sim(mod, n.sim=nsim)

par(mar=c(4,4,0.1,6), mgp=c(2,0.5,0), tck=-0.01)
hist(my, main=NA, xlab="Mean(y)", freq=FALSE, ylim=c(0, 2.5), col="grey",
     cex.axis=0.8, las=1)
bm <- density(bsim@coef[,1])
lines(bm$x, bm$y, lwd=2, col="violet")

abline(v= coef(mod), lwd=2, col="blue")
segments(coef(mod)-2*summary(mod)$coef[2], 0, coef(mod)+2*summary(mod)$coef[2], lwd=3, col="blue")

text(1, 1.6, "True repetitions of the study", adj=c(0,1), cex=0.9, xpd=NA)
text(0.55, 2.53, "Sample mean with 95% CI", col="blue", adj=c(0,1), cex=0.9, xpd=NA)
text(0.65, 1.8, "Posterior distribution of the mean", col="violet", adj=c(0,1), cex=0.9, xpd=NA)
```

Standard deviation and standard error  
```{r, fig.cap='',echo=FALSE, message=FALSE}
par(mar=c(4,4,0.1,6), mgp=c(2,0.5,0), tck=-0.01, cex.lab=0.8, cex.axis=0.7)
hist(statscourses, breaks=seq(-0.5, 10.5, by=1), main=NA, freq=FALSE, ylim=c(0, 2),
     las=1)     # draw histogram of the population
rug(jitter(y), col="green")                                       # add the sample
hist(my, add=TRUE, col=rgb(0,0,0,0.5), freq=FALSE)
abline(v= coef(mod), lwd=2, col="blue")
abline(v= coef(mod), lwd=2, lty=2, col="green")
segments(coef(mod)-sd(y), 0, coef(mod)+sd(y), 
         lwd=3, col="green")
segments(coef(mod)-summary(mod)$coef[2], 0, coef(mod)+summary(mod)$coef[2], 
         lwd=3, col="blue")
abline(v=mean(statscourses))
segments(mean(statscourses)-sd(statscourses), 1.5, mean(statscourses)+sd(statscourses), 1.5)
segments(mean(statscourses)-sd(my), 1.49, mean(statscourses)+sd(my), 1.49, lwd=3)
text(2, 1.5, "True mean with SD and SE", adj=c(0,1), cex=0.9, xpd=NA)
text(2, 0.5, "Estimated mean with SD", adj=c(0,1), col="green", cex=0.9, xpd=NA)
text(2, 0.3, "Estimated mean with SE", adj=c(0,1), col="blue", cex=0.9, xpd=NA)

```




SE = SD/sqrt(n)  
  
SE = SD of posterior distribution


## Central limit theorem / law of large numbers
  
    
```{r, fig.cap='',echo=FALSE, message=FALSE}
par(mar=c(4,4,0.1,6), mgp=c(2,0.5,0), tck=-0.01)

nn <- c(120, 1200)
my1 <- matrix(ncol=length(nn), nrow=nsim)
for(i in 1:nsim){
  for(j in 1:ncol(my1)){
    my1[i,j] <- mean(sample(statscourses, nn[j], replace=FALSE))
  }
}

hist(my, main=NA, xlab="Mean(y)", ylim=c(0, 1400))
hist(my1[,1], add=TRUE, col="blue")
hist(my1[,2], add=TRUE, col=rgb(1,0,0,0.5))
legend(1, 1400, fill=c("white", "blue", "pink"), legend=c("n=12", "n=120", "n=1200"), bty="n")

```



normal distribution = Gaussian distribution  
 
 $p(\theta) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2\sigma^2}(\theta -\mu)^2)$  
   
     
       
   $E(\theta) = \mu$, $var(\theta) = \sigma^2$, $mode(\theta) = \mu$


  
    
```{r, fig.cap='',echo=FALSE, message=FALSE}
par(mar=c(4,4,0.1,6), mgp=c(2,0.5,0), tck=-0.01)

hist(my, main=NA, xlab="Mean(y)", ylim=c(0, 14), freq=FALSE, las=1)
hist(my1[,1], add=TRUE, col="blue", freq=FALSE)
hist(my1[,2], add=TRUE, col=rgb(1,0,0,0.5), freq=FALSE)
legend(1, 1400, fill=c("white", "blue", "pink"), legend=c("n=12", "n=120", "n=1200"), bty="n")
x <- seq(0, 2, length=100)
lines(x, dnorm(x, mean=mean(my), sd=sd(my)), lwd=2)
lines(x, dnorm(x, mean=mean(my1[,1]), sd=sd(my1[,1])), lwd=2, col="blue")
lines(x, dnorm(x, mean=mean(my1[,2]), sd=sd(my1[,2])), lwd=2, col="pink")
legend(1, 14, fill=c("white", "blue", "pink"), legend=c("n=12", "n=120", "n=1200"), bty="n")

```



## Bayes theorem
```{r, echo=FALSE}
dat <- data.frame(sex=c("m", "f", "f", "m", "m", "m", "f"),
                  birthday=c("wine","wine","flower","flower","wine","flower","flower"))
tab <- table(dat$sex, dat$birthday)
```
  
  
$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$  

sex    | flowers        | wine           | **sum**               | 
:------|:---------------|:---------------|:------------------|
female | `r tab[1,1]`  | `r tab[1,2]`  | **`r sum(tab[1,])`** |
male   | `r tab[2,1]`  | `r tab[2,2]`  | **`r sum(tab[2,])`** |
-------|----------------|----------------|-------------------|
**sum**    | **`r sum(tab[,1])`**| **`r sum(tab[,2])`**| **`r sum(tab)`** |

What is the probability that the person likes wine given it is a female?  
$P(A) =$ likes wine $= `r sum(tab[,2])/sum(tab)`$  
$P(B) =$ female $= `r sum(tab[1,])/ sum(tab)`$   

$P(B|A) =$ proportion of females among the wine liker $= `r tab[1,2]/sum(tab[,2])`$

Knowing the person's sex increases the knowledge of the birthday preference.



## Bayes theorem for continuous parameters

$p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} = \frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d\theta}$    


$p(\theta|y)$: posterior distribution

$p(y|\theta)$: likelihood, data model

$p(\theta)$: prior distribution

$p(y)$: scaling constant




### Single parameter model

$p(y|\theta) = Norm(\theta, \sigma)$, with $\sigma$ known 
  
  
$p(\theta) = Norm(\mu_0, \tau_0)$  

$p(\theta|y) = Norm(\mu_n, \tau_n)$, where
 $\mu_n= \frac{\frac{1}{\tau_0^2}\mu_0 + \frac{n}{\sigma^2}\bar{y}}{\frac{1}{\tau_0^2}+\frac{n}{\sigma^2}}$ and
 $\frac{1}{\tau_n^2} = \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}$
  
    
  $\bar{y}$ is a sufficient statistics  
  $p(\theta) = Norm(\mu_0, \tau_0)$ is a conjugate prior for $p(y|\theta) = Norm(\theta, \sigma)$, with $\sigma$ known.



Posterior mean = weighted average between prior mean and $\bar{y}$ with weights
equal to the precisions ($\frac{1}{\tau_0^2}$ and $\frac{n}{\sigma^2}$)
```{r, fig.cap='',echo=FALSE, results='hide'}
library(blmeco)
par(mar=c(4,4,0.8,0.1), mgp=c(2,0.5,0), tck=-0.01)

triplot.normal.knownvariance(theta.data=3, variance.known=2, n=3, 
                             prior.theta=0, prior.variance=10)
```




### A model with two parameters
$p(y|\theta, \sigma) = Norm(\theta, \sigma)$ 
  
\begin{center}
  \includegraphics[width=0.5\textwidth]{images/snowfinch.jpg}
\end{center}

```{r,echo=TRUE}
# weight (g)
y <- c(47.5, 43, 43, 44, 48.5, 37.5, 41.5, 45.5)
n <- length(y)
```


$p(y|\theta, \sigma) = Norm(\theta, \sigma)$ 
  
    
$p(\theta, \sigma) = N-Inv-\chi^2(\mu_0, \sigma_0^2/\kappa_0; v_0, \sigma_0^2)$ conjugate prior

  
$p(\theta,\sigma|y) = \frac{p(y|\theta, \sigma)p(\theta, \sigma)}{p(y)} = N-Inv-\chi^2(\mu_n, \sigma_n^2/\kappa_n; v_n, \sigma_n^2)$, with  

  
$\mu_n= \frac{\kappa_0}{\kappa_0+n}\mu_0 + \frac{n}{\kappa_0+n}\bar{y}$  
  
  $\kappa_n = \kappa_0+n$  
  
  $v_n = v_0 +n$  
  
  $v_n\sigma_n^2=v_0\sigma_0^2+(n-1)s^2+\frac{\kappa_0n}{\kappa_0+n}(\bar{y}-\mu_0)^2$

  
 $\bar{y}$ and $s^2$ are sufficient statistics  


Joint, marginal and conditional posterior distributions
```{r, fig.cap='',echo=FALSE}
mod <- lm(y~1)
bsim <- sim(mod, n.sim=20000)
xrange <- range(bsim@coef)
yrange <- range(bsim@sigma)

layout(matrix(c(2,0,1,3), ncol=2, byrow=TRUE), heights=c(1,2),  
       width=c(2,1))
par(mar=c(5.1, 4.1, 0.2, 0.2), cex.lab=0.8, cex.axis=0.8) 
plot(bsim@coef, bsim@sigma, xlab=expression(theta), ylab=expression(sigma), las=1, cex=0.5, xlim=xrange, ylim=yrange, col=rgb(0,0,0,0.1), pch=16)
par(mar=c(0, 4.1, 1, 0.2)) 
histtheta <- hist(bsim@coef, plot=FALSE)
plot(histtheta$breaks, seq(0, max(histtheta$density)+0.01, length= 
                             length(histtheta$breaks)), type="n", xaxt="n", xlim = xrange, 
     las=1, ylab = "Density", yaxs="i")
rect(histtheta$breaks[1:(length(histtheta$breaks)-1)], 0, 
     histtheta$breaks[2:(length(histtheta$breaks))], histtheta$density, 
     col=grey(0.5))
mtext(paste(expression(p(theta|y)), "= t-distribution"), side=3, adj=0, cex=0.6)
par(mar=c(5.1, 0, 0.2, 0.2)) 
histsigma<- hist(bsim@sigma, plot=FALSE)
plot(seq(0, max(histsigma$density)+0.01, length=length(histsigma$breaks)), 
     histsigma$breaks, type="n", yaxt="n", ylim=yrange, 
     xlab="Density", xaxs="i",
     xaxt="n")
axis(1, at=seq(0, 0.4, by=0.1), labels=c(NA, 0.1, 0.3, 0.3, 0.4))
rect(0, histsigma$breaks[2:(length(histsigma$breaks))], histsigma$density, 
     histsigma$breaks[1:(length(histsigma$breaks)-1)],col=grey(0.5))
text(0.04, 18, "p(sigma|y) = \nInv-Chisq-dist.", adj=c(0,1), cex=0.6)

```




## t-distribution
marginal posterior distribution of a normal mean with unknown variance and conjugate prior distribution
