# The Bayesian and the Frequentist Ways of Analyzing Data {#bayes}

## SHORT HISTORICAL OVERVIEW
Reverend Thomas Bayes (1701 or 1702e1761) developed the Bayes theorem. Based on this theorem, he described how to obtain the probability of a hy- pothesis given an observation, that is, data. However, he was so worried whether it would be acceptable to apply his theory to real-world examples that he did not dare to publish it. His methods were only published posthumously (Bayes, 1763). Without the help of computers, Bayes’ methods were appli- cable to just simple problems.

Much later, the concept of null hypothesis testing was introduced by Ronald A. Fisher (1890e1962) in his book Statistical Methods for Research Workers (Fisher, 1925) and many other publications. Egon Pearson (1895e1980) and others developed the frequentist statistical methods, which are based on the probability of the data given a null hypothesis. These methods are solvable for many simple and some moderately complex examples.

The rapidly improving capacity of computers in recent years now enables us to use Bayesian methods also for more (and even very) complex problems using simulation techniques (Smith et al., 1985; Gelfand & Smith, 1990; Gilks et al., 1996).

## THE BAYESIAN WAY
Bayesian methods use Bayes’ theorem to update prior knowledge about a parameter with information coming from the data to obtain posterior knowl- edge. The prior and posterior knowledge are mathematically described by a probability distribution (prior and posterior distributions of the parameter).

Bayes’ theorem for discrete events says that the probability of event A given event B has occurred, P(A|B), equals the probability of event A, P(A), times the probability of event B conditional on A, P(B|A), divided by the probability of event B, P(B):

\begin{align} 
P(A|B) = \frac{P(A)P(B|A)}{P(B)} (\#eq:bayesiantheorem)
\end{align} 

When using Bayes’ theorem for drawing inference from data, we are interested in the probability distribution of one or several parameters, $\theta$ (called “theta”), after having looked at the data y, that is, the posterior distribution, $p(\theta|y)$. To this end, Bayes’ theorem \@ref(eq:bayesiantheorem) is reformulated for continuous parameters using probability distributions rather than probabilities for discrete events:

\begin{align} 
p(\theta|y) = \frac{p(\theta)p(y|\theta)}{p(y)} (\#eq:bayesiantheoremc)
\end{align} 

The *posterior distribution*, $p(\theta|y)$, describes what we know about the parameter (or about the set of parameters), $\theta$, after having looked at the data and given the prior knowledge and the model. The *prior distribution* of $\theta$, $p(\theta)$, describes what we know about $\theta$ before having looked at the data. This is often very little but it can include information from earlier studies. The probability of the data conditional on $\theta$, $p(y|\theta)$, is called likelihood.

The word *likelihood* is used in Bayesian statistics with a slightly different meaning than it is used in frequentist statistics. The frequentist likelihood, $L(\theta|y)$, is a relative measure for the probability of the observed data given specific parameter values. The likelihood is a number often close to zero (see also Chapter \@ref(likelihood)). In contrast, Bayesians use likelihood for the density distribution of the data conditional on the parameters of a model. Thus, in Bayesian statistics the likelihood is a distribution (i.e., the area under the curve is 1) whereas in frequentist statistics it is a scalar. The *prior probability* of the data, $p(y)$, equals the integral of $p(y|\theta)p(\theta)$ over all possible values of $\theta$; thus $p(y)$ is a constant.

The integral can be solved numerically only for a few simple cases. For this reason, Bayesian statistics were not widely applied before the computer age. Nowadays, a variety of different simulation algorithms exist that allow sam- pling from distributions that are only known to proportionality (Gilks et al., 1996; Bre ́maud, 1999). Dropping the term p(y) in Equation\@ref(eq:bayesiantheoremc) leads to a term that is proportional to the posterior distribution: $p(\theta|y) \propto p(\theta)p(y|\theta)$. Simulation algorithms such as Markov chain Monte Carlo simulation (MCMC) can, therefore, sample from the posterior distribution without having to know $p(y)$. A large enough sample of the posterior distribution can then be used to draw inference about the parameters of interest.

### Estimating the Mean of a Normal Distribution with a Known Variance
The purpose of this section is to illustrate the Bayesian method using a theoretical example. It has only limited practical value. Therefore, feel free to skip this chapter if you are afraid of mathematical meditations. One of the simplest examples is to estimate the mean of a normal distribution with known variance based on a sample of n measurements.

The model of the data is $y \sim Norm(\theta, \sigma^2)$, which means "y is normally distributed with mean $\theta$ and variance $\sigma^2$". $\Theta$ is the true (population) mean and $\sigma^2$ the variance of the population from which the data are a random sample. Given the data contain three measurements, y1 = 27.1, y2 = 14.6, y3 = 14.6, and we know that the variance, $\sigma^2$, is 20, what do we know about the mean $\theta$ of the population from which the data are a random sample? Based on Bayes’ theorem and the normal prior distribution it is possible to see that the posterior distribution of the mean $\theta$ is itself a normal distribution with mean $\mu_n$ and variance $\sigma_n$, $p(\theta|y) \sim Norm(\mu_n, \sigma_n)$, where

\begin{align} 
\mu_n = \frac{\frac{\mu_o}{\sigma^2_0}+\frac{n\overline{y}}{\sigma^2}}{\frac{1}{\sigma^2_0}+\frac{n}{\sigma^2}}\text{, and } \frac{1}{\sigma2_n}=\frac{1}{\sigma^2_0}+\frac{n}{\sigma^2}.
\end{align}

$\mu_0$ and $\sigma^2_0$ are the mean and the variance of the prior distribution for $\theta$. Gelman et al. (2014) provide the derivation of these formulas. When we know in advance that m cannot be very far from 0, but we have very little knowledge about $\theta$, we might assume a flat normal distribution around 0 -- for example, $Norm(0, \sigma^2_0 = 200)$ -- as the prior distribution for $\theta$ (the dotted line in Figure \@ref(fig:triplot); only part of the right tail is visible). This results in a posterior distribution $p(q
\theta|y) \sim Norm(18.2, \sigma^2_0 = 6.5) (the solid line in Figure \@ref(fig:triplot)). This posterior distribution expresses what we know about q based on our prior knowledge, the data, and assuming our model adequately describes the process that generated the data (the data and the model are formalized in the likelihood, which, in this example, is a normal distribution with mean equal to the arithmetic mean of the data and variance equal to 20, the dashed line in Figure \@ref(fig:triplot)).

```{r triplot, results='hide', fig.width=5, fig.asp=0.9, echo=FALSE, fig.cap="Prior distribution, likelihood, and posterior distribution of the mean q. The plot has been drawn using the R function `triplot.normal.knownvariance` provided in the R package blmeco."}
triplot.normal.knownvariance(theta.data=15,
                             variance.known=20, n=3, 
                             prior.theta=0, prior.variance=200)
```

The posterior distribution is a combination of the information of the data and the prior; the data and the prior are weighted according to the information they contain. This information content is measured by the precision (which is equal to the inverse of the variance). If we use a completely flat (i.e., non- informative) prior distribution, the posterior distribution equals the likelihood (upper left panel in Figure \@ref(fig:triplottheta)). In this case the inference drawn does not differ from the inference drawn with frequentist methods. The more we know a priori about q, the stronger the influence the prior has on the posterior (Figure \@ref(fig:triplottheta)). When informative prior distributions are used, the inference drawn with Bayesian methods differ from the ones drawn with frequentist methods (instead of saying “prior distribution” and "posterior distribution" one often only says "prior" and "posterior").

Note that the likelihood is the same in all panels of Figure \@ref(fig:triplottheta) because the same data set and the same model is used.

```{r triplottheta, echo=FALSE, results='hide', fig.asp = .9, fig.cap="Prior, likelihood, and posterior distribution of the mean $\\theta$ using different prior distributions for $\\theta$."}
par(mfrow = c(2, 2), mar=c(4, 4, 2, 0.5))
set.seed(23052353) 
triplot.normal.knownvariance(theta.data = 15, variance.known = 20, n = 3, 
                             prior.theta = 10, prior.variance = 1000, 
                             ylim=c(0, 0.25))
title("prior: N(10, 10000)")
set.seed(23052353) 
triplot.normal.knownvariance(theta.data = 15, variance.known = 20, n = 3, 
                             prior.theta = 10, prior.variance = 200, 
                             legend=FALSE, ylim=c(0, 0.25))
title("prior: N(10, 200)")
set.seed(23052353) 
triplot.normal.knownvariance(theta.data = 15, variance.known = 20, n = 3, 
                             prior.theta = 10, prior.variance = 50, 
                             legend=FALSE, ylim=c(0, 0.25))

title("prior: N(10, 50)")
set.seed(23052353) 
triplot.normal.knownvariance(theta.data = 15, variance.known = 20, n = 3, 
                             prior.theta = 10, prior.variance = 5, 
                             legend=FALSE, ylim=c(0, 0.25))
title("prior: N(10, 5)")
```

### Estimating Mean and Variance of a Normal Distribution Using Simulation
The estimation of a mean $\theta and a variance $\sigma^2$ of a normal distribution, where both parameters are unknown, is more complicated than estimating the mean only, because the posterior distribution is two-dimensional. Such a posterior distribution is called a *joint posterior distribution*. Nevertheless, it is still possible to obtain the joint posterior distribution analytically (Albert, 2007; Gelman et al., 2014). Rather than following the analytical route, we now demonstrate how we can simulate a posterior distribution using the `sim` function in the package arm.

First, we obtain parameter estimates using classical methods such as least-squares or maximum likelihood. Then, `sim` uses the results from the model fit to calculate the posterior distribution assuming flat prior distributions (Gelman and Hill, 2007). Because the rather complicated formula of the joint posterior distribution of the model parameters is not of much help for many users (such as us biologists), `sim` samples pairs of random values of $\theta and \sigma^2 from this distribution.

iven enough random samples, uncertainty measurements for each model parameter can be obtained. Often we calculate an interval within which we expect the true parameter value to be with a probability of 0.95, the so-called 95% credible interval (CrI). Note that the frequentist confidence interval does not allow such a straightforward interpretation (see Section \@ref(frequentist)). Credible intervals can be defined in different ways: two commonly used CrIs are the symmetric CrI and the highest posterior density interval. The symmetric 95% CrI is the interval between the 2.5% and 97.5% quantiles of the posterior distribution. For skewed posterior distributions, the density values at the 2.5% and 97.5% quantiles can be very different. The highest posterior density 95% interval is a 95% interval that is chosen so that, for any value within the interval, the density function is equal to or higher than for any value outside the interval.

Let’s use a simple example. What is the mean height of humans? A random sample of 10 people was drawn and their height measured. A normal model with unknown mean and unknown variance, $y \sim  Norm(\theta, \sigma^2)$, can be assumed for human height, and the model is fitted to the data by the least-squares method in R using the `lm` function to get estimates for the mean and standard deviation:

```{r}
# simulate hypothetical body height measurements
true.mean <- 165     # population mean
true.sd <- 10        # standard deviation
y <- round(rnorm(10, mean=true.mean, sd=true.sd))
mod <- lm(y~1)       # least-squares fit
mod                  # least-squares estimate of mean
summary(mod)$sigma   # least-squares estimate of standard deviation
```

From the R output we obtain least-squares estimates for $\theta$ and $\sigma$. We can describe our data distribution as $y \sim Norm(\hat{q} = `r mod$coef` \text{, } \sigma^2 = `r summary(mod)$sigma`)$. The hats above the model parameters indicate that these parameters were estimated from data, that is, their true values are unknown. Estimates should always be given together with a measurement of their uncertainty. In Bayesian statistics, the uncertainty is measured by the variance of the posterior distribution $p(\theta,\sigma|y)$. It describes which pairs of values of q and s are plausible given the data, the prior, and the model. The function sim draws pairs of values from the joint posterior distribution of the two parameters.

```{r}
library(arm)
nsim <- 5000
bsim <- sim(mod, n.sim=nsim)
str(bsim)
```

The function `sim` produces an object of class "sim" that contains two slots, "coef" (containing 5000 simulated values for $\theta$) and "sigma" (containing the 5000 corresponding values for $\sigma$). Note that the order matters: the coef and sigma values form pairs of reasonable combinations of $\theta$ and $\sigma$ values; we are talking about a joint posterior distribution of the two parameters. A scatterplot allows us to visualize the joint posterior distribution, $p(\theta,\sigma|y)$ (lower left panel in Figure \@ref(fig:jointpost)). We can use the simulated values to draw our conclusions (see the following).

```{r jointpost, fig.width=5, fig.asp=1, echo=FALSE, fig.cap="There are 5000 draws from the joint posterior distribution of $\\theta$ and $\\sigma$. Lower left panel: every dot is one draw from the joint posterior distribution of $\\theta$ and $\\sigma$. The histograms in the upper and right panels give the marginal posterior distributions of $\\theta$ and $\\sigma$, respectively."}
layout(matrix(c(2,0,1,3), ncol=2, byrow=TRUE), heights=c(1,2), width=c(2,1))
par(mar=c(5.1, 4.1, 0.2, 0.2)) 
plot(bsim@coef, bsim@sigma, col=rgb(0,0,0,0.3), pch=16, 
     xlab=expression(theta), ylab=expression(sigma), las=1, cex.lab=1.4, 
     cex=0.5, xlim=range(bsim@coef), ylim=range(bsim@sigma))
par(mar=c(0, 4.1, 4.1, 0.2)) 
histtheta <- hist(bsim@coef, plot=FALSE)
plot(histtheta$breaks, seq(0, max(histtheta$density)+0.01, length = length(histtheta$breaks)), 
     type="n", xaxt="n", xlim = range(bsim@coef), 
     las=1, ylab = "Density", yaxs="i", cex.lab=1.2)
rect(histtheta$breaks[1:(length(histtheta$breaks)-1)], 0, 
     histtheta$breaks[2:(length(histtheta$breaks))], histtheta$density, col=grey(0.5))
par(mar=c(5.1, 0, 0.2, 4.1)) 
histsigma<- hist(bsim@sigma, plot=FALSE)
plot(seq(0, max(histsigma$density)+0.01, length=length(histsigma$breaks)), 
     histsigma$breaks, type="n", yaxt="n", ylim=range(bsim@sigma), 
     xlab="Density", xaxs="i", cex.lab=1.2,
     xaxt="n")
axis(1, at=seq(0, 0.15, by=0.05), labels=c(NA, 0.05, 0.10, 0.15))
rect(0, histsigma$breaks[2:(length(histsigma$breaks))], histsigma$density, 
     histsigma$breaks[1:(length(histsigma$breaks)-1)],col=grey(0.5))
```






## THE FREQUENTIST WAY {#frequentist}
